{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/data_data/reinforcement_learning/results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_action_dict = {'1,1,0':'010010011001','2,1,-2':'010011010111','2,1,2':'010011011011','2,2,-1':'010011100000','2,2,1':'010011100010','3,1,-1':'010100011000','3,1,1':'010100011010','4,1,-1':'010101011000','4,1,0':'010101011001','4,1,1':'010101011010','5,1,-1':'010110011000','5,1,0':'010110011001','5,1,1':'010110011010','6,1,-1':'010111011000','6,1,1':'010111011010','7,1,-2':'011000010111','7,1,2':'011000011011','7,2,-1':'011000100000','7,2,1':'011000100010','8,1,0':'011001011001','9,1,0':'011010011001','9,1,-1':'011010011000','9,1,1':'011010011010','10,1,0':'011011011001','10,1,-1':'011011011000','10,1,1':'011011011010','11,1,0':'011100011001','11,1,-1':'011100011000','11,1,1':'011100011010','12,1,0':'011101011001','12,1,-1':'011101011000','12,1,1':'011101011010','13,1,0':'011110011001','13,1,-1':'011110011000','13,1,1':'011110011010','14,1,0':'011111011001','14,1,-1':'011111011000','14,1,1':'011111011010','15,1,0':'100000011001','15,1,-1':'100000011000','15,1,1':'100000011010','16,1,0':'100001011001','16,1,-1':'100001011000','16,1,1':'100001011010','-16,-1,0':'000001001001','-16,-1,-1':'000001001000','-16,-1,1':'000001001010','-15,-1,0':'000010001001','-15,-1,-1':'000010001000','-15,-1,1':'000010001010','-14,-1,0':'000011001001','-14,-1,-1':'000011001000','-14,-1,1':'000011001010','-13,-1,0':'000100001001','-13,-1,-1':'000100001000','-13,-1,1':'000100001010','-12,-1,0':'000101001001','-12,-1,-1':'000101001000','-12,-1,1':'000101001010','-11,-1,0':'000110001001','-11,-1,-1':'000110001000','-11,-1,1':'000110001010','-10,-1,0':'000111001001','-10,-1,-1':'000111001000','-10,-1,1':'000111001010','-9,-1,0':'001000001001','-9,-1,-1':'001000001000','-9,-1,1':'001000001010','-8,-1,0':'001001001001','-7,-1,-2':'001010000111','-7,-1,2':'001010001011','-7,-2,-1':'001010000000','-7,-2,1':'001010000010','-6,-1,-1':'001011001000','-6,-1,1':'001011001010','-5,-1,-1':'001100001000','-5,-1,0':'001100001001','-5,-1,1':'001100001010','-4,-1,-1':'001101001000','-4,-1,0':'001101001001','-4,-1,1':'001101001010','-3,-1,-1':'001110001000','-3,-1,1':'001110001010','-2,-1,-2':'001111000111','-2,-1,2':'001111001011','-2,-2,-1':'001111000000','-2,-2,1':'001111000010','-1,-1,0':'010000001001'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history_file_1k_combined.tsv  summary_file.tsv\r\n"
     ]
    }
   ],
   "source": [
    "!ls  *.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [\"foo.1_policy.tsv\", \"history_file_0938.2_policy.tsv\", \"foo.2_policy.tsv\", \"history_file_0938.3_policy.tsv\", \"foo.3_policy.tsv\",\"history_file_0938.4_policy.tsv\", \"foo.4_policy.tsv\", \"history_file_0938.5_policy.tsv\", \"foo.5_policy.tsv\",\"history_file_0938.6_policy.tsv\", \"foo.6_policy.tsv\", \"history_file_0938.7_policy.tsv\", \"foo.7_policy.tsv\", \"foo.8_policy.tsv\",  \"history_file_0938.0_policy.tsv\", \"history_file_0938.1_policy.tsv\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foo.1_policy.tsv',\n",
       " 'history_file_0938.2_policy.tsv',\n",
       " 'foo.2_policy.tsv',\n",
       " 'history_file_0938.3_policy.tsv',\n",
       " 'foo.3_policy.tsv',\n",
       " 'history_file_0938.4_policy.tsv',\n",
       " 'foo.4_policy.tsv',\n",
       " 'history_file_0938.5_policy.tsv',\n",
       " 'foo.5_policy.tsv',\n",
       " 'history_file_0938.6_policy.tsv',\n",
       " 'foo.6_policy.tsv',\n",
       " 'history_file_0938.7_policy.tsv',\n",
       " 'foo.7_policy.tsv',\n",
       " 'foo.8_policy.tsv',\n",
       " 'history_file_0938.0_policy.tsv',\n",
       " 'history_file_0938.1_policy.tsv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_dict(dict1, dict2):\n",
    "    dict3 = {}\n",
    "    s1 = set(dict1)\n",
    "    s2 = set(dict2)\n",
    "    for i in s1-s2:\n",
    "        dict3[i] = dict1[i]\n",
    "    for i in s2-s1:\n",
    "        dict3[i] = dict2[i]\n",
    "    for i in s1.intersection(s2):\n",
    "        dict3[i] = dict1[i] if dict1[i] >= dict2[i] else dict2[i]\n",
    "    return dict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'foo.1_policy.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-38ee93151a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtemp_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing:\\t{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mf_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'foo.1_policy.tsv'"
     ]
    }
   ],
   "source": [
    "policy_file = {}\n",
    "\n",
    "for file_path in input_files:\n",
    "    temp_dict = {}\n",
    "    with open(file_path) as f:\n",
    "        print(\"Processing:\\t{}\".format(file_path))\n",
    "        f_ = f.readlines()\n",
    "        for line in f_:\n",
    "            try:\n",
    "                in_dict  = ast.literal_eval(line)\n",
    "                value    = in_dict['expected_value']\n",
    "                action_s = in_dict['action']\n",
    "                state    = in_dict['state']\n",
    "                action_v = in_dict['policy']\n",
    "                action_b = [int(x) for x in sparse_action_dict[action_s]]\n",
    "                state_action = str(ast.literal_eval(state) + action_b)\n",
    "                if not state_action in temp_dict:\n",
    "                    temp_dict[state_action] = value\n",
    "                else:\n",
    "                    if value > temp_dict[state_action]:\n",
    "                        temp_dict[state_action] = value\n",
    "            except:\n",
    "                pass\n",
    "    policy_file = out_dict(policy_file, temp_dict)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(policy_file.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch /data_data/reinforcement_learning/results/policy_file.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data_data/reinforcement_learning/results/policy_file.json\", \"w\") as outfile:  \n",
    "    json.dump(policy_file, outfile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"/data_data/reinforcement_learning/results/policy_file.lines\"\n",
    "\n",
    "with open (output_file, \"a\") as f:\n",
    "    for key, value in policy_file.items():\n",
    "        f.write(\"{}\\t{}\\n\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "from args import args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as pd\n",
    "\n",
    "from my_classes import Dataset\n",
    "from random import sample\n",
    "from operator import itemgetter\n",
    "\n",
    "sys.path.append('./anaconda3/lib/python3.7/site-packages/torchvision')\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs',comment=\"MADDPG Chessy II - mCritic Training\")\n",
    "\n",
    "\n",
    "# CUDA for PyTorch\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device   = torch.device(\"cuda:0\" if use_cuda else \"cuda:1\")\n",
    "\n",
    "device_0 = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "device_1 = torch.device(\"cuda:1\" if use_cuda else \"cpu\")\n",
    "\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "max_epochs = 140000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from args import args\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "    \n",
    "    \n",
    "    \n",
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(args['seed'])\n",
    "        self.fc1 = nn.Linear(state_size, args['FC1_UNITS'])\n",
    "        self.fc2 = nn.Linear(args['FC1_UNITS'], args['FC2_UNITS'])\n",
    "        self.fc3 = nn.Linear(args['FC2_UNITS'], args['FC3_UNITS'])\n",
    "        self.fc4 = nn.Linear(args['FC3_UNITS'], action_size)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.fc4.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return F.softmax(self.fc4(x))\n",
    "\n",
    "    \n",
    "    \n",
    "class CriticNetwork(nn.Module):\n",
    "    \"\"\"Critic (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(args['seed'])\n",
    "        #self.fc1 = nn.Linear(state_size+action_size, args['FC1_UNITS'])\n",
    "        self.fc1 = nn.Linear(state_size + action_size, args['FC1_UNITS'])\n",
    "        self.fc2 = nn.Linear(args['FC1_UNITS'], args['FC2_UNITS'])\n",
    "        self.fc3 = nn.Linear(args['FC2_UNITS'], args['FC3_UNITS'])\n",
    "        self.fc4 = nn.Linear(args['FC3_UNITS'], 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.fc4.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        #state_action = torch.cat((state, action),dim=1)\n",
    "        #print(\"STATE_ACTION.shape:\\t{}\".format(state_action.shape))\n",
    "        x = F.relu(self.fc1(torch.cat((state, action), dim=1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "   \n",
    "class Actor():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "\n",
    "        #self.state_action_size   = state_action_size\n",
    "        self.seed         = args['seed']\n",
    "        self.device       = device_1\n",
    "    \n",
    "        self.network    = ActorNetwork(state_size, action_size).to(self.device)\n",
    "        self.target     = ActorNetwork(state_size, action_size).to(self.device)\n",
    "        self.optimizer  = optim.Adam(self.network.parameters(), lr=args['LR_ACTOR'])\n",
    "    \n",
    "class MCritic():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "\n",
    "        #self.state_action_size   = state_action_size\n",
    "        self.seed         = args['seed']\n",
    "        self.device       = device_0\n",
    "\n",
    "        self.network      = CriticNetwork(state_size, action_size).to(self.device)\n",
    "        self.target       = CriticNetwork(state_size, action_size).to(self.device)\n",
    "        self.optimizer    = optim.Adam(self.network.parameters(), lr=args['LR_CRITIC'], weight_decay=args['WEIGHT_DECAY'])\n",
    "        \n",
    "        #Model takes too long to run --> load model weights from previous run (took > 24hours on my machine)\n",
    "        #self.network.load_state_dict(torch.load(args['mcritic_path']), strict=False)\n",
    "        #self.target.load_state_dict(torch.load(args['mcritic_path']), strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Batch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get total number of batch files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x99953938.tsv',\n",
       " 'x9999024391.tsv',\n",
       " 'x9999021282.tsv',\n",
       " 'x9999001244.tsv',\n",
       " 'x99926665.tsv',\n",
       " 'x9999035688.tsv',\n",
       " 'x99914598.tsv',\n",
       " 'x99905123.tsv',\n",
       " 'x99960980.tsv',\n",
       " 'x99948602.tsv']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"/data_data/reinforcement_learning/results/model_splits/\"\n",
    "batch_files = os.listdir(data_dir)\n",
    "batch_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['99953938', '9999024391', '9999021282', '9999001244', '99926665']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_IDs = [x.split(\".\")[0][1:].strip() for x in batch_files]\n",
    "batch_IDs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['99904323',\n",
       " '99948410',\n",
       " '997412',\n",
       " '99946412',\n",
       " '99921637',\n",
       " '99910659',\n",
       " '99955594',\n",
       " '99986670',\n",
       " '9999016051',\n",
       " '9999016520']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_IDs_shuffled = random.sample(batch_IDs, len(batch_IDs))\n",
    "batch_IDs_shuffled[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MCritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparse_action_dict import sparse_action_dict\n",
    "from numeric_names import numeric_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pctl       = 0.65\n",
    "validation_pctl  = 0.80\n",
    "test_pctl        = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_policies     = len(batch_IDs_shuffled)\n",
    "train_cut_pt     = int(train_pctl*num_policies)\n",
    "val_cut_pt       = int(validation_pctl*num_policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_ = {}\n",
    "partition_['train']      = []\n",
    "partition_['validation'] = []\n",
    "parts = ['train','validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cut_pt       = int(train_pctl*num_policies)\n",
    "val_cut_pt         = int(validation_pctl*num_policies)\n",
    "training_generator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs = batch_IDs_shuffled[:train_cut_pt]\n",
    "val_idxs   = batch_IDs_shuffled[train_cut_pt:val_cut_pt]\n",
    "test_idxs  = batch_IDs_shuffled[val_cut_pt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_action_vect   = np.zeros(args['action_size'])\n",
    "print(null_action_vect)\n",
    "action_sparse_dict = {str([int(x) for x in v]):args['action_id_dict'][k] for k,v in sparse_action_dict.items()}\n",
    "action_sparse_dict\n",
    "\n",
    "for state_action in action_sparse_dict:\n",
    "    null_action_vect   = np.zeros(args['action_size'])\n",
    "    null_action_vect[action_sparse_dict[state_action]] = 1\n",
    "    action_sparse_dict[state_action] = null_action_vect\n",
    "    \n",
    "str(list(action_sparse_dict.values())[-1:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = None\n",
    "state_batch = None\n",
    "action_batch = None\n",
    "labels_batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(batch,depth):\n",
    "    ones = torch.sparse.torch.eye(depth).to(device)\n",
    "    return ones.index_select(0,batch).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_file(idx):\n",
    "    \n",
    "    #print(\"load_batch_file: \\t{}\".format(idx))\n",
    "    \n",
    "    if int(idx) < 10:\n",
    "        \n",
    "        file = os.path.join(data_dir, \"x\" + str(idx) + \".tsv\")\n",
    "    else:\n",
    "        file = os.path.join(data_dir, \"x\" + str(idx) + \".tsv\")\n",
    "        \n",
    "    return np.loadtxt(fname=file,dtype=np.str, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelLoss(idx,model):\n",
    "    \n",
    "    #print('get_data_and_labels...:\\t{}'.format(idx))\n",
    "    \n",
    "    device   = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    if model == mCritic:\n",
    "        device   = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    else:\n",
    "        device   = torch.device(\"cuda:1\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    global labels_batch\n",
    "    global state_batch\n",
    "    global action_batch\n",
    "    \n",
    "    batch_file_df = load_batch_file(idx)\n",
    "    \n",
    "    state   = torch.tensor([ast.literal_eval(x)[:args['state_size']] for x in batch_file_df[:,0]]).float().to(device)\n",
    "    \n",
    "    _action_ = [ast.literal_eval(x)[args['state_size']:] for x in batch_file_df[:,0]]\n",
    "    \n",
    "    action_  = [str(action_sparse_dict[str(x).replace(\"\\n\",\"\").strip()]).replace(\" \",\",\") for x in _action_]\n",
    "    \n",
    "    action  = torch.tensor([ast.literal_eval(x) for x in action_]).float().to(device)\n",
    "    \n",
    "    labels  = torch.tensor([ast.literal_eval(x) for x in batch_file_df[:,1]]).float().to(device)\n",
    "    \n",
    "    state_batch  = state.to(device)\n",
    "        \n",
    "    action_batch = action.to(device)\n",
    "        \n",
    "    labels_batch = labels.view(args['BATCH_SIZE'],-1).to(device)\n",
    "    \n",
    "    if model == mCritic:\n",
    "        \n",
    "        Q_targets  = labels_batch\n",
    "\n",
    "        Q_expected = model.network(state_batch, action_batch)\n",
    "        \n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        mid_point   = int((args['action_size'])/2)\n",
    "        \n",
    "        Q_targets   = torch.argmax(action_batch[:,:mid_point], dim=1)\n",
    "        \n",
    "        Q_expected  = torch.argmax(model.network(state_batch)[:,:mid_point], dim=1)\n",
    "        \n",
    "        #Q_expected  = one_hot_encode(best_action.reshape(-1),mid_point)\n",
    "        \n",
    "        loss        = F.mse_loss(Q_expected.float(), Q_targets.float())\n",
    "        \n",
    "        loss.requires_grad = True\n",
    "    \n",
    "        return loss\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 CPUs\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "epoch = 0\n",
    "num_cpus = psutil.cpu_count(logical=False) - 2 \n",
    "print(\"Using {} CPUs\".format(num_cpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def train_epoch(epoch):    \n",
    "    \n",
    "    for model in [mCritic, actor]:\n",
    "\n",
    "        # Training\n",
    "        train_idx  = sample(train_idxs, 1)[0]\n",
    "        model_loss = getModelLoss(train_idx,model)\n",
    "\n",
    "        # Minimize the loss\n",
    "        model.optimizer.zero_grad()\n",
    "        model_loss.backward()\n",
    "        model.optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        with torch.set_grad_enabled(False):\n",
    "            val_idx     = sample(val_idxs, 1)[0]   \n",
    "            model_val_loss = getModelLoss(val_idx,model)\n",
    "\n",
    "\n",
    "        if epoch % 1 == 0 and epoch > 0:\n",
    "            num_iter = epoch\n",
    "            if model == mCritic:\n",
    "                writer.add_scalars('mCritic_loss_II', {\n",
    "                    'mCritic_train_loss': model_loss, \n",
    "                    'mCritic_val_loss': model_val_loss,\n",
    "                },  epoch)\n",
    "                print(\"Epoch:\\t{}\\tmCritic_train_mse_loss:\\t{}\\tmCritic_val_mse_loss:\\t{}\\n\".format(epoch,model_loss, model_val_loss))\n",
    "            else:\n",
    "                writer.add_scalars('Actor_loss_II', {\n",
    "                    'Actor_train_loss': model_loss, \n",
    "                    'Actor_val_loss': model_val_loss,\n",
    "                },  epoch)\n",
    "                print(\"Epoch:\\t{}\\tActor_train_mse_loss:\\t{}\\tActor_val_mse_loss:\\t{}\\n\".format(epoch,model_loss, model_val_loss))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        train_epoch(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t1\tmCritic_train_mse_loss:\t18.563127517700195\tmCritic_val_mse_loss:\t21.160343170166016\n",
      "\n",
      "Epoch:\t1\tActor_train_mse_loss:\t556.640625\tActor_val_mse_loss:\t543.7119140625\n",
      "\n",
      "Epoch:\t2\tmCritic_train_mse_loss:\t16.613237380981445\tmCritic_val_mse_loss:\t15.520732879638672\n",
      "\n",
      "Epoch:\t2\tActor_train_mse_loss:\t540.5771484375\tActor_val_mse_loss:\t544.388671875\n",
      "\n",
      "Epoch:\t3\tmCritic_train_mse_loss:\t17.201885223388672\tmCritic_val_mse_loss:\t12.89240837097168\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-b1314e681030>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mactor\u001b[0m   \u001b[0;34m=\u001b[0m  \u001b[0mActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m#pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-13fe9b931e46>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-2d61ff64eb85>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrain_idx\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmodel_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetModelLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Minimize the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-fd8dc7c4911e>\u001b[0m in \u001b[0;36mgetModelLoss\u001b[0;34m(idx, model)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mbatch_file_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_batch_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mstate\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_file_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0m_action_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_file_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-fd8dc7c4911e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mbatch_file_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_batch_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mstate\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_file_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0m_action_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_file_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/ast.py\u001b[0m in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mleft\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_convert_signed_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/ast.py\u001b[0m in \u001b[0;36m_convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/ast.py\u001b[0m in \u001b[0;36m_convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_convert_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConstant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    mCritic =  MCritic(args['state_size'], args['action_size'])\n",
    "    \n",
    "    actor   =  Actor(args['state_size'], args['action_size'])\n",
    "    \n",
    "    train()\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mCritic.network.state_dict(), '/data_data/reinforcement_learning/results/checkpoint_mCritic_09192020_minLoss_TBD.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Multiprocessing to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    mCritic =  MCritic(args['state_size'], args['action_size'])\n",
    "    \n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    \n",
    "    mCritic.network.share_memory()\n",
    "    \n",
    "    processes = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        p = mp.Process(target=train, args=(mCritic,))\n",
    "        \n",
    "        p.start()\n",
    "        \n",
    "        processes.append(p)\n",
    "    \n",
    "    for p in processes:\n",
    "        \n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
