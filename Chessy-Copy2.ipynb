{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "import ipykernel\n",
    "import pandas as pd\n",
    "\n",
    "#pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('expand_frame_repr', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device  = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cuda:1\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "import os\n",
    "import jsonlines\n",
    "import ast\n",
    "import time, os, fnmatch, shutil\n",
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import requests\n",
    "\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "HISTORY_FILE = None\n",
    "\n",
    "summary_dict = {\"losses\":0,\"wins\":0,\"draws\":0}\n",
    "\n",
    "cycle = 0\n",
    "\n",
    "actions = [[], []]\n",
    "rewards = [[], []]\n",
    "history = {\"cycle\": cycle, \"actions\": actions, \"rewards\": rewards, \"value\": 0}\n",
    "\n",
    "score_board = {}\n",
    "#step_action_dict = defaultdict()\n",
    "#step_action_dict['random_moves'] = defaultdict()\n",
    "#step_action_dict['policy_moves'] = defaultdict()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting numeric_names.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile numeric_names.py\n",
    "\n",
    "numeric_names = {'w_R0': 1, 'w_K0': 2, 'w_B0': 3, 'w__K': 4, 'w__Q': 5, 'w_B1': 6, 'w_K1': 7, 'w_R1': 8, 'w_P0': 9, 'w_P1':\n",
    " 10, 'w_P2': 11, 'w_P3': 12, 'w_P4': 13, 'w_P5': 14, 'w_P6': 15, 'w_P7': 16, 'b_P0': -16, 'b_P1': -15, 'b_P2': -14, 'b_P3':\n",
    " -13, 'b_P4': -12, 'b_P5': -11, 'b_P6': -10, 'b_P7': -9, 'b_R0': -8, 'b_K0': -7, 'b_B0': -6, 'b__K': -5, 'b__Q': -4, 'b_B1'\n",
    ": -3, 'b_K1': -2, 'b_R1': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1,1,0': 0, '2,1,-2': 1, '2,1,2': 2, '2,2,-1': 3, '2,2,1': 4, '3,1,-1': 5, '3,1,1': 6, '4,1,-1': 7, '4,1,0': 8, '4,1,1': 9, '5,1,-1': 10, '5,1,0': 11, '5,1,1': 12, '6,1,-1': 13, '6,1,1': 14, '7,1,-2': 15, '7,1,2': 16, '7,2,-1': 17, '7,2,1': 18, '8,1,0': 19, '9,1,0': 20, '9,1,-1': 21, '9,1,1': 22, '10,1,0': 23, '10,1,-1': 24, '10,1,1': 25, '11,1,0': 26, '11,1,-1': 27, '11,1,1': 28, '12,1,0': 29, '12,1,-1': 30, '12,1,1': 31, '13,1,0': 32, '13,1,-1': 33, '13,1,1': 34, '14,1,0': 35, '14,1,-1': 36, '14,1,1': 37, '15,1,0': 38, '15,1,-1': 39, '15,1,1': 40, '16,1,0': 41, '16,1,-1': 42, '16,1,1': 43, '-16,-1,0': 44, '-16,-1,-1': 45, '-16,-1,1': 46, '-15,-1,0': 47, '-15,-1,-1': 48, '-15,-1,1': 49, '-14,-1,0': 50, '-14,-1,-1': 51, '-14,-1,1': 52, '-13,-1,0': 53, '-13,-1,-1': 54, '-13,-1,1': 55, '-12,-1,0': 56, '-12,-1,-1': 57, '-12,-1,1': 58, '-11,-1,0': 59, '-11,-1,-1': 60, '-11,-1,1': 61, '-10,-1,0': 62, '-10,-1,-1': 63, '-10,-1,1': 64, '-9,-1,0': 65, '-9,-1,-1': 66, '-9,-1,1': 67, '-8,-1,0': 68, '-7,-1,-2': 69, '-7,-1,2': 70, '-7,-2,-1': 71, '-7,-2,1': 72, '-6,-1,-1': 73, '-6,-1,1': 74, '-5,-1,-1': 75, '-5,-1,0': 76, '-5,-1,1': 77, '-4,-1,-1': 78, '-4,-1,0': 79, '-4,-1,1': 80, '-3,-1,-1': 81, '-3,-1,1': 82, '-2,-1,-2': 83, '-2,-1,2': 84, '-2,-2,-1': 85, '-2,-2,1': 86, '-1,-1,0': 87}\n",
      "{0: '1,1,0', 1: '2,1,-2', 2: '2,1,2', 3: '2,2,-1', 4: '2,2,1', 5: '3,1,-1', 6: '3,1,1', 7: '4,1,-1', 8: '4,1,0', 9: '4,1,1', 10: '5,1,-1', 11: '5,1,0', 12: '5,1,1', 13: '6,1,-1', 14: '6,1,1', 15: '7,1,-2', 16: '7,1,2', 17: '7,2,-1', 18: '7,2,1', 19: '8,1,0', 20: '9,1,0', 21: '9,1,-1', 22: '9,1,1', 23: '10,1,0', 24: '10,1,-1', 25: '10,1,1', 26: '11,1,0', 27: '11,1,-1', 28: '11,1,1', 29: '12,1,0', 30: '12,1,-1', 31: '12,1,1', 32: '13,1,0', 33: '13,1,-1', 34: '13,1,1', 35: '14,1,0', 36: '14,1,-1', 37: '14,1,1', 38: '15,1,0', 39: '15,1,-1', 40: '15,1,1', 41: '16,1,0', 42: '16,1,-1', 43: '16,1,1', 44: '-16,-1,0', 45: '-16,-1,-1', 46: '-16,-1,1', 47: '-15,-1,0', 48: '-15,-1,-1', 49: '-15,-1,1', 50: '-14,-1,0', 51: '-14,-1,-1', 52: '-14,-1,1', 53: '-13,-1,0', 54: '-13,-1,-1', 55: '-13,-1,1', 56: '-12,-1,0', 57: '-12,-1,-1', 58: '-12,-1,1', 59: '-11,-1,0', 60: '-11,-1,-1', 61: '-11,-1,1', 62: '-10,-1,0', 63: '-10,-1,-1', 64: '-10,-1,1', 65: '-9,-1,0', 66: '-9,-1,-1', 67: '-9,-1,1', 68: '-8,-1,0', 69: '-7,-1,-2', 70: '-7,-1,2', 71: '-7,-2,-1', 72: '-7,-2,1', 73: '-6,-1,-1', 74: '-6,-1,1', 75: '-5,-1,-1', 76: '-5,-1,0', 77: '-5,-1,1', 78: '-4,-1,-1', 79: '-4,-1,0', 80: '-4,-1,1', 81: '-3,-1,-1', 82: '-3,-1,1', 83: '-2,-1,-2', 84: '-2,-1,2', 85: '-2,-2,-1', 86: '-2,-2,1', 87: '-1,-1,0'}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from games import games\n",
    "from numeric_names import numeric_names\n",
    "\n",
    "Pieces_detail = defaultdict()\n",
    "actions_array = []\n",
    "sparse_action_dict = defaultdict()\n",
    "bin_action_dict = defaultdict()\n",
    "actions = []\n",
    "\n",
    "for piece in numeric_names.keys():\n",
    "    action_sparse = []\n",
    "    if piece[0] == 'w':\n",
    "        idx = 0\n",
    "    else:\n",
    "        idx = 1\n",
    "        \n",
    "    if piece[1:3] == '_R':\n",
    "        player_id = 'Rook'\n",
    "    elif piece[1:3] == '_K':\n",
    "        player_id = 'Knight'\n",
    "    elif piece[1:3] == '_B':\n",
    "        player_id = 'Bishop'\n",
    "    elif piece[1:3] == '_P':\n",
    "        player_id = 'Pawn'\n",
    "    elif piece[1:4] == '__K':\n",
    "        player_id = 'King'\n",
    "    elif piece[:3] == '__Q':\n",
    "        player_id = 'Queen'\n",
    "        \n",
    "    actions = games['chessy']['players'][player_id]['moves']\n",
    "    \n",
    "    actions = actions[idx]\n",
    "    \n",
    "    #action_diff = 4 - len(actions)\n",
    "    \n",
    "    #for i in range(action_diff):\n",
    "    #    actions.append((0,0))\n",
    "    \n",
    "    piece_num_id = numeric_names[piece]\n",
    "    \n",
    "    piece_bin_id = '{0:06b}'.format(piece_num_id + 17)\n",
    "    \n",
    "    actions_ = []\n",
    "    \n",
    "    for action in actions:\n",
    "        action_value_  = (8*action[0] + action[1] + 17)\n",
    "        action_value   = '{0:06b}'.format(action_value_)\n",
    "        actions_.append(action_value)\n",
    "        \n",
    "        piece_move_bin = (str(piece_bin_id)+str(action_value))\n",
    "        \n",
    "        actions_array.append(piece_move_bin)\n",
    "        \n",
    "        sparse_action  = str(piece_num_id)+\",\"+str(action[0])+ \",\" +str(action[1])\n",
    "        \n",
    "        action_sparse.append(sparse_action)\n",
    "        \n",
    "        sparse_action_dict[sparse_action] = piece_move_bin\n",
    "    \n",
    "    #print(piece_num_id, \"\\t\",piece_bin_id, \"\\t\", action_sparse,  \"\\t\", actions, \"\\t\", actions_)\n",
    "    \n",
    "    Pieces_detail[piece_num_id] = {\n",
    "        'piece_num_id':piece_num_id,\n",
    "        'piece_bin_id':piece_bin_id,\n",
    "        'action_sparse':action_sparse,\n",
    "        'actions_verbose': actions,\n",
    "        'actions_bin': actions_\n",
    "       }\n",
    "    \n",
    "action_ids = np.arange(0,len(sparse_action_dict.keys()),1)\n",
    "#print(action_ids)\n",
    "\n",
    "\n",
    "action_id_dict = {y:x for x,y in zip(action_ids,sparse_action_dict.keys())}\n",
    "print(action_id_dict) \n",
    "\n",
    "id_action_dict = {x:y for x,y in zip(action_ids,sparse_action_dict.keys())}\n",
    "print(id_action_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins_draws_losses = [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting args.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile args.py\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numeric_names import numeric_names\n",
    "\n",
    "args = { \n",
    "    \"update_every\":100,\n",
    "    \"window_size\":100,\n",
    "    \"BUFFER_SIZE\":int(1e6),\n",
    "    \"BATCH_SIZE\":1024,  \n",
    "    \"GAMMA\":0.99,\n",
    "    \"TAU\":2e-3,\n",
    "    \"LR_ACTOR\":1e-3,\n",
    "    \"LR_CRITIC\":1.1e-3,\n",
    "    \"WEIGHT_DECAY\":0.0001,\n",
    "    \"UPDATE_EVERY\":5,\n",
    "    \"EXPLORE_NOISE\":0.05,\n",
    "    \"FC1_UNITS\":768,\n",
    "    \"FC2_UNITS\":2048,\n",
    "    \"FC3_UNITS\":256,\n",
    "    \"seed\":0,\n",
    "    \"state_size\":384,\n",
    "    \"action_size\":88,\n",
    "    \"action_size_binary\":12,\n",
    "    \"num_agents\":2,\n",
    "    \"device\":torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cuda:1\"),\n",
    "    'mcritic_path':'/home/ubuntu/chessy/checkpoint_mCritic.pth',\n",
    "    'agent_p0_path':'/home/ubuntu/chessy/checkpoint_p0.pth',\n",
    "    'agent_p1_path':'/home/ubuntu/chessy/checkpoint_p1.pth',\n",
    "    'action_id_dict': {'1,1,0': 0, '2,1,-2': 1, '2,1,2': 2, '2,2,-1': 3, '2,2,1': 4, '3,1,-1': 5, '3,1,1': 6, '4,1,-1': 7, '4,1,0': 8, '4,1,1': 9, '5,1,-1': 10, '5,1,0': 11, '5,1,1': 12, '6,1,-1': 13, '6,1,1': 14, '7,1,-2': 15, '7,1,2': 16, '7,2,-1': 17, '7,2,1': 18, '8,1,0': 19, '9,1,0': 20, '9,1,-1': 21, '9,1,1': 22, '10,1,0': 23, '10,1,-1': 24, '10,1,1': 25, '11,1,0': 26, '11,1,-1': 27, '11,1,1': 28, '12,1,0': 29, '12,1,-1': 30, '12,1,1': 31, '13,1,0': 32, '13,1,-1': 33, '13,1,1': 34, '14,1,0': 35, '14,1,-1': 36, '14,1,1': 37, '15,1,0': 38, '15,1,-1': 39, '15,1,1': 40, '16,1,0': 41, '16,1,-1': 42, '16,1,1': 43, '-16,-1,0': 44, '-16,-1,-1': 45, '-16,-1,1': 46, '-15,-1,0': 47, '-15,-1,-1': 48, '-15,-1,1': 49, '-14,-1,0': 50, '-14,-1,-1': 51, '-14,-1,1': 52, '-13,-1,0': 53, '-13,-1,-1': 54, '-13,-1,1': 55, '-12,-1,0': 56, '-12,-1,-1': 57, '-12,-1,1': 58, '-11,-1,0': 59, '-11,-1,-1': 60, '-11,-1,1': 61, '-10,-1,0': 62, '-10,-1,-1': 63, '-10,-1,1': 64, '-9,-1,0': 65, '-9,-1,-1': 66, '-9,-1,1': 67, '-8,-1,0': 68, '-7,-1,-2': 69, '-7,-1,2': 70, '-7,-2,-1': 71, '-7,-2,1': 72, '-6,-1,-1': 73, '-6,-1,1': 74, '-5,-1,-1': 75, '-5,-1,0': 76, '-5,-1,1': 77, '-4,-1,-1': 78, '-4,-1,0': 79, '-4,-1,1': 80, '-3,-1,-1': 81, '-3,-1,1': 82, '-2,-1,-2': 83, '-2,-1,2': 84, '-2,-2,-1': 85, '-2,-2,1': 86, '-1,-1,0': 87},\n",
    "    'id_action_dict': {0: '1,1,0', 1: '2,1,-2', 2: '2,1,2', 3: '2,2,-1', 4: '2,2,1', 5: '3,1,-1', 6: '3,1,1', 7: '4,1,-1', 8: '4,1,0', 9: '4,1,1', 10: '5,1,-1', 11: '5,1,0', 12: '5,1,1', 13: '6,1,-1', 14: '6,1,1', 15: '7,1,-2', 16: '7,1,2', 17: '7,2,-1', 18: '7,2,1', 19: '8,1,0', 20: '9,1,0', 21: '9,1,-1', 22: '9,1,1', 23: '10,1,0', 24: '10,1,-1', 25: '10,1,1', 26: '11,1,0', 27: '11,1,-1', 28: '11,1,1', 29: '12,1,0', 30: '12,1,-1', 31: '12,1,1', 32: '13,1,0', 33: '13,1,-1', 34: '13,1,1', 35: '14,1,0', 36: '14,1,-1', 37: '14,1,1', 38: '15,1,0', 39: '15,1,-1', 40: '15,1,1', 41: '16,1,0', 42: '16,1,-1', 43: '16,1,1', 44: '-16,-1,0', 45: '-16,-1,-1', 46: '-16,-1,1', 47: '-15,-1,0', 48: '-15,-1,-1', 49: '-15,-1,1', 50: '-14,-1,0', 51: '-14,-1,-1', 52: '-14,-1,1', 53: '-13,-1,0', 54: '-13,-1,-1', 55: '-13,-1,1', 56: '-12,-1,0', 57: '-12,-1,-1', 58: '-12,-1,1', 59: '-11,-1,0', 60: '-11,-1,-1', 61: '-11,-1,1', 62: '-10,-1,0', 63: '-10,-1,-1', 64: '-10,-1,1', 65: '-9,-1,0', 66: '-9,-1,-1', 67: '-9,-1,1', 68: '-8,-1,0', 69: '-7,-1,-2', 70: '-7,-1,2', 71: '-7,-2,-1', 72: '-7,-2,1', 73: '-6,-1,-1', 74: '-6,-1,1', 75: '-5,-1,-1', 76: '-5,-1,0', 77: '-5,-1,1', 78: '-4,-1,-1', 79: '-4,-1,0', 80: '-4,-1,1', 81: '-3,-1,-1', 82: '-3,-1,1', 83: '-2,-1,-2', 84: '-2,-1,2', 85: '-2,-2,-1', 86: '-2,-2,1', 87: '-1,-1,0'},\n",
    "    'numeric_names' : numeric_names,\n",
    "    'initial_state'  : \"010010010011010100010101010110010111011000011001011010011011011100011101011110011111100000100001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000010000011000100000101000110000111001000001001001010001011001100001101001110001111010000\",\n",
    "    'SUMMARY_FILE':\"/data_data/reinforcement_learning/results/summary_file.tsv\",\n",
    "    'HISTORY_FILE':None,\n",
    "    'TAKE_KING_REWARD':100,\n",
    "    'MORE_POINTS_REWARD':10,\n",
    "    'EQUAL_POINTS_REWARD':0,\n",
    "    'STEP_REWARD':-1,\n",
    "    'WINS_DRAWS_LOSSES': [0,0,0],\n",
    "    'in_channels_1':1,\n",
    "    'in_channels_2':64,\n",
    "    'in_channels_l':128,\n",
    "    'out_channels_1':64,\n",
    "    'out_channels_2':128,\n",
    "    'out_channels_l':16,\n",
    "    'kernel_1_size':(6,1),\n",
    "    'kernel_2_size':(1,1),\n",
    "    'kernel_l_size':(1,1),\n",
    "    'stride_1_size':(6,1),\n",
    "    'stride_2_size':(6,1),\n",
    "    'stride_l_size':(1,1),\n",
    "    'reshape_size':(8,48),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sparse_action_dict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sparse_action_dict.py\n",
    "\n",
    "sparse_action_dict = {'1,1,0':'010010011001','2,1,-2':'010011010111','2,1,2':'010011011011','2,2,-1':'010011100000','2,2,1':'010011100010','3,1,-1':'010100011000','3,1,1':'010100011010','4,1,-1':'010101011000','4,1,0':'010101011001','4,1,1':'010101011010','5,1,-1':'010110011000','5,1,0':'010110011001','5,1,1':'010110011010','6,1,-1':'010111011000','6,1,1':'010111011010','7,1,-2':'011000010111','7,1,2':'011000011011','7,2,-1':'011000100000','7,2,1':'011000100010','8,1,0':'011001011001','9,1,0':'011010011001','9,1,-1':'011010011000','9,1,1':'011010011010','10,1,0':'011011011001','10,1,-1':'011011011000','10,1,1':'011011011010','11,1,0':'011100011001','11,1,-1':'011100011000','11,1,1':'011100011010','12,1,0':'011101011001','12,1,-1':'011101011000','12,1,1':'011101011010','13,1,0':'011110011001','13,1,-1':'011110011000','13,1,1':'011110011010','14,1,0':'011111011001','14,1,-1':'011111011000','14,1,1':'011111011010','15,1,0':'100000011001','15,1,-1':'100000011000','15,1,1':'100000011010','16,1,0':'100001011001','16,1,-1':'100001011000','16,1,1':'100001011010','-16,-1,0':'000001001001','-16,-1,-1':'000001001000','-16,-1,1':'000001001010','-15,-1,0':'000010001001','-15,-1,-1':'000010001000','-15,-1,1':'000010001010','-14,-1,0':'000011001001','-14,-1,-1':'000011001000','-14,-1,1':'000011001010','-13,-1,0':'000100001001','-13,-1,-1':'000100001000','-13,-1,1':'000100001010','-12,-1,0':'000101001001','-12,-1,-1':'000101001000','-12,-1,1':'000101001010','-11,-1,0':'000110001001','-11,-1,-1':'000110001000','-11,-1,1':'000110001010','-10,-1,0':'000111001001','-10,-1,-1':'000111001000','-10,-1,1':'000111001010','-9,-1,0':'001000001001','-9,-1,-1':'001000001000','-9,-1,1':'001000001010','-8,-1,0':'001001001001','-7,-1,-2':'001010000111','-7,-1,2':'001010001011','-7,-2,-1':'001010000000','-7,-2,1':'001010000010','-6,-1,-1':'001011001000','-6,-1,1':'001011001010','-5,-1,-1':'001100001000','-5,-1,0':'001100001001','-5,-1,1':'001100001010','-4,-1,-1':'001101001000','-4,-1,0':'001101001001','-4,-1,1':'001101001010','-3,-1,-1':'001110001000','-3,-1,1':'001110001010','-2,-1,-2':'001111000111','-2,-1,2':'001111001011','-2,-2,-1':'001111000000','-2,-2,1':'001111000010','-1,-1,0':'010000001001'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pieces = {\n",
    "    'Pawn':(\"w_P\",\"b_P\"),\n",
    "    'Knight':(\"w_K\",\"b_K\"),\n",
    "    'King':(\"w__K\",\"b__K\"),\n",
    "    'Queen':(\"w__Q\",\"b__Q\"),\n",
    "    'Bishop':(\"w_B\",\"b_B\"),\n",
    "    'Rook':(\"w_R\",\"b_R\"),\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def getBin(num):\n",
    "        if int(num) != 0:\n",
    "            return \"{0:{fill}6b}\".format(int(num)+17, fill='0')\n",
    "        else:\n",
    "            return \"{0:{fill}6b}\".format(0, fill='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse_action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_moves   = 0\n",
    "policy_moves = 0\n",
    "random_moves = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFunc(piece):\n",
    "    def piece_func():\n",
    "        return piece\n",
    "    return piece_func()\n",
    "\n",
    "def Bishop():\n",
    "    return getFunc(\"Bishop\")\n",
    "\n",
    "def Pawn():\n",
    "    return getFunc(\"Pawn\")\n",
    "\n",
    "def King():\n",
    "    return getFunc(\"King\")\n",
    "\n",
    "def Queen():\n",
    "    return getFunc(\"Queen\")\n",
    "\n",
    "def Knight():\n",
    "    return getFunc(\"Knight\")\n",
    "\n",
    "def Rook():\n",
    "    return getFunc(\"Rook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pawn'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pawn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Incorrect_Input_error(Exception):\n",
    "  \"\"\"Generic input error handler: raised in the case that any of the user inputed data is incorrect\"\"\"\n",
    "  pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp0  = None\n",
    "temp1  = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from args import args\n",
    "from sparse_action_dict import sparse_action_dict\n",
    "from numeric_names import numeric_names\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "#from tensorboardX import SummaryWriter\n",
    "import sys\n",
    "\n",
    "#sys.path.append('./anaconda3/lib/python3.7/site-packages/torchvision')\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#writer = SummaryWriter(comment=\"MADDPG Chessy\")\n",
    "\n",
    "id_names = {value:key for key,value in numeric_names.items()}\n",
    "names    = {value: key for key in Pieces for value in Pieces[key]}\n",
    "\n",
    "stats_rewards_list = []\n",
    "\n",
    "def run_trial(user_input, env, mCritic, userInput):\n",
    "    global temp0\n",
    "    global temp1\n",
    "    global cycle\n",
    "    global scores\n",
    "    global total_reward_agent\n",
    "    global total_reward_env\n",
    "    \n",
    "    for j in range(user_input.num_sides):\n",
    "        team_name = \"team_\" + str(j)\n",
    "        team_name = Team(user_input.game, user_input.teams[j][\"team_name\"], j,user_input.teams[j][\"skill\"] / 10, user_input.teams[j][\"strategy\"])\n",
    "\n",
    "        env.insert_team(team_name)\n",
    "            \n",
    "    env.not_deadlocked  = True\n",
    "    env.states.append(args['initial_state'])\n",
    "    time_step    = 0\n",
    "        \n",
    "    total_reward_agent = 0\n",
    "    \n",
    "    total_reward_env   = 0\n",
    "        \n",
    "    while env.not_deadlocked:\n",
    "         \n",
    "        curr_board_ids = {value:key for key,value in env.board.items()}\n",
    "        state   = env.state \n",
    "        state   = np.array(list(map(int, state))).astype(np.float32)\n",
    "        \n",
    "        #state_prior_tensor = torch.IntTensor(state_prior_bin).to(device)\n",
    "        action_ = agent_p0.act(state)\n",
    "        \n",
    "\n",
    "        #Determine feasible moves for each team:\n",
    "        feasible_moves     = []\n",
    "        feasible_moves_0   = env.get_feasible_moves(env.team[0])\n",
    "        feasible_moves_1   = env.get_feasible_moves(env.team[1])\n",
    "        [feasible_moves.append(x) for x in feasible_moves_0]\n",
    "        [feasible_moves.append(x) for x in feasible_moves_1]\n",
    "        \n",
    "        f_moves   = set([int(x) for x in [action_id_dict[str((args['numeric_names'][env.board[curr_pos]],move)).replace(\"(\",\"\").replace(\")\",\"\").replace(\" \",\"\")] for name, move, curr_pos, new_pos in feasible_moves]])\n",
    "        \n",
    "        #f_moves  = set(np.sort(f_moves, axis=-1, kind='quicksort', order=None))\n",
    "        \n",
    "        action_   = [v if k in f_moves else 0 for k,v in enumerate(action_)]\n",
    "\n",
    "        mid_point        = int((args['action_size']+1)/2)\n",
    "\n",
    "        action_idx_0     = np.argmax(action_[:mid_point])\n",
    "        action_idx_1     = np.argmax(action_[mid_point:]) + mid_point\n",
    "\n",
    "        action_sparse_0  = id_action_dict[action_idx_0]\n",
    "        action_sparse_1  = id_action_dict[action_idx_1]\n",
    "        #action_verbose_0 = (curr_board_ids[action_sparse_0[0]],action_sparse_0[1:])\n",
    "        \n",
    "        action_one_hot   = np.zeros(args['action_size'])\n",
    "        \n",
    "        if time_step % 2 == 0:\n",
    "            action_one_hot[action_idx_0]   = 1\n",
    "        else:\n",
    "            action_one_hot[action_idx_1]   = 1\n",
    "        \n",
    "        env.best_moves_sparse          = [action_sparse_0,action_sparse_1]\n",
    "        \n",
    "        \n",
    "        #############################################\n",
    "        #  SEPARATE OUT INTO A STANDALONE FUNCTION  #\n",
    "        #############################################\n",
    "        \n",
    "        \n",
    "        def get_verbose_action(sp_action,i):\n",
    "\n",
    "            act      = sp_action.split(\",\")\n",
    "            name     = id_names[int(act[0])]\n",
    "\n",
    "            curr_pos = curr_board_ids[name]\n",
    "            next_pos = zip(curr_pos,action)\n",
    "\n",
    "            move     = tuple([int(x) for x in act[1:]])\n",
    "\n",
    "            next_pos = tuple(sum(tuples) for tuples in zip(curr_pos,move))\n",
    "            \n",
    "            player   = env.team[i].players[name]\n",
    "                \n",
    "            moves    = [player,move, curr_pos,next_pos]\n",
    "            \n",
    "            return moves\n",
    "\n",
    "        \n",
    "        #############################################\n",
    "        # \n",
    "        #############################################\n",
    "        \n",
    "        \n",
    "        env.best_moves_verbose         = [get_verbose_action(action_sparse_0,0),get_verbose_action(action_sparse_1,1)]\n",
    "        \n",
    "        next_state, reward, done, info = env.step(cycle,user_input,args)\n",
    "        \n",
    "        next_state                     = np.array(list(map(int, env.state))).astype(np.float32)\n",
    "        \n",
    "        total_reward_agent             += reward[0]\n",
    "        total_reward_env               += reward[1]\n",
    "        \n",
    "        if userInput.teams[0][\"skill\"] > 1:\n",
    "            agent_p0.step(state, action_one_hot, int(reward[0]), next_state, done, mCritic)\n",
    "        \n",
    "        if userInput.teams[1][\"skill\"] > 1:\n",
    "            agent_p1.step(state, action_one_hot, int(reward[1]), next_state, done, mCritic)\n",
    "        \n",
    "        #num_iter = cycle*10 + time_step\n",
    "        #writer.add_scalar('Actor_loss', Agent_p0.actor_loss, num_iter)\n",
    "        #writer.add_scalar('Critic_loss', Agent_p0.mCriticLoss, num_iterp)\n",
    "        #writer.add_scalar('Total_reward_agent', total_reward_agent, num_iter)\n",
    "        #writer.add_scalar('Total_reward_env', total_reward_env , num_iter)\n",
    "\n",
    "        time_step                      += 1\n",
    "\n",
    "    scores.append((cycle, info[0],info[1]))\n",
    "    \n",
    "    stats_rewards_list.append((cycle, total_reward_agent, total_reward_env, time_step))\n",
    "    \n",
    "    if cycle % args['update_every'] == 0:\n",
    "            print(\"Agent scores: {}\".format(np.mean(agent_scores[-args['window_size']:], axis=0)))\n",
    "            \n",
    "    if time_step > args['window_size'] and cycle % args['update_every'] == 0:\n",
    "        \n",
    "        torch.save(agent_p0.actor_network.state_dict(), 'checkpoint_p0_twoSided.pth')\n",
    "        torch.save(agent_p1.actor_network.state_dict(), 'checkpoint_p1_twoSided.pth')\n",
    "        torch.save(mCritic.network.state_dict(), 'checkpoint_mCritic_twoSided.pth')\n",
    "        \n",
    "        agent_win_margin = np.mean(stats_rewards_list[-args['window_size']:],  axis=0)[1] - np.mean(stats_rewards_list[-args['window_size']:],  axis=0)[2]\n",
    "        print('Episode: {}'.format(cycle),\n",
    "        'Timestep: {}'.format(cycle),\n",
    "        'Total reward Agent: {}'.format(np.mean(stats_rewards_list[-args['window_size']:],  axis=0)[1]),\n",
    "        'Total reward Env: {}'.format(np.mean(stats_rewards_list[-args['window_size']:],  axis=0)[2]),\n",
    "        'Agent Win Margin: {}'.format(agent_win_margin),\n",
    "        'Agent Wins: {}'.format(args['WINS_DRAWS_LOSSES'][0]),\n",
    "        'Agent Draws: {}'.format(args['WINS_DRAWS_LOSSES'][1]),\n",
    "        'Agent Losses: {}'.format(args['WINS_DRAWS_LOSSES'][2]),\n",
    "        'Episode length: {}'.format(np.mean(stats_rewards_list[-args['window_size']:],axis=0)[3]),\n",
    "        'mCriticLoss: {}'.format(agent_p0.mCriticLoss),\n",
    "        'actorLoss: {}'.format(agent_p0.actorLoss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_trials(user_input):\n",
    "    \"\"\" Runs the num_trials \"\"\"\n",
    "    global cycle\n",
    "    global HISTORY_FILE\n",
    "    global SUMMARY_FILE\n",
    "    global env\n",
    "    global agent_scores\n",
    "    \n",
    "    print(\"==============================RUN_TRIALS==================================\")\n",
    "    \n",
    "    t = time.localtime()\n",
    "    timestamp = time.strftime('%b_%d_%Y_%H%M', t)\n",
    "    num_trials = user_input.num_trials\n",
    "    num_sides = user_input.num_sides\n",
    "    \n",
    "    #SUMMARY_FILE = (\"/data_data/reinforcement_learning/results/summary_file.tsv\")\n",
    "\n",
    "    #HISTORY_FILE = (\"/data_data/reinforcement_learning/results/history_file_\" + str(num_trials) + \"_trials_\" + str(num_sides) + \"_sides_\" + str(timestamp))\n",
    "    \n",
    "    \"\"\" Clear output file \"\"\"\n",
    "    \n",
    "    args['HISTORY_FILE'] = \"/data_data/reinforcement_learning/results/history_file_\" + str(num_trials) + \"_trials_\" + str(num_sides) + \"_sides_\" + str(timestamp)\n",
    "    \n",
    "    with open(args['HISTORY_FILE'], \"w\") as history_file:\n",
    "        history_file.write(\"\\n\")\n",
    "        \n",
    "        \n",
    "    for i in range(num_trials):\n",
    "        \n",
    "        sides = [x for x in range(num_sides)]\n",
    "        \n",
    "        env = Game(user_input.game, 8, sides, user_input.display_board_positions)\n",
    "\n",
    "        run_trial(user_input,env, mCritic, user_input)\n",
    "\n",
    "        cycle += 1\n",
    "        \n",
    "        agent_scores.append(env.last_reward)\n",
    "  \n",
    "        \n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from model import ActorNetwork, CriticNetwork, MCritic\n",
    "from replaybuffer import ReplayBuffer\n",
    "from args import args\n",
    "\n",
    "mCritic       =  MCritic(args['state_size'],args['action_size'])\n",
    "agent_p0      =  Agent(args['state_size'],args['action_size'], 0)\n",
    "agent_p1      =  Agent(args['state_size'],args['action_size'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CriticNetwork(\n",
      "  (fc1): Linear(in_features=472, out_features=768, bias=True)\n",
      "  (fc2): Linear(in_features=768, out_features=2048, bias=True)\n",
      "  (fc3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "ActorNetwork(\n",
      "  (fc1): Linear(in_features=384, out_features=768, bias=True)\n",
      "  (fc2): Linear(in_features=768, out_features=2048, bias=True)\n",
      "  (fc3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=88, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(mCritic.network)\n",
    "print(agent_p0.actor_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select a game: ['chessy' or 'checkers']: chessy\n",
      "select number of teams:  [0, 1 or 2] 2\n",
      "Choose a name for this team [e.g. 'blue_team' or 'green_team']: Agent\n",
      "Choose a color for team_2 [e.g. 'blue': green\n",
      "Please enter team's skill_level [1 = Novice, 10 = expert]: 10\n",
      "Please enter team's strategy [0 = 'cooperative', 1 = 'competitive']: 1\n",
      "Choose a name for this team [e.g. 'blue_team' or 'green_team']: Environment\n",
      "Choose a color for team_2 [e.g. 'blue': red\n",
      "Please enter team's skill_level [1 = Novice, 10 = expert]: 10\n",
      "Please enter team's strategy [0 = 'cooperative', 1 = 'competitive']: 1\n",
      "How many trials would you like to run? [1 - 1,000,000] 5000\n",
      "Do you want to see the board positions in realtime? [ 'Yes' or 'No' ]no\n",
      "==============================RUN_TRIALS==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/chessy/model.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.fc4(x))\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent scores: nan\n",
      "Episode: 0 Timestep: 0 Total reward Agent: -142.0 Total reward Env: -122.0 Agent Win Margin: -20.0 Agent Wins: 0 Agent Draws: 0 Agent Losses: 2 Episode length: 133.0 mCriticLoss: 0 actorLoss: 0\n",
      "Agent scores: [ 3.5 -3.5]\n",
      "Episode: 100 Timestep: 100 Total reward Agent: -158.85 Total reward Env: -166.05 Agent Win Margin: 7.200000000000017 Agent Wins: 134 Agent Draws: 4 Agent Losses: 64 Episode length: 163.45 mCriticLoss: 0.7785621285438538 actorLoss: 0.9867943525314331\n",
      "Agent scores: [ 3.6 -3.6]\n",
      "Episode: 200 Timestep: 200 Total reward Agent: -159.8 Total reward Env: -167.2 Agent Win Margin: 7.399999999999977 Agent Wins: 266 Agent Draws: 14 Agent Losses: 122 Episode length: 164.5 mCriticLoss: 0.39071106910705566 actorLoss: 0.9906545877456665\n",
      "Agent scores: [ 1.7 -1.7]\n",
      "Episode: 300 Timestep: 300 Total reward Agent: -161.91 Total reward Env: -165.31 Agent Win Margin: 3.4000000000000057 Agent Wins: 378 Agent Draws: 24 Agent Losses: 200 Episode length: 164.61 mCriticLoss: 0.7823089361190796 actorLoss: 0.9995378255844116\n",
      "Agent scores: [ 3. -3.]\n",
      "Episode: 400 Timestep: 400 Total reward Agent: -159.54 Total reward Env: -165.54 Agent Win Margin: 6.0 Agent Wins: 506 Agent Draws: 28 Agent Losses: 268 Episode length: 163.54 mCriticLoss: 0.9767774343490601 actorLoss: 0.9950642585754395\n",
      "Agent scores: [ 3.3 -3.3]\n",
      "Episode: 500 Timestep: 500 Total reward Agent: -161.03 Total reward Env: -167.23 Agent Win Margin: 6.199999999999989 Agent Wins: 636 Agent Draws: 30 Agent Losses: 336 Episode length: 165.13 mCriticLoss: 0.5868845582008362 actorLoss: 1.0009340047836304\n",
      "Agent scores: [ 3.8 -3.8]\n",
      "Episode: 600 Timestep: 600 Total reward Agent: -159.53 Total reward Env: -167.53 Agent Win Margin: 8.0 Agent Wins: 774 Agent Draws: 34 Agent Losses: 394 Episode length: 164.53 mCriticLoss: 0.4880279004573822 actorLoss: 0.9954272508621216\n",
      "Agent scores: [ 2.8 -2.8]\n",
      "Episode: 700 Timestep: 700 Total reward Agent: -159.95 Total reward Env: -165.55 Agent Win Margin: 5.600000000000023 Agent Wins: 900 Agent Draws: 38 Agent Losses: 464 Episode length: 163.75 mCriticLoss: 0.4887668788433075 actorLoss: 0.9994232654571533\n",
      "Agent scores: [ 2.8 -2.8]\n",
      "Episode: 800 Timestep: 800 Total reward Agent: -159.85 Total reward Env: -165.45 Agent Win Margin: 5.599999999999994 Agent Wins: 1026 Agent Draws: 42 Agent Losses: 534 Episode length: 163.65 mCriticLoss: 0.7810566425323486 actorLoss: 0.9955124855041504\n",
      "Agent scores: [ 2. -2.]\n",
      "Episode: 900 Timestep: 900 Total reward Agent: -160.47 Total reward Env: -164.07 Agent Win Margin: 3.5999999999999943 Agent Wins: 1140 Agent Draws: 50 Agent Losses: 612 Episode length: 163.27 mCriticLoss: 0.8790517449378967 actorLoss: 0.9977617263793945\n",
      "Agent scores: [ 2.1 -2.1]\n",
      "Episode: 1000 Timestep: 1000 Total reward Agent: -160.02 Total reward Env: -164.62 Agent Win Margin: 4.599999999999994 Agent Wins: 1258 Agent Draws: 60 Agent Losses: 684 Episode length: 163.32 mCriticLoss: 0.4884399473667145 actorLoss: 0.9977397918701172\n",
      "Agent scores: [ 2.4 -2.4]\n",
      "Episode: 1100 Timestep: 1100 Total reward Agent: -160.24 Total reward Env: -165.04 Agent Win Margin: 4.799999999999983 Agent Wins: 1378 Agent Draws: 68 Agent Losses: 756 Episode length: 163.64 mCriticLoss: 0.7819709181785583 actorLoss: 0.9981193542480469\n",
      "Agent scores: [ 3.4 -3.4]\n",
      "Episode: 1200 Timestep: 1200 Total reward Agent: -158.47 Total reward Env: -165.27 Agent Win Margin: 6.800000000000011 Agent Wins: 1510 Agent Draws: 72 Agent Losses: 820 Episode length: 162.87 mCriticLoss: 0.4884772300720215 actorLoss: 0.9994310140609741\n",
      "Agent scores: [ 3.6 -3.6]\n",
      "Episode: 1300 Timestep: 1300 Total reward Agent: -159.55 Total reward Env: -166.75 Agent Win Margin: 7.199999999999989 Agent Wins: 1644 Agent Draws: 76 Agent Losses: 882 Episode length: 164.15 mCriticLoss: 0.6841038465499878 actorLoss: 0.9972859025001526\n",
      "Agent scores: [ 3.5 -3.5]\n",
      "Episode: 1400 Timestep: 1400 Total reward Agent: -159.01 Total reward Env: -166.01 Agent Win Margin: 7.0 Agent Wins: 1778 Agent Draws: 78 Agent Losses: 946 Episode length: 163.51 mCriticLoss: 0.5862560272216797 actorLoss: 0.9979783892631531\n",
      "Agent scores: [ 3.1 -3.1]\n",
      "Episode: 1500 Timestep: 1500 Total reward Agent: -161.43 Total reward Env: -167.23 Agent Win Margin: 5.799999999999983 Agent Wins: 1906 Agent Draws: 80 Agent Losses: 1016 Episode length: 165.33 mCriticLoss: 1.1716411113739014 actorLoss: 0.9961997866630554\n",
      "Agent scores: [ 1.8 -1.8]\n",
      "Episode: 1600 Timestep: 1600 Total reward Agent: -161.03 Total reward Env: -165.03 Agent Win Margin: 4.0 Agent Wins: 2024 Agent Draws: 84 Agent Losses: 1094 Episode length: 164.03 mCriticLoss: 0.5864134430885315 actorLoss: 1.0018022060394287\n",
      "Agent scores: [ 2.2 -2.2]\n",
      "Episode: 1700 Timestep: 1700 Total reward Agent: -161.85 Total reward Env: -166.25 Agent Win Margin: 4.400000000000006 Agent Wins: 2144 Agent Draws: 88 Agent Losses: 1170 Episode length: 165.05 mCriticLoss: 0.5854202508926392 actorLoss: 1.0052993297576904\n",
      "Agent scores: [ 3.5 -3.5]\n",
      "Episode: 1800 Timestep: 1800 Total reward Agent: -160.04 Total reward Env: -167.04 Agent Win Margin: 7.0 Agent Wins: 2278 Agent Draws: 90 Agent Losses: 1234 Episode length: 164.54 mCriticLoss: 0.29314959049224854 actorLoss: 0.9979429841041565\n",
      "Agent scores: [ 1.2 -1.2]\n",
      "Episode: 1900 Timestep: 1900 Total reward Agent: -161.77 Total reward Env: -163.77 Agent Win Margin: 2.0 Agent Wins: 2382 Agent Draws: 102 Agent Losses: 1318 Episode length: 163.77 mCriticLoss: 0.48847293853759766 actorLoss: 1.0009419918060303\n",
      "Agent scores: [ 2.7 -2.7]\n",
      "Episode: 2000 Timestep: 2000 Total reward Agent: -159.48 Total reward Env: -165.28 Agent Win Margin: 5.800000000000011 Agent Wins: 2508 Agent Draws: 108 Agent Losses: 1386 Episode length: 163.38 mCriticLoss: 0.19531793892383575 actorLoss: 0.9977218508720398\n",
      "Agent scores: [ 2.7 -2.7]\n",
      "Episode: 2100 Timestep: 2100 Total reward Agent: -160.39 Total reward Env: -165.39 Agent Win Margin: 5.0 Agent Wins: 2630 Agent Draws: 114 Agent Losses: 1458 Episode length: 163.89 mCriticLoss: 0.8867120146751404 actorLoss: 1.003063440322876\n",
      "Agent scores: [ 2.2 -2.2]\n",
      "Episode: 2200 Timestep: 2200 Total reward Agent: -160.59 Total reward Env: -165.39 Agent Win Margin: 4.799999999999983 Agent Wins: 2754 Agent Draws: 114 Agent Losses: 1534 Episode length: 163.99 mCriticLoss: 0.19531729817390442 actorLoss: 0.9980819225311279\n",
      "Agent scores: [ 2. -2.]\n",
      "Episode: 2300 Timestep: 2300 Total reward Agent: -161.46 Total reward Env: -165.46 Agent Win Margin: 4.0 Agent Wins: 2874 Agent Draws: 114 Agent Losses: 1614 Episode length: 164.46 mCriticLoss: 0.39174264669418335 actorLoss: 1.0045466423034668\n",
      "Agent scores: [ 3.7 -3.7]\n",
      "Episode: 2400 Timestep: 2400 Total reward Agent: -159.43 Total reward Env: -166.83 Agent Win Margin: 7.400000000000006 Agent Wins: 3010 Agent Draws: 116 Agent Losses: 1676 Episode length: 164.13 mCriticLoss: 0.48892539739608765 actorLoss: 1.00020432472229\n",
      "Agent scores: [ 5.4 -5.4]\n",
      "Agent scores: [ 3.3 -3.3]\n",
      "Episode: 2600 Timestep: 2600 Total reward Agent: -159.05 Total reward Env: -165.25 Agent Win Margin: 6.199999999999989 Agent Wins: 3292 Agent Draws: 122 Agent Losses: 1788 Episode length: 163.15 mCriticLoss: 0.39064210653305054 actorLoss: 0.9957281351089478\n",
      "Agent scores: [ 3.3 -3.3]\n",
      "Agent scores: [ 1.5 -1.5]\n",
      "Episode: 2800 Timestep: 2800 Total reward Agent: -161.61 Total reward Env: -164.61 Agent Win Margin: 3.0 Agent Wins: 3536 Agent Draws: 134 Agent Losses: 1932 Episode length: 164.11 mCriticLoss: 0.8792816400527954 actorLoss: 0.9932793378829956\n",
      "Agent scores: [ 3. -3.]\n",
      "Episode: 2900 Timestep: 2900 Total reward Agent: -161.02 Total reward Env: -167.02 Agent Win Margin: 6.0 Agent Wins: 3664 Agent Draws: 138 Agent Losses: 2000 Episode length: 165.02 mCriticLoss: 0.19533050060272217 actorLoss: 0.9957598447799683\n",
      "Agent scores: [ 3.4 -3.4]\n",
      "Episode: 3000 Timestep: 3000 Total reward Agent: -159.66 Total reward Env: -166.46 Agent Win Margin: 6.800000000000011 Agent Wins: 3798 Agent Draws: 138 Agent Losses: 2066 Episode length: 164.06 mCriticLoss: 0.39092352986335754 actorLoss: 0.9982080459594727\n",
      "Agent scores: [ 3.1 -3.1]\n",
      "Episode: 3100 Timestep: 3100 Total reward Agent: -157.55 Total reward Env: -163.75 Agent Win Margin: 6.199999999999989 Agent Wins: 3926 Agent Draws: 144 Agent Losses: 2132 Episode length: 161.65 mCriticLoss: 0.2931625247001648 actorLoss: 0.999675452709198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent scores: [ 1.9 -1.9]\n",
      "Episode: 3200 Timestep: 3200 Total reward Agent: -160.48 Total reward Env: -164.28 Agent Win Margin: 3.8000000000000114 Agent Wins: 4042 Agent Draws: 150 Agent Losses: 2210 Episode length: 163.38 mCriticLoss: 0.39062950015068054 actorLoss: 0.9977452754974365\n",
      "Agent scores: [ 4.4 -4.4]\n",
      "Episode: 3300 Timestep: 3300 Total reward Agent: -157.07 Total reward Env: -165.87 Agent Win Margin: 8.800000000000011 Agent Wins: 4182 Agent Draws: 158 Agent Losses: 2262 Episode length: 162.47 mCriticLoss: 0.48821064829826355 actorLoss: 0.9949726462364197\n",
      "Agent scores: [ 2.1 -2.1]\n",
      "Episode: 3400 Timestep: 3400 Total reward Agent: -162.49 Total reward Env: -166.69 Agent Win Margin: 4.199999999999989 Agent Wins: 4300 Agent Draws: 164 Agent Losses: 2338 Episode length: 165.59 mCriticLoss: 0.48885688185691833 actorLoss: 0.9997910261154175\n",
      "Agent scores: [ 2.9 -2.9]\n",
      "Episode: 3500 Timestep: 3500 Total reward Agent: -159.77 Total reward Env: -165.57 Agent Win Margin: 5.799999999999983 Agent Wins: 4426 Agent Draws: 170 Agent Losses: 2406 Episode length: 163.67 mCriticLoss: 0.39138442277908325 actorLoss: 0.998529314994812\n",
      "Agent scores: [ 1.8 -1.8]\n",
      "Episode: 3600 Timestep: 3600 Total reward Agent: -161.81 Total reward Env: -165.41 Agent Win Margin: 3.5999999999999943 Agent Wins: 4538 Agent Draws: 182 Agent Losses: 2482 Episode length: 164.61 mCriticLoss: 0.5872913002967834 actorLoss: 0.9998692870140076\n",
      "Agent scores: [ 4. -4.]\n",
      "Episode: 3700 Timestep: 3700 Total reward Agent: -158.7 Total reward Env: -166.7 Agent Win Margin: 8.0 Agent Wins: 4676 Agent Draws: 186 Agent Losses: 2540 Episode length: 163.7 mCriticLoss: 0.4880863428115845 actorLoss: 1.0006403923034668\n",
      "Agent scores: [ 3.3 -3.3]\n",
      "Episode: 3800 Timestep: 3800 Total reward Agent: -159.28 Total reward Env: -165.48 Agent Win Margin: 6.199999999999989 Agent Wins: 4804 Agent Draws: 192 Agent Losses: 2606 Episode length: 163.38 mCriticLoss: 0.7814616560935974 actorLoss: 0.9942821264266968\n",
      "Agent scores: [ 1.6 -1.6]\n",
      "Episode: 3900 Timestep: 3900 Total reward Agent: -159.24 Total reward Env: -162.44 Agent Win Margin: 3.1999999999999886 Agent Wins: 4918 Agent Draws: 196 Agent Losses: 2688 Episode length: 161.84 mCriticLoss: 1.3683347702026367 actorLoss: 0.9989163279533386\n",
      "Agent scores: [ 2.5 -2.5]\n",
      "Episode: 4000 Timestep: 4000 Total reward Agent: -160.49 Total reward Env: -165.89 Agent Win Margin: 5.399999999999977 Agent Wins: 5042 Agent Draws: 202 Agent Losses: 2758 Episode length: 164.19 mCriticLoss: 0.09789274632930756 actorLoss: 1.0027607679367065\n",
      "Agent scores: [ 3.7 -3.7]\n",
      "Episode: 4100 Timestep: 4100 Total reward Agent: -158.18 Total reward Env: -165.58 Agent Win Margin: 7.400000000000006 Agent Wins: 5176 Agent Draws: 208 Agent Losses: 2818 Episode length: 162.88 mCriticLoss: 0.09783648699522018 actorLoss: 1.0002715587615967\n",
      "Agent scores: [ 4.1 -4.1]\n",
      "Episode: 4200 Timestep: 4200 Total reward Agent: -158.88 Total reward Env: -167.08 Agent Win Margin: 8.200000000000017 Agent Wins: 5314 Agent Draws: 214 Agent Losses: 2874 Episode length: 163.98 mCriticLoss: 0.4871499836444855 actorLoss: 1.0017321109771729\n",
      "Agent scores: [ 3.8 -3.8]\n",
      "Episode: 4300 Timestep: 4300 Total reward Agent: -158.87 Total reward Env: -166.07 Agent Win Margin: 7.199999999999989 Agent Wins: 5448 Agent Draws: 218 Agent Losses: 2936 Episode length: 163.47 mCriticLoss: 0.48849570751190186 actorLoss: 1.000472068786621\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from agent import Agent\n",
    "from model import ActorNetwork, CriticNetwork, MCritic\n",
    "from replaybuffer import ReplayBuffer\n",
    "from plotutils import plot_results\n",
    "\n",
    "from games  import games\n",
    "from games import games\n",
    "from game   import Game\n",
    "from player import Player_Template\n",
    "from team   import Team\n",
    "from userinput import userInput\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    global cycle\n",
    "    global HISTORY_FILE\n",
    "    user_input = userInput()\n",
    "    run_trials(user_input)\n",
    "    \n",
    "    \n",
    "\n",
    "#main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_board = []\n",
    "for i in [0,1]:\n",
    "    score_board.append([(x[0],x[i+1]) for x in scores])\n",
    "#score_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  team_scores = []\n",
    "  min_score = 99999999\n",
    "  max_score = 0\n",
    "\n",
    "  for j in range(user_input.num_sides):\n",
    "    team_scores.append([team_points for cycle, team_points in score_board[j]])\n",
    "    plot.hist(\n",
    "      team_scores[j],\n",
    "      bins=100,\n",
    "      label=user_input.teams[j][\"team_name\"],\n",
    "      color=user_input.teams[j][\"color\"])\n",
    "\n",
    "    min_j_score = min(team_scores[j])\n",
    "    max_j_score = max(team_scores[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins   = args['WINS_DRAWS_LOSSES'][0]\n",
    "draws  = args['WINS_DRAWS_LOSSES'][1]\n",
    "losses = args['WINS_DRAWS_LOSSES'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " args['WINS_DRAWS_LOSSES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.hist(wins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent_p0.actor_network.state_dict(), 'checkpoint_p0_twoSided.pth')\n",
    "torch.save(agent_p1.actor_network.state_dict(), 'checkpoint_p1_twoSided.pth')\n",
    "torch.save(mCritic.network.state_dict(), 'checkpoint_mCritic_twoSided.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent_p0.actor_network.state_dict(), 'checkpoint_p0.pth')\n",
    "torch.save(agent_p1.actor_network.state_dict(), 'checkpoint_p1.pth')\n",
    "torch.save(mCritic.network.state_dict(), 'checkpoint_mCritic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
