{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "import ipykernel\n",
    "import pandas as pd\n",
    "\n",
    "#pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('expand_frame_repr', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device  = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "import os\n",
    "import jsonlines\n",
    "import ast\n",
    "import time, os, fnmatch, shutil\n",
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import requests\n",
    "\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "HISTORY_FILE = None\n",
    "\n",
    "summary_dict = {\"losses\":0,\"wins\":0,\"draws\":0}\n",
    "\n",
    "cycle = 0\n",
    "\n",
    "actions = [[], []]\n",
    "rewards = [[], []]\n",
    "history = {\"cycle\": cycle, \"actions\": actions, \"rewards\": rewards, \"value\": 0}\n",
    "\n",
    "score_board = {}\n",
    "#step_action_dict = defaultdict()\n",
    "#step_action_dict['random_moves'] = defaultdict()\n",
    "#step_action_dict['policy_moves'] = defaultdict()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting numeric_names.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile numeric_names.py\n",
    "\n",
    "numeric_names = {'w_R0': 1, 'w_K0': 2, 'w_B0': 3, 'w__K': 4, 'w__Q': 5, 'w_B1': 6, 'w_K1': 7, 'w_R1': 8, 'w_P0': 9, 'w_P1':\n",
    " 10, 'w_P2': 11, 'w_P3': 12, 'w_P4': 13, 'w_P5': 14, 'w_P6': 15, 'w_P7': 16, 'b_P0': -16, 'b_P1': -15, 'b_P2': -14, 'b_P3':\n",
    " -13, 'b_P4': -12, 'b_P5': -11, 'b_P6': -10, 'b_P7': -9, 'b_R0': -8, 'b_K0': -7, 'b_B0': -6, 'b__K': -5, 'b__Q': -4, 'b_B1'\n",
    ": -3, 'b_K1': -2, 'b_R1': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1,1,0': 0, '2,1,-2': 1, '2,1,2': 2, '2,2,-1': 3, '2,2,1': 4, '3,1,-1': 5, '3,1,1': 6, '4,1,-1': 7, '4,1,0': 8, '4,1,1': 9, '5,1,-1': 10, '5,1,0': 11, '5,1,1': 12, '6,1,-1': 13, '6,1,1': 14, '7,1,-2': 15, '7,1,2': 16, '7,2,-1': 17, '7,2,1': 18, '8,1,0': 19, '9,1,0': 20, '9,1,-1': 21, '9,1,1': 22, '10,1,0': 23, '10,1,-1': 24, '10,1,1': 25, '11,1,0': 26, '11,1,-1': 27, '11,1,1': 28, '12,1,0': 29, '12,1,-1': 30, '12,1,1': 31, '13,1,0': 32, '13,1,-1': 33, '13,1,1': 34, '14,1,0': 35, '14,1,-1': 36, '14,1,1': 37, '15,1,0': 38, '15,1,-1': 39, '15,1,1': 40, '16,1,0': 41, '16,1,-1': 42, '16,1,1': 43, '-16,-1,0': 44, '-16,-1,-1': 45, '-16,-1,1': 46, '-15,-1,0': 47, '-15,-1,-1': 48, '-15,-1,1': 49, '-14,-1,0': 50, '-14,-1,-1': 51, '-14,-1,1': 52, '-13,-1,0': 53, '-13,-1,-1': 54, '-13,-1,1': 55, '-12,-1,0': 56, '-12,-1,-1': 57, '-12,-1,1': 58, '-11,-1,0': 59, '-11,-1,-1': 60, '-11,-1,1': 61, '-10,-1,0': 62, '-10,-1,-1': 63, '-10,-1,1': 64, '-9,-1,0': 65, '-9,-1,-1': 66, '-9,-1,1': 67, '-8,-1,0': 68, '-7,-1,-2': 69, '-7,-1,2': 70, '-7,-2,-1': 71, '-7,-2,1': 72, '-6,-1,-1': 73, '-6,-1,1': 74, '-5,-1,-1': 75, '-5,-1,0': 76, '-5,-1,1': 77, '-4,-1,-1': 78, '-4,-1,0': 79, '-4,-1,1': 80, '-3,-1,-1': 81, '-3,-1,1': 82, '-2,-1,-2': 83, '-2,-1,2': 84, '-2,-2,-1': 85, '-2,-2,1': 86, '-1,-1,0': 87}\n",
      "{0: '1,1,0', 1: '2,1,-2', 2: '2,1,2', 3: '2,2,-1', 4: '2,2,1', 5: '3,1,-1', 6: '3,1,1', 7: '4,1,-1', 8: '4,1,0', 9: '4,1,1', 10: '5,1,-1', 11: '5,1,0', 12: '5,1,1', 13: '6,1,-1', 14: '6,1,1', 15: '7,1,-2', 16: '7,1,2', 17: '7,2,-1', 18: '7,2,1', 19: '8,1,0', 20: '9,1,0', 21: '9,1,-1', 22: '9,1,1', 23: '10,1,0', 24: '10,1,-1', 25: '10,1,1', 26: '11,1,0', 27: '11,1,-1', 28: '11,1,1', 29: '12,1,0', 30: '12,1,-1', 31: '12,1,1', 32: '13,1,0', 33: '13,1,-1', 34: '13,1,1', 35: '14,1,0', 36: '14,1,-1', 37: '14,1,1', 38: '15,1,0', 39: '15,1,-1', 40: '15,1,1', 41: '16,1,0', 42: '16,1,-1', 43: '16,1,1', 44: '-16,-1,0', 45: '-16,-1,-1', 46: '-16,-1,1', 47: '-15,-1,0', 48: '-15,-1,-1', 49: '-15,-1,1', 50: '-14,-1,0', 51: '-14,-1,-1', 52: '-14,-1,1', 53: '-13,-1,0', 54: '-13,-1,-1', 55: '-13,-1,1', 56: '-12,-1,0', 57: '-12,-1,-1', 58: '-12,-1,1', 59: '-11,-1,0', 60: '-11,-1,-1', 61: '-11,-1,1', 62: '-10,-1,0', 63: '-10,-1,-1', 64: '-10,-1,1', 65: '-9,-1,0', 66: '-9,-1,-1', 67: '-9,-1,1', 68: '-8,-1,0', 69: '-7,-1,-2', 70: '-7,-1,2', 71: '-7,-2,-1', 72: '-7,-2,1', 73: '-6,-1,-1', 74: '-6,-1,1', 75: '-5,-1,-1', 76: '-5,-1,0', 77: '-5,-1,1', 78: '-4,-1,-1', 79: '-4,-1,0', 80: '-4,-1,1', 81: '-3,-1,-1', 82: '-3,-1,1', 83: '-2,-1,-2', 84: '-2,-1,2', 85: '-2,-2,-1', 86: '-2,-2,1', 87: '-1,-1,0'}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from games import games\n",
    "from numeric_names import numeric_names\n",
    "\n",
    "Pieces_detail = defaultdict()\n",
    "actions_array = []\n",
    "sparse_action_dict = defaultdict()\n",
    "bin_action_dict = defaultdict()\n",
    "actions = []\n",
    "\n",
    "for piece in numeric_names.keys():\n",
    "    action_sparse = []\n",
    "    if piece[0] == 'w':\n",
    "        idx = 0\n",
    "    else:\n",
    "        idx = 1\n",
    "        \n",
    "    if piece[1:3] == '_R':\n",
    "        player_id = 'Rook'\n",
    "    elif piece[1:3] == '_K':\n",
    "        player_id = 'Knight'\n",
    "    elif piece[1:3] == '_B':\n",
    "        player_id = 'Bishop'\n",
    "    elif piece[1:3] == '_P':\n",
    "        player_id = 'Pawn'\n",
    "    elif piece[1:4] == '__K':\n",
    "        player_id = 'King'\n",
    "    elif piece[:3] == '__Q':\n",
    "        player_id = 'Queen'\n",
    "        \n",
    "    actions = games['chessy']['players'][player_id]['moves']\n",
    "    \n",
    "    actions = actions[idx]\n",
    "    \n",
    "    #action_diff = 4 - len(actions)\n",
    "    \n",
    "    #for i in range(action_diff):\n",
    "    #    actions.append((0,0))\n",
    "    \n",
    "    piece_num_id = numeric_names[piece]\n",
    "    \n",
    "    piece_bin_id = '{0:06b}'.format(piece_num_id + 17)\n",
    "    \n",
    "    actions_ = []\n",
    "    \n",
    "    for action in actions:\n",
    "        action_value_  = (8*action[0] + action[1] + 17)\n",
    "        action_value   = '{0:06b}'.format(action_value_)\n",
    "        actions_.append(action_value)\n",
    "        \n",
    "        piece_move_bin = (str(piece_bin_id)+str(action_value))\n",
    "        \n",
    "        actions_array.append(piece_move_bin)\n",
    "        \n",
    "        sparse_action  = str(piece_num_id)+\",\"+str(action[0])+ \",\" +str(action[1])\n",
    "        \n",
    "        action_sparse.append(sparse_action)\n",
    "        \n",
    "        sparse_action_dict[sparse_action] = piece_move_bin\n",
    "    \n",
    "    #print(piece_num_id, \"\\t\",piece_bin_id, \"\\t\", action_sparse,  \"\\t\", actions, \"\\t\", actions_)\n",
    "    \n",
    "    Pieces_detail[piece_num_id] = {\n",
    "        'piece_num_id':piece_num_id,\n",
    "        'piece_bin_id':piece_bin_id,\n",
    "        'action_sparse':action_sparse,\n",
    "        'actions_verbose': actions,\n",
    "        'actions_bin': actions_\n",
    "       }\n",
    "    \n",
    "action_ids = np.arange(0,len(sparse_action_dict.keys()),1)\n",
    "#print(action_ids)\n",
    "\n",
    "\n",
    "action_id_dict = {y:x for x,y in zip(action_ids,sparse_action_dict.keys())}\n",
    "print(action_id_dict) \n",
    "\n",
    "id_action_dict = {x:y for x,y in zip(action_ids,sparse_action_dict.keys())}\n",
    "print(id_action_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins_draws_losses = [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting args.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile args.py\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numeric_names import numeric_names\n",
    "\n",
    "args = { \n",
    "    \"update_every\":100,\n",
    "    \"window_size\":100,\n",
    "    \"BUFFER_SIZE\":int(1e6),\n",
    "    \"BATCH_SIZE\":1024,  \n",
    "    \"GAMMA\":0.99,\n",
    "    \"TAU\":2e-3,\n",
    "    \"LR_ACTOR\":1e-3,\n",
    "    \"LR_CRITIC\":1.1e-3,\n",
    "    \"WEIGHT_DECAY\":0.0001,\n",
    "    \"UPDATE_EVERY\":5,\n",
    "    \"EXPLORE_NOISE\":0.05,\n",
    "    \"FC1_UNITS\":1024,\n",
    "    \"FC2_UNITS\":4096,\n",
    "    \"FC3_UNITS\":512,\n",
    "    \"seed\":0,\n",
    "    \"state_size\":384,\n",
    "    \"action_size\":88,\n",
    "    \"action_size_binary\":12,\n",
    "    \"num_agents\":2,\n",
    "    \"device\":torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cuda:0\"),\n",
    "    'mcritic_path':'/home/ubuntu/chessy/checkpoint_mCritic.pth',\n",
    "    'agent_p0_path':'/home/ubuntu/chessy/checkpoint_p0.pth',\n",
    "    'agent_p1_path':'/home/ubuntu/chessy/checkpoint_p1.pth',\n",
    "    'action_id_dict': {'1,1,0': 0, '2,1,-2': 1, '2,1,2': 2, '2,2,-1': 3, '2,2,1': 4, '3,1,-1': 5, '3,1,1': 6, '4,1,-1': 7, '4,1,0': 8, '4,1,1': 9, '5,1,-1': 10, '5,1,0': 11, '5,1,1': 12, '6,1,-1': 13, '6,1,1': 14, '7,1,-2': 15, '7,1,2': 16, '7,2,-1': 17, '7,2,1': 18, '8,1,0': 19, '9,1,0': 20, '9,1,-1': 21, '9,1,1': 22, '10,1,0': 23, '10,1,-1': 24, '10,1,1': 25, '11,1,0': 26, '11,1,-1': 27, '11,1,1': 28, '12,1,0': 29, '12,1,-1': 30, '12,1,1': 31, '13,1,0': 32, '13,1,-1': 33, '13,1,1': 34, '14,1,0': 35, '14,1,-1': 36, '14,1,1': 37, '15,1,0': 38, '15,1,-1': 39, '15,1,1': 40, '16,1,0': 41, '16,1,-1': 42, '16,1,1': 43, '-16,-1,0': 44, '-16,-1,-1': 45, '-16,-1,1': 46, '-15,-1,0': 47, '-15,-1,-1': 48, '-15,-1,1': 49, '-14,-1,0': 50, '-14,-1,-1': 51, '-14,-1,1': 52, '-13,-1,0': 53, '-13,-1,-1': 54, '-13,-1,1': 55, '-12,-1,0': 56, '-12,-1,-1': 57, '-12,-1,1': 58, '-11,-1,0': 59, '-11,-1,-1': 60, '-11,-1,1': 61, '-10,-1,0': 62, '-10,-1,-1': 63, '-10,-1,1': 64, '-9,-1,0': 65, '-9,-1,-1': 66, '-9,-1,1': 67, '-8,-1,0': 68, '-7,-1,-2': 69, '-7,-1,2': 70, '-7,-2,-1': 71, '-7,-2,1': 72, '-6,-1,-1': 73, '-6,-1,1': 74, '-5,-1,-1': 75, '-5,-1,0': 76, '-5,-1,1': 77, '-4,-1,-1': 78, '-4,-1,0': 79, '-4,-1,1': 80, '-3,-1,-1': 81, '-3,-1,1': 82, '-2,-1,-2': 83, '-2,-1,2': 84, '-2,-2,-1': 85, '-2,-2,1': 86, '-1,-1,0': 87},\n",
    "    'id_action_dict': {0: '1,1,0', 1: '2,1,-2', 2: '2,1,2', 3: '2,2,-1', 4: '2,2,1', 5: '3,1,-1', 6: '3,1,1', 7: '4,1,-1', 8: '4,1,0', 9: '4,1,1', 10: '5,1,-1', 11: '5,1,0', 12: '5,1,1', 13: '6,1,-1', 14: '6,1,1', 15: '7,1,-2', 16: '7,1,2', 17: '7,2,-1', 18: '7,2,1', 19: '8,1,0', 20: '9,1,0', 21: '9,1,-1', 22: '9,1,1', 23: '10,1,0', 24: '10,1,-1', 25: '10,1,1', 26: '11,1,0', 27: '11,1,-1', 28: '11,1,1', 29: '12,1,0', 30: '12,1,-1', 31: '12,1,1', 32: '13,1,0', 33: '13,1,-1', 34: '13,1,1', 35: '14,1,0', 36: '14,1,-1', 37: '14,1,1', 38: '15,1,0', 39: '15,1,-1', 40: '15,1,1', 41: '16,1,0', 42: '16,1,-1', 43: '16,1,1', 44: '-16,-1,0', 45: '-16,-1,-1', 46: '-16,-1,1', 47: '-15,-1,0', 48: '-15,-1,-1', 49: '-15,-1,1', 50: '-14,-1,0', 51: '-14,-1,-1', 52: '-14,-1,1', 53: '-13,-1,0', 54: '-13,-1,-1', 55: '-13,-1,1', 56: '-12,-1,0', 57: '-12,-1,-1', 58: '-12,-1,1', 59: '-11,-1,0', 60: '-11,-1,-1', 61: '-11,-1,1', 62: '-10,-1,0', 63: '-10,-1,-1', 64: '-10,-1,1', 65: '-9,-1,0', 66: '-9,-1,-1', 67: '-9,-1,1', 68: '-8,-1,0', 69: '-7,-1,-2', 70: '-7,-1,2', 71: '-7,-2,-1', 72: '-7,-2,1', 73: '-6,-1,-1', 74: '-6,-1,1', 75: '-5,-1,-1', 76: '-5,-1,0', 77: '-5,-1,1', 78: '-4,-1,-1', 79: '-4,-1,0', 80: '-4,-1,1', 81: '-3,-1,-1', 82: '-3,-1,1', 83: '-2,-1,-2', 84: '-2,-1,2', 85: '-2,-2,-1', 86: '-2,-2,1', 87: '-1,-1,0'},\n",
    "    'numeric_names' : numeric_names,\n",
    "    'initial_state'  : \"010010010011010100010101010110010111011000011001011010011011011100011101011110011111100000100001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000010000011000100000101000110000111001000001001001010001011001100001101001110001111010000\",\n",
    "    'SUMMARY_FILE':\"/data_data/reinforcement_learning/results/summary_file.tsv\",\n",
    "    'HISTORY_FILE':None,\n",
    "    'TAKE_KING_REWARD':100,\n",
    "    'MORE_POINTS_REWARD':10,\n",
    "    'EQUAL_POINTS_REWARD':0,\n",
    "    'STEP_REWARD':-1,\n",
    "    'WINS_DRAWS_LOSSES': [0,0,0],\n",
    "    'in_channels_1':1,\n",
    "    'in_channels_2':64,\n",
    "    'in_channels_l':128,\n",
    "    'out_channels_1':64,\n",
    "    'out_channels_2':128,\n",
    "    'out_channels_l':16,\n",
    "    'kernel_1_size':(6,1),\n",
    "    'kernel_2_size':(1,1),\n",
    "    'kernel_l_size':(1,1),\n",
    "    'stride_1_size':(6,1),\n",
    "    'stride_2_size':(6,1),\n",
    "    'stride_l_size':(1,1),\n",
    "    'reshape_size':(8,48)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_cnn.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from args import args\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1  = nn.Conv2d(in_channels=args['in_channels_1'],out_channels=args['out_channels_1'], kernel_size=args['kernel_1_size'], stride=args['stride_1_size']).to(device)\n",
    "        self.conv2  = nn.Conv2d(in_channels=args['in_channels_2'],out_channels=args['out_channels_2'], kernel_size=args['kernel_2_size'], stride=args['stride_2_size']).to(device)\n",
    "        self.linear = nn.Linear(args['in_channels_l']*args['out_channels_l'], args['action_size']).to(device)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.conv2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.linear.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x      = x.reshape(args['reshape_size']).unsqueeze(0).unsqueeze(0)\n",
    "        out    = F.relu(self.conv1(x))\n",
    "        out    = F.relu(self.conv2(out))\n",
    "        logits = self.linear(out.view(-1, args['in_channels_l']*args['out_channels_l']))\n",
    "        probs  = torch.mean(F.softmax(logits, dim=1), axis=0)\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    \n",
    "class CriticNetwork(nn.Module):\n",
    "    \"\"\"Critic (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(args['seed'])\n",
    "        #self.fc1 = nn.Linear(state_size+action_size, args['FC1_UNITS'])\n",
    "        self.fc1 = nn.Linear(state_size+action_size, args['FC1_UNITS'])\n",
    "        self.fc2 = nn.Linear(args['FC1_UNITS'], args['FC2_UNITS'])\n",
    "        self.fc3 = nn.Linear(args['FC2_UNITS'], args['FC3_UNITS'])\n",
    "        self.fc4 = nn.Linear(args['FC3_UNITS'], 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.fc4.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(torch.cat((state, action),dim=1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "    \n",
    "    \n",
    "class MCritic():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "\n",
    "        self.state_size   = state_size\n",
    "        self.action_size  = args['action_size']\n",
    "        self.seed         = args['seed']\n",
    "        self.device       = args['device']\n",
    "\n",
    "        self.network      = CriticNetwork(state_size, self.action_size).to(self.device)\n",
    "        self.target       = CriticNetwork(state_size, self.action_size).to(self.device)\n",
    "        self.optimizer    = optim.Adam(self.network.parameters(), lr=args['LR_CRITIC'], weight_decay=args['WEIGHT_DECAY'])\n",
    "        \n",
    "        #Model takes too long to run --> load model weights from previous run (took > 24hours on my machine)\n",
    "        #self.network.load_state_dict(torch.load(args['mcritic_path']), strict=False)\n",
    "        #self.target.load_state_dict(torch.load(args['mcritic_path']), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sparse_action_dict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sparse_action_dict.py\n",
    "\n",
    "sparse_action_dict = {'1,1,0':'010010011001','2,1,-2':'010011010111','2,1,2':'010011011011','2,2,-1':'010011100000','2,2,1':'010011100010','3,1,-1':'010100011000','3,1,1':'010100011010','4,1,-1':'010101011000','4,1,0':'010101011001','4,1,1':'010101011010','5,1,-1':'010110011000','5,1,0':'010110011001','5,1,1':'010110011010','6,1,-1':'010111011000','6,1,1':'010111011010','7,1,-2':'011000010111','7,1,2':'011000011011','7,2,-1':'011000100000','7,2,1':'011000100010','8,1,0':'011001011001','9,1,0':'011010011001','9,1,-1':'011010011000','9,1,1':'011010011010','10,1,0':'011011011001','10,1,-1':'011011011000','10,1,1':'011011011010','11,1,0':'011100011001','11,1,-1':'011100011000','11,1,1':'011100011010','12,1,0':'011101011001','12,1,-1':'011101011000','12,1,1':'011101011010','13,1,0':'011110011001','13,1,-1':'011110011000','13,1,1':'011110011010','14,1,0':'011111011001','14,1,-1':'011111011000','14,1,1':'011111011010','15,1,0':'100000011001','15,1,-1':'100000011000','15,1,1':'100000011010','16,1,0':'100001011001','16,1,-1':'100001011000','16,1,1':'100001011010','-16,-1,0':'000001001001','-16,-1,-1':'000001001000','-16,-1,1':'000001001010','-15,-1,0':'000010001001','-15,-1,-1':'000010001000','-15,-1,1':'000010001010','-14,-1,0':'000011001001','-14,-1,-1':'000011001000','-14,-1,1':'000011001010','-13,-1,0':'000100001001','-13,-1,-1':'000100001000','-13,-1,1':'000100001010','-12,-1,0':'000101001001','-12,-1,-1':'000101001000','-12,-1,1':'000101001010','-11,-1,0':'000110001001','-11,-1,-1':'000110001000','-11,-1,1':'000110001010','-10,-1,0':'000111001001','-10,-1,-1':'000111001000','-10,-1,1':'000111001010','-9,-1,0':'001000001001','-9,-1,-1':'001000001000','-9,-1,1':'001000001010','-8,-1,0':'001001001001','-7,-1,-2':'001010000111','-7,-1,2':'001010001011','-7,-2,-1':'001010000000','-7,-2,1':'001010000010','-6,-1,-1':'001011001000','-6,-1,1':'001011001010','-5,-1,-1':'001100001000','-5,-1,0':'001100001001','-5,-1,1':'001100001010','-4,-1,-1':'001101001000','-4,-1,0':'001101001001','-4,-1,1':'001101001010','-3,-1,-1':'001110001000','-3,-1,1':'001110001010','-2,-1,-2':'001111000111','-2,-1,2':'001111001011','-2,-2,-1':'001111000000','-2,-2,1':'001111000010','-1,-1,0':'010000001001'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pieces = {\n",
    "    'Pawn':(\"w_P\",\"b_P\"),\n",
    "    'Knight':(\"w_K\",\"b_K\"),\n",
    "    'King':(\"w__K\",\"b__K\"),\n",
    "    'Queen':(\"w__Q\",\"b__Q\"),\n",
    "    'Bishop':(\"w_B\",\"b_B\"),\n",
    "    'Rook':(\"w_R\",\"b_R\"),\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def getBin(num):\n",
    "        if int(num) != 0:\n",
    "            return \"{0:{fill}6b}\".format(int(num)+17, fill='0')\n",
    "        else:\n",
    "            return \"{0:{fill}6b}\".format(0, fill='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse_action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_moves   = 0\n",
    "policy_moves = 0\n",
    "random_moves = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFunc(piece):\n",
    "    def piece_func():\n",
    "        return piece\n",
    "    return piece_func()\n",
    "\n",
    "def Bishop():\n",
    "    return getFunc(\"Bishop\")\n",
    "\n",
    "def Pawn():\n",
    "    return getFunc(\"Pawn\")\n",
    "\n",
    "def King():\n",
    "    return getFunc(\"King\")\n",
    "\n",
    "def Queen():\n",
    "    return getFunc(\"Queen\")\n",
    "\n",
    "def Knight():\n",
    "    return getFunc(\"Knight\")\n",
    "\n",
    "def Rook():\n",
    "    return getFunc(\"Rook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pawn'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pawn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Incorrect_Input_error(Exception):\n",
    "  \"\"\"Generic input error handler: raised in the case that any of the user inputed data is incorrect\"\"\"\n",
    "  pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp0  = None\n",
    "temp1  = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from args import args\n",
    "from sparse_action_dict import sparse_action_dict\n",
    "from numeric_names import numeric_names\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "#from tensorboardX import SummaryWriter\n",
    "import sys\n",
    "\n",
    "#sys.path.append('./anaconda3/lib/python3.7/site-packages/torchvision')\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#writer = SummaryWriter(comment=\"MADDPG Chessy\")\n",
    "\n",
    "id_names = {value:key for key,value in numeric_names.items()}\n",
    "names    = {value: key for key in Pieces for value in Pieces[key]}\n",
    "\n",
    "stats_rewards_list = []\n",
    "\n",
    "def run_trial(user_input, env, mCritic, userInput):\n",
    "    global temp0\n",
    "    global temp1\n",
    "    global cycle\n",
    "    global scores\n",
    "    global total_reward_agent\n",
    "    global total_reward_env\n",
    "    \n",
    "    for j in range(user_input.num_sides):\n",
    "        team_name = \"team_\" + str(j)\n",
    "        team_name = Team(user_input.game, user_input.teams[j][\"team_name\"], j,user_input.teams[j][\"skill\"] / 10, user_input.teams[j][\"strategy\"])\n",
    "\n",
    "        env.insert_team(team_name)\n",
    "            \n",
    "    env.not_deadlocked  = True\n",
    "    env.states.append(args['initial_state'])\n",
    "    time_step    = 0\n",
    "        \n",
    "    total_reward_agent = 0\n",
    "    \n",
    "    total_reward_env   = 0\n",
    "        \n",
    "    while env.not_deadlocked:\n",
    "         \n",
    "        curr_board_ids = {value:key for key,value in env.board.items()}\n",
    "        state   = env.state \n",
    "        state   = np.array(list(map(int, state))).astype(np.float32)\n",
    "        \n",
    "        #state_prior_tensor = torch.IntTensor(state_prior_bin).to(device)\n",
    "        action_ = agent_p0.act(state)\n",
    "        \n",
    "\n",
    "        #Determine feasible moves for each team:\n",
    "        feasible_moves     = []\n",
    "        feasible_moves_0   = env.get_feasible_moves(env.team[0])\n",
    "        feasible_moves_1   = env.get_feasible_moves(env.team[1])\n",
    "        [feasible_moves.append(x) for x in feasible_moves_0]\n",
    "        [feasible_moves.append(x) for x in feasible_moves_1]\n",
    "        \n",
    "        f_moves   = set([int(x) for x in [action_id_dict[str((args['numeric_names'][env.board[curr_pos]],move)).replace(\"(\",\"\").replace(\")\",\"\").replace(\" \",\"\")] for name, move, curr_pos, new_pos in feasible_moves]])\n",
    "        \n",
    "        #f_moves  = set(np.sort(f_moves, axis=-1, kind='quicksort', order=None))\n",
    "        \n",
    "        action_   = [v if k in f_moves else 0 for k,v in enumerate(action_)]\n",
    "\n",
    "        mid_point        = int((args['action_size']+1)/2)\n",
    "\n",
    "        action_idx_0     = np.argmax(action_[:mid_point])\n",
    "        action_idx_1     = np.argmax(action_[mid_point:]) + mid_point\n",
    "\n",
    "        action_sparse_0  = id_action_dict[action_idx_0]\n",
    "        action_sparse_1  = id_action_dict[action_idx_1]\n",
    "        #action_verbose_0 = (curr_board_ids[action_sparse_0[0]],action_sparse_0[1:])\n",
    "        \n",
    "        action_one_hot   = np.zeros(args['action_size'])\n",
    "        \n",
    "        if time_step % 2 == 0:\n",
    "            action_one_hot[action_idx_0]   = 1\n",
    "        else:\n",
    "            action_one_hot[action_idx_1]   = 1\n",
    "        \n",
    "        env.best_moves_sparse          = [action_sparse_0,action_sparse_1]\n",
    "        \n",
    "        \n",
    "        #############################################\n",
    "        #  SEPARATE OUT INTO A STANDALONE FUNCTION  #\n",
    "        #############################################\n",
    "        \n",
    "        \n",
    "        def get_verbose_action(sp_action,i):\n",
    "\n",
    "            act      = sp_action.split(\",\")\n",
    "            name     = id_names[int(act[0])]\n",
    "\n",
    "            curr_pos = curr_board_ids[name]\n",
    "            next_pos = zip(curr_pos,action)\n",
    "\n",
    "            move     = tuple([int(x) for x in act[1:]])\n",
    "\n",
    "            next_pos = tuple(sum(tuples) for tuples in zip(curr_pos,move))\n",
    "            \n",
    "            player   = env.team[i].players[name]\n",
    "                \n",
    "            moves    = [player,move, curr_pos,next_pos]\n",
    "            \n",
    "            return moves\n",
    "\n",
    "        \n",
    "        #############################################\n",
    "        # \n",
    "        #############################################\n",
    "        \n",
    "        \n",
    "        env.best_moves_verbose         = [get_verbose_action(action_sparse_0,0),get_verbose_action(action_sparse_1,1)]\n",
    "        \n",
    "        next_state, reward, done, info = env.step(cycle,user_input,args)\n",
    "        \n",
    "        next_state                     = np.array(list(map(int, env.state))).astype(np.float32)\n",
    "        \n",
    "        total_reward_agent             += reward[0]\n",
    "        total_reward_env               += reward[1]\n",
    "        \n",
    "        if userInput.teams[0][\"skill\"] > 1:\n",
    "            agent_p0.step(state, action_one_hot, int(reward[0]), next_state, done, mCritic)\n",
    "        \n",
    "        if userInput.teams[1][\"skill\"] > 1:\n",
    "            agent_p1.step(state, action_one_hot, int(reward[1]), next_state, done, mCritic)\n",
    "        \n",
    "        #num_iter = cycle*10 + time_step\n",
    "        #writer.add_scalar('Actor_loss', Agent_p0.actor_loss, num_iter)\n",
    "        #writer.add_scalar('Critic_loss', Agent_p0.mCriticLoss, num_iterp)\n",
    "        #writer.add_scalar('Total_reward_agent', total_reward_agent, num_iter)\n",
    "        #writer.add_scalar('Total_reward_env', total_reward_env , num_iter)\n",
    "\n",
    "        time_step                      += 1\n",
    "\n",
    "    scores.append((cycle, info[0],info[1]))\n",
    "    \n",
    "    stats_rewards_list.append((cycle, total_reward_agent, total_reward_env, time_step))\n",
    "    \n",
    "    if cycle % args['update_every'] == 0:\n",
    "            print(\"Agent scores: {}\".format(np.mean(agent_scores[-args['window_size']:], axis=0)))\n",
    "            \n",
    "    if time_step > args['window_size'] and cycle % args['update_every'] == 0:\n",
    "        agent_win_margin = np.mean(stats_rewards_list[-args['window_size']:],  axis=0)[1] - np.mean(stats_rewards_list[-args['window_size']:],  axis=0)[2]\n",
    "        print('Episode: {}'.format(cycle),\n",
    "        'Timestep: {}'.format(cycle),\n",
    "        'Total reward Agent: {}'.format(np.mean(stats_rewards_list[-args['window_size']:],  axis=0)[1]),\n",
    "        'Total reward Env: {}'.format(np.mean(stats_rewards_list[-args['window_size']:],  axis=0)[2]),\n",
    "        'Agent Win Margin: {}'.format(agent_win_margin),\n",
    "        'Agent Wins: {}'.format(args['WINS_DRAWS_LOSSES'][0]),\n",
    "        'Agent Draws: {}'.format(args['WINS_DRAWS_LOSSES'][1]),\n",
    "        'Agent Losses: {}'.format(args['WINS_DRAWS_LOSSES'][2]),\n",
    "        'Episode length: {}'.format(np.mean(stats_rewards_list[-args['window_size']:],axis=0)[3]),\n",
    "        'mCriticLoss: {}'.format(agent_p0.mCriticLoss),\n",
    "        'actorLoss: {}'.format(agent_p0.actorLoss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_trials(user_input):\n",
    "    \"\"\" Runs the num_trials \"\"\"\n",
    "    global cycle\n",
    "    global HISTORY_FILE\n",
    "    global SUMMARY_FILE\n",
    "    global env\n",
    "    global agent_scores\n",
    "    \n",
    "    print(\"==============================RUN_TRIALS==================================\")\n",
    "    \n",
    "    t = time.localtime()\n",
    "    timestamp = time.strftime('%b_%d_%Y_%H%M', t)\n",
    "    num_trials = user_input.num_trials\n",
    "    num_sides = user_input.num_sides\n",
    "    \n",
    "    #SUMMARY_FILE = (\"/data_data/reinforcement_learning/results/summary_file.tsv\")\n",
    "\n",
    "    #HISTORY_FILE = (\"/data_data/reinforcement_learning/results/history_file_\" + str(num_trials) + \"_trials_\" + str(num_sides) + \"_sides_\" + str(timestamp))\n",
    "    \n",
    "    \"\"\" Clear output file \"\"\"\n",
    "    \n",
    "    args['HISTORY_FILE'] = \"/data_data/reinforcement_learning/results/history_file_\" + str(num_trials) + \"_trials_\" + str(num_sides) + \"_sides_\" + str(timestamp)\n",
    "    \n",
    "    with open(args['HISTORY_FILE'], \"w\") as history_file:\n",
    "        history_file.write(\"\\n\")\n",
    "        \n",
    "        \n",
    "    for i in range(num_trials):\n",
    "        \n",
    "        sides = [x for x in range(num_sides)]\n",
    "        \n",
    "        env = Game(user_input.game, 8, sides, user_input.display_board_positions)\n",
    "\n",
    "        run_trial(user_input,env, mCritic, user_input)\n",
    "\n",
    "        cycle += 1\n",
    "        \n",
    "        agent_scores.append(env.last_reward)\n",
    "  \n",
    "        \n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from model import ActorNetwork, CriticNetwork, MCritic\n",
    "from replaybuffer import ReplayBuffer\n",
    "from args import args\n",
    "\n",
    "mCritic       =  MCritic(args['state_size'],args['action_size'])\n",
    "agent_p0      =  Agent(args['state_size'],args['action_size'], 0)\n",
    "agent_p1      =  Agent(args['state_size'],args['action_size'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CriticNetwork(\n",
      "  (fc1): Linear(in_features=472, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc3): Linear(in_features=4096, out_features=512, bias=True)\n",
      "  (fc4): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "ActorNetwork(\n",
      "  (fc1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc3): Linear(in_features=4096, out_features=512, bias=True)\n",
      "  (fc4): Linear(in_features=512, out_features=88, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(mCritic.network)\n",
    "print(agent_p0.actor_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select a game: ['chessy' or 'checkers']: chessy\n",
      "select number of teams:  [0, 1 or 2] 2\n",
      "Choose a name for this team [e.g. 'blue_team' or 'green_team']: Agent\n",
      "Choose a color for team_2 [e.g. 'blue': green\n",
      "Please enter team's skill_level [1 = Novice, 10 = expert]: 10\n",
      "Please enter team's strategy [0 = 'cooperative', 1 = 'competitive']: 1\n",
      "Choose a name for this team [e.g. 'blue_team' or 'green_team']: Environment\n",
      "Choose a color for team_2 [e.g. 'blue': red\n",
      "Please enter team's skill_level [1 = Novice, 10 = expert]: 1\n",
      "Please enter team's strategy [0 = 'cooperative', 1 = 'competitive']: 1\n",
      "How many trials would you like to run? [1 - 1,000,000] 5000\n",
      "Do you want to see the board positions in realtime? [ 'Yes' or 'No' ]no\n",
      "==============================RUN_TRIALS==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/chessy/model.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.fc4(x))\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent scores: nan\n",
      "Episode: 0 Timestep: 0 Total reward Agent: -151.0 Total reward Env: -171.0 Agent Win Margin: 20.0 Agent Wins: 2 Agent Draws: 0 Agent Losses: 0 Episode length: 162.0 mCriticLoss: 0 actorLoss: 0\n",
      "Agent scores: [ 4.4 -4.4]\n",
      "Episode: 100 Timestep: 100 Total reward Agent: -158.85 Total reward Env: -167.65 Agent Win Margin: 8.800000000000011 Agent Wins: 144 Agent Draws: 4 Agent Losses: 54 Episode length: 164.25 mCriticLoss: 0.011723075062036514 actorLoss: 0.622635006904602\n",
      "Agent scores: [ 2.7 -2.7]\n",
      "Episode: 200 Timestep: 200 Total reward Agent: -158.33 Total reward Env: -163.73 Agent Win Margin: 5.399999999999977 Agent Wins: 270 Agent Draws: 6 Agent Losses: 126 Episode length: 162.03 mCriticLoss: 0.3675180971622467 actorLoss: 1.1455078125\n",
      "Agent scores: [ 2.1 -2.1]\n",
      "Episode: 300 Timestep: 300 Total reward Agent: -161.71 Total reward Env: -165.51 Agent Win Margin: 3.799999999999983 Agent Wins: 388 Agent Draws: 8 Agent Losses: 206 Episode length: 164.61 mCriticLoss: 0.32453057169914246 actorLoss: -0.86316978931427\n",
      "Agent scores: [ 2.4 -2.4]\n",
      "Episode: 400 Timestep: 400 Total reward Agent: -159.57 Total reward Env: -164.77 Agent Win Margin: 5.200000000000017 Agent Wins: 510 Agent Draws: 16 Agent Losses: 276 Episode length: 163.17 mCriticLoss: 2.7374937534332275 actorLoss: -9.249543190002441\n",
      "Agent scores: [ 2.6 -2.6]\n",
      "Episode: 500 Timestep: 500 Total reward Agent: -161.04 Total reward Env: -166.24 Agent Win Margin: 5.200000000000017 Agent Wins: 632 Agent Draws: 24 Agent Losses: 346 Episode length: 164.64 mCriticLoss: 421.4664611816406 actorLoss: -0.012642443180084229\n",
      "Agent scores: [ 2.5 -2.5]\n",
      "Episode: 600 Timestep: 600 Total reward Agent: -159.98 Total reward Env: -164.98 Agent Win Margin: 5.0 Agent Wins: 754 Agent Draws: 30 Agent Losses: 418 Episode length: 163.48 mCriticLoss: 4451.193359375 actorLoss: -148.60821533203125\n",
      "Agent scores: [ 2.8 -2.8]\n",
      "Episode: 700 Timestep: 700 Total reward Agent: -161.12 Total reward Env: -166.52 Agent Win Margin: 5.400000000000006 Agent Wins: 878 Agent Draws: 36 Agent Losses: 488 Episode length: 164.82 mCriticLoss: 344048.96875 actorLoss: -973.9603881835938\n",
      "Agent scores: [ 4. -4.]\n",
      "Episode: 800 Timestep: 800 Total reward Agent: -159.62 Total reward Env: -167.42 Agent Win Margin: 7.799999999999983 Agent Wins: 1016 Agent Draws: 38 Agent Losses: 548 Episode length: 164.52 mCriticLoss: 5293232.5 actorLoss: -4333.3271484375\n",
      "Agent scores: [ 2.2 -2.2]\n",
      "Episode: 900 Timestep: 900 Total reward Agent: -160.3 Total reward Env: -164.7 Agent Win Margin: 4.399999999999977 Agent Wins: 1134 Agent Draws: 46 Agent Losses: 622 Episode length: 163.5 mCriticLoss: 75176824.0 actorLoss: -23346.11328125\n",
      "Agent scores: [ 3.5 -3.5]\n",
      "Episode: 1000 Timestep: 1000 Total reward Agent: -160.0 Total reward Env: -167.0 Agent Win Margin: 7.0 Agent Wins: 1266 Agent Draws: 52 Agent Losses: 684 Episode length: 164.5 mCriticLoss: 3848025856.0 actorLoss: -98997.203125\n",
      "Agent scores: [ 3.6 -3.6]\n",
      "Episode: 1100 Timestep: 1100 Total reward Agent: -160.67 Total reward Env: -168.27 Agent Win Margin: 7.600000000000023 Agent Wins: 1400 Agent Draws: 60 Agent Losses: 742 Episode length: 165.47 mCriticLoss: 332631776.0 actorLoss: -50833.8125\n",
      "Agent scores: [ 3.4 -3.4]\n",
      "Episode: 1200 Timestep: 1200 Total reward Agent: -158.29 Total reward Env: -165.09 Agent Win Margin: 6.800000000000011 Agent Wins: 1532 Agent Draws: 64 Agent Losses: 806 Episode length: 162.69 mCriticLoss: 577588494336.0 actorLoss: -1888341.5\n",
      "Agent scores: [ 3.3 -3.3]\n",
      "Episode: 1300 Timestep: 1300 Total reward Agent: -159.41 Total reward Env: -166.01 Agent Win Margin: 6.599999999999994 Agent Wins: 1664 Agent Draws: 66 Agent Losses: 872 Episode length: 163.71 mCriticLoss: 206462265589760.0 actorLoss: -19014520.0\n",
      "Agent scores: [ 3. -3.]\n",
      "Episode: 1400 Timestep: 1400 Total reward Agent: -161.13 Total reward Env: -166.73 Agent Win Margin: 5.599999999999994 Agent Wins: 1790 Agent Draws: 70 Agent Losses: 942 Episode length: 164.93 mCriticLoss: 4552368981016576.0 actorLoss: -241645760.0\n",
      "Agent scores: [ 3.6 -3.6]\n",
      "Episode: 1500 Timestep: 1500 Total reward Agent: -159.04 Total reward Env: -166.64 Agent Win Margin: 7.599999999999994 Agent Wins: 1924 Agent Draws: 78 Agent Losses: 1000 Episode length: 163.84 mCriticLoss: 4.521283897513738e+17 actorLoss: -1528524032.0\n",
      "Agent scores: [ 3.1 -3.1]\n",
      "Episode: 1600 Timestep: 1600 Total reward Agent: -160.58 Total reward Env: -166.78 Agent Win Margin: 6.199999999999989 Agent Wins: 2052 Agent Draws: 84 Agent Losses: 1066 Episode length: 164.68 mCriticLoss: 2.6337848866304426e+19 actorLoss: -4727250432.0\n",
      "Agent scores: [ 4.5 -4.5]\n",
      "Episode: 1700 Timestep: 1700 Total reward Agent: -158.06 Total reward Env: -166.66 Agent Win Margin: 8.599999999999994 Agent Wins: 2192 Agent Draws: 90 Agent Losses: 1120 Episode length: 163.36 mCriticLoss: 2.0165948195461965e+21 actorLoss: -46730625024.0\n",
      "Agent scores: [ 3.5 -3.5]\n",
      "Episode: 1800 Timestep: 1800 Total reward Agent: -160.23 Total reward Env: -167.23 Agent Win Margin: 7.0 Agent Wins: 2324 Agent Draws: 96 Agent Losses: 1182 Episode length: 164.73 mCriticLoss: 1.6113010272003552e+22 actorLoss: -79894298624.0\n",
      "Agent scores: [ 4.2 -4.2]\n",
      "Episode: 1900 Timestep: 1900 Total reward Agent: -159.73 Total reward Env: -168.13 Agent Win Margin: 8.400000000000006 Agent Wins: 2466 Agent Draws: 96 Agent Losses: 1240 Episode length: 164.93 mCriticLoss: 1.5514872568793688e+22 actorLoss: -74679287808.0\n",
      "Agent scores: [ 2.8 -2.8]\n",
      "Episode: 2000 Timestep: 2000 Total reward Agent: -159.43 Total reward Env: -165.43 Agent Win Margin: 6.0 Agent Wins: 2596 Agent Draws: 96 Agent Losses: 1310 Episode length: 163.43 mCriticLoss: 9.846802518521906e+20 actorLoss: -15018110976.0\n",
      "Agent scores: [ 2.6 -2.6]\n",
      "Episode: 2100 Timestep: 2100 Total reward Agent: -160.01 Total reward Env: -164.81 Agent Win Margin: 4.800000000000011 Agent Wins: 2718 Agent Draws: 100 Agent Losses: 1384 Episode length: 163.41 mCriticLoss: 1.4248715482495995e+23 actorLoss: -123595833344.0\n",
      "Agent scores: [ 2.6 -2.6]\n",
      "Episode: 2200 Timestep: 2200 Total reward Agent: -161.0 Total reward Env: -166.2 Agent Win Margin: 5.199999999999989 Agent Wins: 2840 Agent Draws: 108 Agent Losses: 1454 Episode length: 164.6 mCriticLoss: 3.23049470281967e+23 actorLoss: -321214545920.0\n",
      "Agent scores: [ 0.6 -0.6]\n",
      "Episode: 2300 Timestep: 2300 Total reward Agent: -162.71 Total reward Env: -163.91 Agent Win Margin: 1.1999999999999886 Agent Wins: 2944 Agent Draws: 112 Agent Losses: 1546 Episode length: 164.31 mCriticLoss: 1.309207400459694e+23 actorLoss: -138324262912.0\n",
      "Agent scores: [ 2.3 -2.3]\n",
      "Episode: 2400 Timestep: 2400 Total reward Agent: -162.54 Total reward Env: -167.14 Agent Win Margin: 4.599999999999994 Agent Wins: 3066 Agent Draws: 114 Agent Losses: 1622 Episode length: 165.84 mCriticLoss: 4.544473396868585e+22 actorLoss: -249337266176.0\n",
      "Agent scores: [ 3.3 -3.3]\n",
      "Episode: 2500 Timestep: 2500 Total reward Agent: -160.09 Total reward Env: -167.09 Agent Win Margin: 7.0 Agent Wins: 3200 Agent Draws: 116 Agent Losses: 1686 Episode length: 164.59 mCriticLoss: 1.8268695523449355e+24 actorLoss: -130183225344.0\n",
      "Agent scores: [ 2.5 -2.5]\n",
      "Episode: 2600 Timestep: 2600 Total reward Agent: -161.67 Total reward Env: -166.27 Agent Win Margin: 4.600000000000023 Agent Wins: 3322 Agent Draws: 118 Agent Losses: 1762 Episode length: 164.97 mCriticLoss: 2.5197645319457478e+23 actorLoss: -167925710848.0\n",
      "Agent scores: [ 3.3 -3.3]\n",
      "Episode: 2700 Timestep: 2700 Total reward Agent: -158.7 Total reward Env: -165.7 Agent Win Margin: 7.0 Agent Wins: 3452 Agent Draws: 128 Agent Losses: 1822 Episode length: 163.2 mCriticLoss: 1.9268225145323903e+24 actorLoss: -110516338688.0\n",
      "Agent scores: [ 2.4 -2.4]\n",
      "Episode: 2800 Timestep: 2800 Total reward Agent: -159.54 Total reward Env: -163.94 Agent Win Margin: 4.400000000000006 Agent Wins: 3572 Agent Draws: 132 Agent Losses: 1898 Episode length: 162.74 mCriticLoss: 1.6757241371643632e+24 actorLoss: -280125505536.0\n",
      "Agent scores: [ 2. -2.]\n",
      "Episode: 2900 Timestep: 2900 Total reward Agent: -160.88 Total reward Env: -165.28 Agent Win Margin: 4.400000000000006 Agent Wins: 3694 Agent Draws: 132 Agent Losses: 1976 Episode length: 164.08 mCriticLoss: 2.1426117295695106e+21 actorLoss: -84480049152.0\n",
      "Agent scores: [ 4.2 -4.2]\n",
      "Episode: 3000 Timestep: 3000 Total reward Agent: -155.57 Total reward Env: -163.57 Agent Win Margin: 8.0 Agent Wins: 3834 Agent Draws: 132 Agent Losses: 2036 Episode length: 160.57 mCriticLoss: 1.2708405146581843e+23 actorLoss: -197477531648.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent scores: [ 1.5 -1.5]\n",
      "Episode: 3100 Timestep: 3100 Total reward Agent: -161.53 Total reward Env: -164.93 Agent Win Margin: 3.4000000000000057 Agent Wins: 3946 Agent Draws: 142 Agent Losses: 2114 Episode length: 164.23 mCriticLoss: 6.507374320073435e+24 actorLoss: -96295518208.0\n",
      "Agent scores: [ 3.3 -3.3]\n",
      "Episode: 3200 Timestep: 3200 Total reward Agent: -159.0 Total reward Env: -165.6 Agent Win Margin: 6.599999999999994 Agent Wins: 4076 Agent Draws: 148 Agent Losses: 2178 Episode length: 163.3 mCriticLoss: 9.72928390099544e+22 actorLoss: -136598413312.0\n",
      "Agent scores: [ 3.7 -3.7]\n",
      "Episode: 3300 Timestep: 3300 Total reward Agent: -159.74 Total reward Env: -167.14 Agent Win Margin: 7.399999999999977 Agent Wins: 4208 Agent Draws: 158 Agent Losses: 2236 Episode length: 164.44 mCriticLoss: 5.474763056776074e+24 actorLoss: -244606451712.0\n",
      "Agent scores: [ 1. -1.]\n",
      "Episode: 3400 Timestep: 3400 Total reward Agent: -162.55 Total reward Env: -164.15 Agent Win Margin: 1.5999999999999943 Agent Wins: 4314 Agent Draws: 162 Agent Losses: 2326 Episode length: 164.35 mCriticLoss: 6.807360172116494e+23 actorLoss: -156315582464.0\n",
      "Agent scores: [ 3.3 -3.3]\n",
      "Episode: 3500 Timestep: 3500 Total reward Agent: -159.77 Total reward Env: -166.37 Agent Win Margin: 6.599999999999994 Agent Wins: 4444 Agent Draws: 168 Agent Losses: 2390 Episode length: 164.07 mCriticLoss: 6.993441702960039e+23 actorLoss: -406989242368.0\n",
      "Agent scores: [ 1.7 -1.7]\n",
      "Episode: 3600 Timestep: 3600 Total reward Agent: -161.42 Total reward Env: -164.82 Agent Win Margin: 3.4000000000000057 Agent Wins: 4560 Agent Draws: 170 Agent Losses: 2472 Episode length: 164.12 mCriticLoss: 8.552796860978415e+22 actorLoss: -275944243200.0\n",
      "Agent scores: [ 4.2 -4.2]\n",
      "Episode: 3700 Timestep: 3700 Total reward Agent: -159.13 Total reward Env: -167.93 Agent Win Margin: 8.800000000000011 Agent Wins: 4704 Agent Draws: 170 Agent Losses: 2528 Episode length: 164.53 mCriticLoss: 1.6044096010361544e+24 actorLoss: -251304755200.0\n",
      "Agent scores: [ 2.9 -2.9]\n",
      "Episode: 3800 Timestep: 3800 Total reward Agent: -158.32 Total reward Env: -164.12 Agent Win Margin: 5.800000000000011 Agent Wins: 4830 Agent Draws: 176 Agent Losses: 2596 Episode length: 162.22 mCriticLoss: 8.070964843325375e+22 actorLoss: -17402191872.0\n",
      "Agent scores: [ 3.7 -3.7]\n",
      "Episode: 3900 Timestep: 3900 Total reward Agent: -159.1 Total reward Env: -166.5 Agent Win Margin: 7.400000000000006 Agent Wins: 4966 Agent Draws: 178 Agent Losses: 2658 Episode length: 163.8 mCriticLoss: 8.313341693770224e+24 actorLoss: -270255390720.0\n",
      "Agent scores: [ 2.5 -2.5]\n",
      "Episode: 4000 Timestep: 4000 Total reward Agent: -159.4 Total reward Env: -164.4 Agent Win Margin: 5.0 Agent Wins: 5086 Agent Draws: 188 Agent Losses: 2728 Episode length: 162.9 mCriticLoss: 1.0708128129493506e+24 actorLoss: -219213971456.0\n",
      "Agent scores: [ 5.1 -5.1]\n",
      "Episode: 4100 Timestep: 4100 Total reward Agent: -158.16 Total reward Env: -168.36 Agent Win Margin: 10.200000000000017 Agent Wins: 5234 Agent Draws: 194 Agent Losses: 2774 Episode length: 164.26 mCriticLoss: 1.8529467632816974e+24 actorLoss: -149889220608.0\n",
      "Agent scores: [ 3.1 -3.1]\n",
      "Episode: 4200 Timestep: 4200 Total reward Agent: -159.72 Total reward Env: -165.52 Agent Win Margin: 5.800000000000011 Agent Wins: 5362 Agent Draws: 196 Agent Losses: 2844 Episode length: 163.62 mCriticLoss: 1.1802930140477181e+24 actorLoss: -192331825152.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from agent import Agent\n",
    "from model import ActorNetwork, CriticNetwork, MCritic\n",
    "from replaybuffer import ReplayBuffer\n",
    "from plotutils import plot_results\n",
    "\n",
    "from games  import games\n",
    "from games import games\n",
    "from game   import Game\n",
    "from player import Player_Template\n",
    "from team   import Team\n",
    "from userinput import userInput\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    global cycle\n",
    "    global HISTORY_FILE\n",
    "    user_input = userInput()\n",
    "    run_trials(user_input)\n",
    "    \n",
    "    \n",
    "\n",
    "#main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_board = []\n",
    "for i in [0,1]:\n",
    "    score_board.append([(x[0],x[i+1]) for x in scores])\n",
    "#score_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  team_scores = []\n",
    "  min_score = 99999999\n",
    "  max_score = 0\n",
    "\n",
    "  for j in range(user_input.num_sides):\n",
    "    team_scores.append([team_points for cycle, team_points in score_board[j]])\n",
    "    plot.hist(\n",
    "      team_scores[j],\n",
    "      bins=100,\n",
    "      label=user_input.teams[j][\"team_name\"],\n",
    "      color=user_input.teams[j][\"color\"])\n",
    "\n",
    "    min_j_score = min(team_scores[j])\n",
    "    max_j_score = max(team_scores[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins   = args['WINS_DRAWS_LOSSES'][0]\n",
    "draws  = args['WINS_DRAWS_LOSSES'][1]\n",
    "losses = args['WINS_DRAWS_LOSSES'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " args['WINS_DRAWS_LOSSES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.hist(wins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
