{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "import ipykernel\n",
    "import pandas as pd\n",
    "\n",
    "#pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('expand_frame_repr', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device  = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "import os\n",
    "import jsonlines\n",
    "import ast\n",
    "import time, os, fnmatch, shutil\n",
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import requests\n",
    "\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "HISTORY_FILE = None\n",
    "\n",
    "summary_dict = {\"losses\":0,\"wins\":0,\"draws\":0}\n",
    "\n",
    "cycle = 0\n",
    "\n",
    "actions = [[], []]\n",
    "rewards = [[], []]\n",
    "history = {\"cycle\": cycle, \"actions\": actions, \"rewards\": rewards, \"value\": 0}\n",
    "\n",
    "score_board = {}\n",
    "#step_action_dict = defaultdict()\n",
    "#step_action_dict['random_moves'] = defaultdict()\n",
    "#step_action_dict['policy_moves'] = defaultdict()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting numeric_names.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile numeric_names.py\n",
    "\n",
    "numeric_names = {'w_R0': 1, 'w_K0': 2, 'w_B0': 3, 'w__K': 4, 'w__Q': 5, 'w_B1': 6, 'w_K1': 7, 'w_R1': 8, 'w_P0': 9, 'w_P1':\n",
    " 10, 'w_P2': 11, 'w_P3': 12, 'w_P4': 13, 'w_P5': 14, 'w_P6': 15, 'w_P7': 16, 'b_P0': -16, 'b_P1': -15, 'b_P2': -14, 'b_P3':\n",
    " -13, 'b_P4': -12, 'b_P5': -11, 'b_P6': -10, 'b_P7': -9, 'b_R0': -8, 'b_K0': -7, 'b_B0': -6, 'b__K': -5, 'b__Q': -4, 'b_B1'\n",
    ": -3, 'b_K1': -2, 'b_R1': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1,1,0': 0, '2,1,-2': 1, '2,1,2': 2, '2,2,-1': 3, '2,2,1': 4, '3,1,-1': 5, '3,1,1': 6, '4,1,-1': 7, '4,1,0': 8, '4,1,1': 9, '5,1,-1': 10, '5,1,0': 11, '5,1,1': 12, '6,1,-1': 13, '6,1,1': 14, '7,1,-2': 15, '7,1,2': 16, '7,2,-1': 17, '7,2,1': 18, '8,1,0': 19, '9,1,0': 20, '9,1,-1': 21, '9,1,1': 22, '10,1,0': 23, '10,1,-1': 24, '10,1,1': 25, '11,1,0': 26, '11,1,-1': 27, '11,1,1': 28, '12,1,0': 29, '12,1,-1': 30, '12,1,1': 31, '13,1,0': 32, '13,1,-1': 33, '13,1,1': 34, '14,1,0': 35, '14,1,-1': 36, '14,1,1': 37, '15,1,0': 38, '15,1,-1': 39, '15,1,1': 40, '16,1,0': 41, '16,1,-1': 42, '16,1,1': 43, '-16,-1,0': 44, '-16,-1,-1': 45, '-16,-1,1': 46, '-15,-1,0': 47, '-15,-1,-1': 48, '-15,-1,1': 49, '-14,-1,0': 50, '-14,-1,-1': 51, '-14,-1,1': 52, '-13,-1,0': 53, '-13,-1,-1': 54, '-13,-1,1': 55, '-12,-1,0': 56, '-12,-1,-1': 57, '-12,-1,1': 58, '-11,-1,0': 59, '-11,-1,-1': 60, '-11,-1,1': 61, '-10,-1,0': 62, '-10,-1,-1': 63, '-10,-1,1': 64, '-9,-1,0': 65, '-9,-1,-1': 66, '-9,-1,1': 67, '-8,-1,0': 68, '-7,-1,-2': 69, '-7,-1,2': 70, '-7,-2,-1': 71, '-7,-2,1': 72, '-6,-1,-1': 73, '-6,-1,1': 74, '-5,-1,-1': 75, '-5,-1,0': 76, '-5,-1,1': 77, '-4,-1,-1': 78, '-4,-1,0': 79, '-4,-1,1': 80, '-3,-1,-1': 81, '-3,-1,1': 82, '-2,-1,-2': 83, '-2,-1,2': 84, '-2,-2,-1': 85, '-2,-2,1': 86, '-1,-1,0': 87}\n",
      "{0: '1,1,0', 1: '2,1,-2', 2: '2,1,2', 3: '2,2,-1', 4: '2,2,1', 5: '3,1,-1', 6: '3,1,1', 7: '4,1,-1', 8: '4,1,0', 9: '4,1,1', 10: '5,1,-1', 11: '5,1,0', 12: '5,1,1', 13: '6,1,-1', 14: '6,1,1', 15: '7,1,-2', 16: '7,1,2', 17: '7,2,-1', 18: '7,2,1', 19: '8,1,0', 20: '9,1,0', 21: '9,1,-1', 22: '9,1,1', 23: '10,1,0', 24: '10,1,-1', 25: '10,1,1', 26: '11,1,0', 27: '11,1,-1', 28: '11,1,1', 29: '12,1,0', 30: '12,1,-1', 31: '12,1,1', 32: '13,1,0', 33: '13,1,-1', 34: '13,1,1', 35: '14,1,0', 36: '14,1,-1', 37: '14,1,1', 38: '15,1,0', 39: '15,1,-1', 40: '15,1,1', 41: '16,1,0', 42: '16,1,-1', 43: '16,1,1', 44: '-16,-1,0', 45: '-16,-1,-1', 46: '-16,-1,1', 47: '-15,-1,0', 48: '-15,-1,-1', 49: '-15,-1,1', 50: '-14,-1,0', 51: '-14,-1,-1', 52: '-14,-1,1', 53: '-13,-1,0', 54: '-13,-1,-1', 55: '-13,-1,1', 56: '-12,-1,0', 57: '-12,-1,-1', 58: '-12,-1,1', 59: '-11,-1,0', 60: '-11,-1,-1', 61: '-11,-1,1', 62: '-10,-1,0', 63: '-10,-1,-1', 64: '-10,-1,1', 65: '-9,-1,0', 66: '-9,-1,-1', 67: '-9,-1,1', 68: '-8,-1,0', 69: '-7,-1,-2', 70: '-7,-1,2', 71: '-7,-2,-1', 72: '-7,-2,1', 73: '-6,-1,-1', 74: '-6,-1,1', 75: '-5,-1,-1', 76: '-5,-1,0', 77: '-5,-1,1', 78: '-4,-1,-1', 79: '-4,-1,0', 80: '-4,-1,1', 81: '-3,-1,-1', 82: '-3,-1,1', 83: '-2,-1,-2', 84: '-2,-1,2', 85: '-2,-2,-1', 86: '-2,-2,1', 87: '-1,-1,0'}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from games import games\n",
    "from numeric_names import numeric_names\n",
    "\n",
    "Pieces_detail = defaultdict()\n",
    "actions_array = []\n",
    "sparse_action_dict = defaultdict()\n",
    "bin_action_dict = defaultdict()\n",
    "actions = []\n",
    "\n",
    "for piece in numeric_names.keys():\n",
    "    action_sparse = []\n",
    "    if piece[0] == 'w':\n",
    "        idx = 0\n",
    "    else:\n",
    "        idx = 1\n",
    "        \n",
    "    if piece[1:3] == '_R':\n",
    "        player_id = 'Rook'\n",
    "    elif piece[1:3] == '_K':\n",
    "        player_id = 'Knight'\n",
    "    elif piece[1:3] == '_B':\n",
    "        player_id = 'Bishop'\n",
    "    elif piece[1:3] == '_P':\n",
    "        player_id = 'Pawn'\n",
    "    elif piece[1:4] == '__K':\n",
    "        player_id = 'King'\n",
    "    elif piece[:3] == '__Q':\n",
    "        player_id = 'Queen'\n",
    "        \n",
    "    actions = games['chessy']['players'][player_id]['moves']\n",
    "    \n",
    "    actions = actions[idx]\n",
    "    \n",
    "    #action_diff = 4 - len(actions)\n",
    "    \n",
    "    #for i in range(action_diff):\n",
    "    #    actions.append((0,0))\n",
    "    \n",
    "    piece_num_id = numeric_names[piece]\n",
    "    \n",
    "    piece_bin_id = '{0:06b}'.format(piece_num_id + 17)\n",
    "    \n",
    "    actions_ = []\n",
    "    \n",
    "    for action in actions:\n",
    "        action_value_  = (8*action[0] + action[1] + 17)\n",
    "        action_value   = '{0:06b}'.format(action_value_)\n",
    "        actions_.append(action_value)\n",
    "        \n",
    "        piece_move_bin = (str(piece_bin_id)+str(action_value))\n",
    "        \n",
    "        actions_array.append(piece_move_bin)\n",
    "        \n",
    "        sparse_action  = str(piece_num_id)+\",\"+str(action[0])+ \",\" +str(action[1])\n",
    "        \n",
    "        action_sparse.append(sparse_action)\n",
    "        \n",
    "        sparse_action_dict[sparse_action] = piece_move_bin\n",
    "    \n",
    "    #print(piece_num_id, \"\\t\",piece_bin_id, \"\\t\", action_sparse,  \"\\t\", actions, \"\\t\", actions_)\n",
    "    \n",
    "    Pieces_detail[piece_num_id] = {\n",
    "        'piece_num_id':piece_num_id,\n",
    "        'piece_bin_id':piece_bin_id,\n",
    "        'action_sparse':action_sparse,\n",
    "        'actions_verbose': actions,\n",
    "        'actions_bin': actions_\n",
    "       }\n",
    "    \n",
    "action_ids = np.arange(0,len(sparse_action_dict.keys()),1)\n",
    "#print(action_ids)\n",
    "\n",
    "\n",
    "action_id_dict = {y:x for x,y in zip(action_ids,sparse_action_dict.keys())}\n",
    "print(action_id_dict) \n",
    "\n",
    "id_action_dict = {x:y for x,y in zip(action_ids,sparse_action_dict.keys())}\n",
    "print(id_action_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting args.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile args.py\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numeric_names import numeric_names\n",
    "\n",
    "args = { \n",
    "    \"update_every\":100,\n",
    "    \"window_size\":100,\n",
    "    \"BUFFER_SIZE\":int(1e6),\n",
    "    \"BATCH_SIZE\":1024,  \n",
    "    \"GAMMA\":0.99,\n",
    "    \"TAU\":2e-3,\n",
    "    \"LR_ACTOR\":1e-3,\n",
    "    \"LR_CRITIC\":1.1e-3,\n",
    "    \"WEIGHT_DECAY\":0.0001,\n",
    "    \"UPDATE_EVERY\":5,\n",
    "    \"EXPLORE_NOISE\":0.05,\n",
    "    \"FC1_UNITS\":1024,\n",
    "    \"FC2_UNITS\":10240,\n",
    "    \"FC3_UNITS\":1024,\n",
    "    \"seed\":0,\n",
    "    \"state_size\":384,\n",
    "    \"action_size\":88,\n",
    "    \"action_size_binary\":12,\n",
    "    \"num_agents\":2,\n",
    "    \"device\":torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cuda:0\"),\n",
    "    'mcritic_path':'/home/ubuntu/chessy/checkpoint_mCritic.pth',\n",
    "    'agent_p0_path':'/home/ubuntu/chessy/checkpoint_p0.pth',\n",
    "    'agent_p1_path':'/home/ubuntu/chessy/checkpoint_p1.pth',\n",
    "    'action_id_dict': {'1,1,0': 0, '2,1,-2': 1, '2,1,2': 2, '2,2,-1': 3, '2,2,1': 4, '3,1,-1': 5, '3,1,1': 6, '4,1,-1': 7, '4,1,0': 8, '4,1,1': 9, '5,1,-1': 10, '5,1,0': 11, '5,1,1': 12, '6,1,-1': 13, '6,1,1': 14, '7,1,-2': 15, '7,1,2': 16, '7,2,-1': 17, '7,2,1': 18, '8,1,0': 19, '9,1,0': 20, '9,1,-1': 21, '9,1,1': 22, '10,1,0': 23, '10,1,-1': 24, '10,1,1': 25, '11,1,0': 26, '11,1,-1': 27, '11,1,1': 28, '12,1,0': 29, '12,1,-1': 30, '12,1,1': 31, '13,1,0': 32, '13,1,-1': 33, '13,1,1': 34, '14,1,0': 35, '14,1,-1': 36, '14,1,1': 37, '15,1,0': 38, '15,1,-1': 39, '15,1,1': 40, '16,1,0': 41, '16,1,-1': 42, '16,1,1': 43, '-16,-1,0': 44, '-16,-1,-1': 45, '-16,-1,1': 46, '-15,-1,0': 47, '-15,-1,-1': 48, '-15,-1,1': 49, '-14,-1,0': 50, '-14,-1,-1': 51, '-14,-1,1': 52, '-13,-1,0': 53, '-13,-1,-1': 54, '-13,-1,1': 55, '-12,-1,0': 56, '-12,-1,-1': 57, '-12,-1,1': 58, '-11,-1,0': 59, '-11,-1,-1': 60, '-11,-1,1': 61, '-10,-1,0': 62, '-10,-1,-1': 63, '-10,-1,1': 64, '-9,-1,0': 65, '-9,-1,-1': 66, '-9,-1,1': 67, '-8,-1,0': 68, '-7,-1,-2': 69, '-7,-1,2': 70, '-7,-2,-1': 71, '-7,-2,1': 72, '-6,-1,-1': 73, '-6,-1,1': 74, '-5,-1,-1': 75, '-5,-1,0': 76, '-5,-1,1': 77, '-4,-1,-1': 78, '-4,-1,0': 79, '-4,-1,1': 80, '-3,-1,-1': 81, '-3,-1,1': 82, '-2,-1,-2': 83, '-2,-1,2': 84, '-2,-2,-1': 85, '-2,-2,1': 86, '-1,-1,0': 87},\n",
    "    'id_action_dict': {0: '1,1,0', 1: '2,1,-2', 2: '2,1,2', 3: '2,2,-1', 4: '2,2,1', 5: '3,1,-1', 6: '3,1,1', 7: '4,1,-1', 8: '4,1,0', 9: '4,1,1', 10: '5,1,-1', 11: '5,1,0', 12: '5,1,1', 13: '6,1,-1', 14: '6,1,1', 15: '7,1,-2', 16: '7,1,2', 17: '7,2,-1', 18: '7,2,1', 19: '8,1,0', 20: '9,1,0', 21: '9,1,-1', 22: '9,1,1', 23: '10,1,0', 24: '10,1,-1', 25: '10,1,1', 26: '11,1,0', 27: '11,1,-1', 28: '11,1,1', 29: '12,1,0', 30: '12,1,-1', 31: '12,1,1', 32: '13,1,0', 33: '13,1,-1', 34: '13,1,1', 35: '14,1,0', 36: '14,1,-1', 37: '14,1,1', 38: '15,1,0', 39: '15,1,-1', 40: '15,1,1', 41: '16,1,0', 42: '16,1,-1', 43: '16,1,1', 44: '-16,-1,0', 45: '-16,-1,-1', 46: '-16,-1,1', 47: '-15,-1,0', 48: '-15,-1,-1', 49: '-15,-1,1', 50: '-14,-1,0', 51: '-14,-1,-1', 52: '-14,-1,1', 53: '-13,-1,0', 54: '-13,-1,-1', 55: '-13,-1,1', 56: '-12,-1,0', 57: '-12,-1,-1', 58: '-12,-1,1', 59: '-11,-1,0', 60: '-11,-1,-1', 61: '-11,-1,1', 62: '-10,-1,0', 63: '-10,-1,-1', 64: '-10,-1,1', 65: '-9,-1,0', 66: '-9,-1,-1', 67: '-9,-1,1', 68: '-8,-1,0', 69: '-7,-1,-2', 70: '-7,-1,2', 71: '-7,-2,-1', 72: '-7,-2,1', 73: '-6,-1,-1', 74: '-6,-1,1', 75: '-5,-1,-1', 76: '-5,-1,0', 77: '-5,-1,1', 78: '-4,-1,-1', 79: '-4,-1,0', 80: '-4,-1,1', 81: '-3,-1,-1', 82: '-3,-1,1', 83: '-2,-1,-2', 84: '-2,-1,2', 85: '-2,-2,-1', 86: '-2,-2,1', 87: '-1,-1,0'},\n",
    "    'numeric_names' : numeric_names,\n",
    "    'initial_state'  : \"010010010011010100010101010110010111011000011001011010011011011100011101011110011111100000100001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000010000011000100000101000110000111001000001001001010001011001100001101001110001111010000\",\n",
    "    'SUMMARY_FILE':\"/data_data/reinforcement_learning/results/summary_file.tsv\",\n",
    "    'HISTORY_FILE':None,\n",
    "    'TAKE_KING_REWARD':10,\n",
    "    'MORE_POINTS_REWARD':1,\n",
    "    'EQUAL_POINTS_REWARD':0,\n",
    "    'STEP_REWARD':-1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sparse_action_dict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sparse_action_dict.py\n",
    "\n",
    "sparse_action_dict = {'1,1,0':'010010011001','2,1,-2':'010011010111','2,1,2':'010011011011','2,2,-1':'010011100000','2,2,1':'010011100010','3,1,-1':'010100011000','3,1,1':'010100011010','4,1,-1':'010101011000','4,1,0':'010101011001','4,1,1':'010101011010','5,1,-1':'010110011000','5,1,0':'010110011001','5,1,1':'010110011010','6,1,-1':'010111011000','6,1,1':'010111011010','7,1,-2':'011000010111','7,1,2':'011000011011','7,2,-1':'011000100000','7,2,1':'011000100010','8,1,0':'011001011001','9,1,0':'011010011001','9,1,-1':'011010011000','9,1,1':'011010011010','10,1,0':'011011011001','10,1,-1':'011011011000','10,1,1':'011011011010','11,1,0':'011100011001','11,1,-1':'011100011000','11,1,1':'011100011010','12,1,0':'011101011001','12,1,-1':'011101011000','12,1,1':'011101011010','13,1,0':'011110011001','13,1,-1':'011110011000','13,1,1':'011110011010','14,1,0':'011111011001','14,1,-1':'011111011000','14,1,1':'011111011010','15,1,0':'100000011001','15,1,-1':'100000011000','15,1,1':'100000011010','16,1,0':'100001011001','16,1,-1':'100001011000','16,1,1':'100001011010','-16,-1,0':'000001001001','-16,-1,-1':'000001001000','-16,-1,1':'000001001010','-15,-1,0':'000010001001','-15,-1,-1':'000010001000','-15,-1,1':'000010001010','-14,-1,0':'000011001001','-14,-1,-1':'000011001000','-14,-1,1':'000011001010','-13,-1,0':'000100001001','-13,-1,-1':'000100001000','-13,-1,1':'000100001010','-12,-1,0':'000101001001','-12,-1,-1':'000101001000','-12,-1,1':'000101001010','-11,-1,0':'000110001001','-11,-1,-1':'000110001000','-11,-1,1':'000110001010','-10,-1,0':'000111001001','-10,-1,-1':'000111001000','-10,-1,1':'000111001010','-9,-1,0':'001000001001','-9,-1,-1':'001000001000','-9,-1,1':'001000001010','-8,-1,0':'001001001001','-7,-1,-2':'001010000111','-7,-1,2':'001010001011','-7,-2,-1':'001010000000','-7,-2,1':'001010000010','-6,-1,-1':'001011001000','-6,-1,1':'001011001010','-5,-1,-1':'001100001000','-5,-1,0':'001100001001','-5,-1,1':'001100001010','-4,-1,-1':'001101001000','-4,-1,0':'001101001001','-4,-1,1':'001101001010','-3,-1,-1':'001110001000','-3,-1,1':'001110001010','-2,-1,-2':'001111000111','-2,-1,2':'001111001011','-2,-2,-1':'001111000000','-2,-2,1':'001111000010','-1,-1,0':'010000001001'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pieces = {\n",
    "    'Pawn':(\"w_P\",\"b_P\"),\n",
    "    'Knight':(\"w_K\",\"b_K\"),\n",
    "    'King':(\"w__K\",\"b__K\"),\n",
    "    'Queen':(\"w__Q\",\"b__Q\"),\n",
    "    'Bishop':(\"w_B\",\"b_B\"),\n",
    "    'Rook':(\"w_R\",\"b_R\"),\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def getBin(num):\n",
    "        if int(num) != 0:\n",
    "            return \"{0:{fill}6b}\".format(int(num)+17, fill='0')\n",
    "        else:\n",
    "            return \"{0:{fill}6b}\".format(0, fill='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse_action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_moves   = 0\n",
    "policy_moves = 0\n",
    "random_moves = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFunc(piece):\n",
    "    def piece_func():\n",
    "        return piece\n",
    "    return piece_func()\n",
    "\n",
    "def Bishop():\n",
    "    return getFunc(\"Bishop\")\n",
    "\n",
    "def Pawn():\n",
    "    return getFunc(\"Pawn\")\n",
    "\n",
    "def King():\n",
    "    return getFunc(\"King\")\n",
    "\n",
    "def Queen():\n",
    "    return getFunc(\"Queen\")\n",
    "\n",
    "def Knight():\n",
    "    return getFunc(\"Knight\")\n",
    "\n",
    "def Rook():\n",
    "    return getFunc(\"Rook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pawn'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pawn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Incorrect_Input_error(Exception):\n",
    "  \"\"\"Generic input error handler: raised in the case that any of the user inputed data is incorrect\"\"\"\n",
    "  pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp0  = None\n",
    "temp1  = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from args import args\n",
    "from sparse_action_dict import sparse_action_dict\n",
    "from numeric_names import numeric_names\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "#from tensorboardX import SummaryWriter\n",
    "import sys\n",
    "\n",
    "#sys.path.append('./anaconda3/lib/python3.7/site-packages/torchvision')\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#writer = SummaryWriter(comment=\"MADDPG Chessy\")\n",
    "\n",
    "id_names = {value:key for key,value in numeric_names.items()}\n",
    "names    = {value: key for key in Pieces for value in Pieces[key]}\n",
    "\n",
    "stats_rewards_list = []\n",
    "\n",
    "def run_trial(user_input, env, mCritic, userInput):\n",
    "    global temp0\n",
    "    global temp1\n",
    "    global cycle\n",
    "    global scores\n",
    "    global total_reward_agent\n",
    "    global total_reward_env\n",
    "    \n",
    "    for j in range(user_input.num_sides):\n",
    "        team_name = \"team_\" + str(j)\n",
    "        team_name = Team(user_input.game, user_input.teams[j][\"team_name\"], j,user_input.teams[j][\"skill\"] / 10, user_input.teams[j][\"strategy\"])\n",
    "\n",
    "        env.insert_team(team_name)\n",
    "            \n",
    "    env.not_deadlocked  = True\n",
    "    env.states.append(args['initial_state'])\n",
    "    time_step    = 0\n",
    "        \n",
    "    total_reward_agent = 0\n",
    "    \n",
    "    total_reward_env   = 0\n",
    "        \n",
    "    while env.not_deadlocked:\n",
    "         \n",
    "        curr_board_ids = {value:key for key,value in env.board.items()}\n",
    "        state   = env.state \n",
    "        state   = np.array(list(map(int, state))).astype(np.float32)\n",
    "        \n",
    "        #state_prior_tensor = torch.IntTensor(state_prior_bin).to(device)\n",
    "        action_ = agent_p0.act(state)\n",
    "        \n",
    "\n",
    "        #Determine feasible moves for each team:\n",
    "        feasible_moves     = []\n",
    "        feasible_moves_0   = env.get_feasible_moves(env.team[0])\n",
    "        feasible_moves_1   = env.get_feasible_moves(env.team[1])\n",
    "        [feasible_moves.append(x) for x in feasible_moves_0]\n",
    "        [feasible_moves.append(x) for x in feasible_moves_1]\n",
    "        \n",
    "        f_moves   = set([int(x) for x in [action_id_dict[str((args['numeric_names'][env.board[curr_pos]],move)).replace(\"(\",\"\").replace(\")\",\"\").replace(\" \",\"\")] for name, move, curr_pos, new_pos in feasible_moves]])\n",
    "        \n",
    "        #f_moves  = set(np.sort(f_moves, axis=-1, kind='quicksort', order=None))\n",
    "        \n",
    "        action_   = [v if k in f_moves else 0 for k,v in enumerate(action_)]\n",
    "\n",
    "        mid_point        = int((args['action_size']+1)/2)\n",
    "\n",
    "        action_idx_0     = np.argmax(action_[:mid_point])\n",
    "        action_idx_1     = np.argmax(action_[mid_point:]) + mid_point\n",
    "\n",
    "        action_sparse_0  = id_action_dict[action_idx_0]\n",
    "        action_sparse_1  = id_action_dict[action_idx_1]\n",
    "        #action_verbose_0 = (curr_board_ids[action_sparse_0[0]],action_sparse_0[1:])\n",
    "        \n",
    "        action_one_hot   = np.zeros(args['action_size'])\n",
    "        \n",
    "        if time_step % 2 == 0:\n",
    "            action_one_hot[action_idx_0]   = 1\n",
    "        else:\n",
    "            action_one_hot[action_idx_1]   = 1\n",
    "        \n",
    "        env.best_moves_sparse          = [action_sparse_0,action_sparse_1]\n",
    "        \n",
    "        \n",
    "        #############################################\n",
    "        #  SEPARATE OUT INTO A STANDALONE FUNCTION  #\n",
    "        #############################################\n",
    "        \n",
    "        \n",
    "        def get_verbose_action(sp_action,i):\n",
    "\n",
    "            act      = sp_action.split(\",\")\n",
    "            name     = id_names[int(act[0])]\n",
    "\n",
    "            curr_pos = curr_board_ids[name]\n",
    "            next_pos = zip(curr_pos,action)\n",
    "\n",
    "            move     = tuple([int(x) for x in act[1:]])\n",
    "\n",
    "            next_pos = tuple(sum(tuples) for tuples in zip(curr_pos,move))\n",
    "            \n",
    "            player   = env.team[i].players[name]\n",
    "                \n",
    "            moves    = [player,move, curr_pos,next_pos]\n",
    "            \n",
    "            return moves\n",
    "\n",
    "        \n",
    "        #############################################\n",
    "        # \n",
    "        #############################################\n",
    "        \n",
    "        \n",
    "        env.best_moves_verbose         = [get_verbose_action(action_sparse_0,0),get_verbose_action(action_sparse_1,1)]\n",
    "        \n",
    "        next_state, reward, done, info = env.step(cycle,user_input,args)\n",
    "        \n",
    "        next_state                     = np.array(list(map(int, env.state))).astype(np.float32)\n",
    "        \n",
    "        total_reward_agent             += reward[0]\n",
    "        total_reward_env               += reward[1]\n",
    "        \n",
    "        if userInput.teams[0][\"skill\"] > 1:\n",
    "            agent_p0.step(state, action_one_hot, int(reward[0]), next_state, done, mCritic)\n",
    "        \n",
    "        if userInput.teams[1][\"skill\"] > 1:\n",
    "            agent_p1.step(state, action_one_hot, int(reward[1]), next_state, done, mCritic)\n",
    "        \n",
    "        #num_iter = cycle*10 + time_step\n",
    "        #writer.add_scalar('Actor_loss', Agent_p0.actor_loss, num_iter)\n",
    "        #writer.add_scalar('Critic_loss', Agent_p0.mCriticLoss, num_iterp)\n",
    "        #writer.add_scalar('Total_reward_agent', total_reward_agent, num_iter)\n",
    "        #writer.add_scalar('Total_reward_env', total_reward_env , num_iter)\n",
    "\n",
    "        time_step                      += 1\n",
    "\n",
    "    scores.append((cycle, info[0],info[1]))\n",
    "    \n",
    "    stats_rewards_list.append((cycle, total_reward_agent, total_reward_env, time_step))\n",
    "    \n",
    "    if cycle % args['update_every'] == 0:\n",
    "            print(\"Agent scores: {}\".format(np.mean(agent_scores[-args['window_size']:], axis=0)))\n",
    "            \n",
    "    if time_step > args['window_size'] and cycle % args['update_every'] == 0:\n",
    "        agent_win_margin = np.mean(stats_rewards_list[-args['window_size']:],  axis=0)[1] - np.mean(stats_rewards_list[-args['window_size']:],  axis=0)[2]\n",
    "        print('Episode: {}'.format(cycle),\n",
    "        'Timestep: {}'.format(cycle),\n",
    "        'Total reward Agent: {}'.format(np.mean(stats_rewards_list[-args['window_size']:],  axis=0)[1]),\n",
    "        'Total reward Env: {}'.format(np.mean(stats_rewards_list[-args['window_size']:],  axis=0)[2]),\n",
    "        'Agent Win Margin: {}'.format(agent_win_margin),\n",
    "        'Episode length: {}'.format(np.mean(stats_rewards_list[-args['window_size']:],axis=0)[3]),\n",
    "        'mCriticLoss: {}'.format(agent_p0.mCriticLoss),\n",
    "        'actorLoss: {}'.format(agent_p0.actorLoss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_trials(user_input):\n",
    "    \"\"\" Runs the num_trials \"\"\"\n",
    "    global cycle\n",
    "    global HISTORY_FILE\n",
    "    global SUMMARY_FILE\n",
    "    global env\n",
    "    global agent_scores\n",
    "    \n",
    "    print(\"==============================RUN_TRIALS==================================\")\n",
    "    \n",
    "    t = time.localtime()\n",
    "    timestamp = time.strftime('%b_%d_%Y_%H%M', t)\n",
    "    num_trials = user_input.num_trials\n",
    "    num_sides = user_input.num_sides\n",
    "    \n",
    "    #SUMMARY_FILE = (\"/data_data/reinforcement_learning/results/summary_file.tsv\")\n",
    "\n",
    "    #HISTORY_FILE = (\"/data_data/reinforcement_learning/results/history_file_\" + str(num_trials) + \"_trials_\" + str(num_sides) + \"_sides_\" + str(timestamp))\n",
    "    \n",
    "    \"\"\" Clear output file \"\"\"\n",
    "    \n",
    "    args['HISTORY_FILE'] = \"/data_data/reinforcement_learning/results/history_file_\" + str(num_trials) + \"_trials_\" + str(num_sides) + \"_sides_\" + str(timestamp)\n",
    "    \n",
    "    with open(args['HISTORY_FILE'], \"w\") as history_file:\n",
    "        history_file.write(\"\\n\")\n",
    "        \n",
    "        \n",
    "    for i in range(num_trials):\n",
    "        \n",
    "        sides = [x for x in range(num_sides)]\n",
    "        \n",
    "        env = Game(user_input.game, 8, sides, user_input.display_board_positions)\n",
    "\n",
    "        run_trial(user_input,env, mCritic, user_input)\n",
    "\n",
    "        cycle += 1\n",
    "        \n",
    "        agent_scores.append(env.last_reward)\n",
    "  \n",
    "        \n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from model import ActorNetwork, CriticNetwork, MCritic\n",
    "from replaybuffer import ReplayBuffer\n",
    "from args import args\n",
    "\n",
    "mCritic       =  MCritic(args['state_size'],args['action_size'])\n",
    "agent_p0      =  Agent(args['state_size'],args['action_size'], 0)\n",
    "agent_p1      =  Agent(args['state_size'],args['action_size'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CriticNetwork(\n",
      "  (fc1): Linear(in_features=472, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=10240, bias=True)\n",
      "  (fc3): Linear(in_features=10240, out_features=1024, bias=True)\n",
      "  (fc4): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n",
      "ActorNetwork(\n",
      "  (fc1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=10240, bias=True)\n",
      "  (fc3): Linear(in_features=10240, out_features=1024, bias=True)\n",
      "  (fc4): Linear(in_features=1024, out_features=88, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(mCritic.network)\n",
    "print(agent_p0.actor_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select a game: ['chessy' or 'checkers']: chessy\n",
      "select number of teams:  [0, 1 or 2] 2\n",
      "Choose a name for this team [e.g. 'blue_team' or 'green_team']: Agent\n",
      "Choose a color for team_2 [e.g. 'blue': green\n",
      "Please enter team's skill_level [1 = Novice, 10 = expert]: 10\n",
      "Please enter team's strategy [0 = 'cooperative', 1 = 'competitive']: 1\n",
      "Choose a name for this team [e.g. 'blue_team' or 'green_team']: Environment\n",
      "Choose a color for team_2 [e.g. 'blue': red\n",
      "Please enter team's skill_level [1 = Novice, 10 = expert]: 1\n",
      "Please enter team's strategy [0 = 'cooperative', 1 = 'competitive']: 1\n",
      "How many trials would you like to run? [1 - 1,000,000] 5000\n",
      "Do you want to see the board positions in realtime? [ 'Yes' or 'No' ]no\n",
      "==============================RUN_TRIALS==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/chessy/model.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.fc4(x))\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent scores: nan\n",
      "Episode: 0 Timestep: 0 Total reward Agent: -159.0 Total reward Env: -161.0 Agent Win Margin: 2.0 Episode length: 161.0 mCriticLoss: 0 actorLoss: 0\n",
      "Agent scores: [ 0.07 -0.07]\n",
      "Agent scores: [ 0.23 -0.23]\n",
      "Episode: 200 Timestep: 200 Total reward Agent: -163.66 Total reward Env: -164.08 Agent Win Margin: 0.4200000000000159 Episode length: 164.87 mCriticLoss: 0.0022790415678173304 actorLoss: 0.9898063540458679\n",
      "Agent scores: [ 0.35 -0.35]\n",
      "Episode: 300 Timestep: 300 Total reward Agent: -161.47 Total reward Env: -162.21 Agent Win Margin: 0.7400000000000091 Episode length: 162.84 mCriticLoss: 0.004388684872537851 actorLoss: 1.0093122720718384\n",
      "Agent scores: [ 0.24 -0.24]\n",
      "Episode: 400 Timestep: 400 Total reward Agent: -160.14 Total reward Env: -160.62 Agent Win Margin: 0.4800000000000182 Episode length: 161.38 mCriticLoss: 0.07124298810958862 actorLoss: 0.9943101406097412\n",
      "Agent scores: [ 0.25 -0.25]\n",
      "Episode: 500 Timestep: 500 Total reward Agent: -163.81 Total reward Env: -164.27 Agent Win Margin: 0.46000000000000796 Episode length: 165.04 mCriticLoss: 0.01297137700021267 actorLoss: 0.9472247362136841\n",
      "Agent scores: [ 0.27 -0.27]\n",
      "Episode: 600 Timestep: 600 Total reward Agent: -159.69 Total reward Env: -160.27 Agent Win Margin: 0.5800000000000125 Episode length: 160.98 mCriticLoss: 0.02733556553721428 actorLoss: 0.9550343155860901\n",
      "Agent scores: [ 0.16 -0.16]\n",
      "Episode: 700 Timestep: 700 Total reward Agent: -163.82 Total reward Env: -164.1 Agent Win Margin: 0.28000000000000114 Episode length: 164.96 mCriticLoss: 32786.765625 actorLoss: 101.98360443115234\n",
      "Agent scores: [ 0.17 -0.17]\n",
      "Episode: 800 Timestep: 800 Total reward Agent: -163.88 Total reward Env: -164.26 Agent Win Margin: 0.37999999999999545 Episode length: 165.07 mCriticLoss: 0.5892418622970581 actorLoss: 1.1377581357955933\n",
      "Agent scores: [ 0.2 -0.2]\n",
      "Episode: 900 Timestep: 900 Total reward Agent: -162.47 Total reward Env: -162.87 Agent Win Margin: 0.4000000000000057 Episode length: 163.67 mCriticLoss: 191.9108123779297 actorLoss: -3.272446632385254\n",
      "Agent scores: [ 0.14 -0.14]\n",
      "Episode: 1000 Timestep: 1000 Total reward Agent: -163.61 Total reward Env: -163.89 Agent Win Margin: 0.2799999999999727 Episode length: 164.75 mCriticLoss: 6604.93408203125 actorLoss: 2.4840407371520996\n",
      "Agent scores: [ 0.3 -0.3]\n",
      "Episode: 1100 Timestep: 1100 Total reward Agent: -161.17 Total reward Env: -161.77 Agent Win Margin: 0.6000000000000227 Episode length: 162.47 mCriticLoss: 62737.125 actorLoss: 81.37301635742188\n",
      "Agent scores: [ 0.22 -0.22]\n",
      "Episode: 1200 Timestep: 1200 Total reward Agent: -164.44 Total reward Env: -164.84 Agent Win Margin: 0.4000000000000057 Episode length: 165.64 mCriticLoss: 3495.4326171875 actorLoss: -4.541849136352539\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from agent import Agent\n",
    "from model import ActorNetwork, CriticNetwork, MCritic\n",
    "from replaybuffer import ReplayBuffer\n",
    "from plotutils import plot_results\n",
    "\n",
    "from games  import games\n",
    "from games import games\n",
    "from game   import Game\n",
    "from player import Player_Template\n",
    "from team   import Team\n",
    "from userinput import userInput\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    global cycle\n",
    "    global HISTORY_FILE\n",
    "    user_input = userInput()\n",
    "    run_trials(user_input)\n",
    "    \n",
    "    \n",
    "\n",
    "#main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(agent_scores[:10], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_board = []\n",
    "for i in [0,1]:\n",
    "    score_board.append([(x[0],x[i+1]) for x in scores])\n",
    "score_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  team_scores = []\n",
    "  min_score = 99999999\n",
    "  max_score = 0\n",
    "\n",
    "  for j in range(user_input.num_sides):\n",
    "    team_scores.append([team_points for cycle, team_points in score_board[j]])\n",
    "    plot.hist(\n",
    "      team_scores[j],\n",
    "      bins=100,\n",
    "      label=user_input.teams[j][\"team_name\"],\n",
    "      color=user_input.teams[j][\"color\"])\n",
    "\n",
    "    min_j_score = min(team_scores[j])\n",
    "    max_j_score = max(team_scores[j])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
