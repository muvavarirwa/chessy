{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "import ipykernel\n",
    "import pandas as pd\n",
    "\n",
    "#pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('expand_frame_repr', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device  = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "import os\n",
    "import jsonlines\n",
    "import ast\n",
    "import time, os, fnmatch, shutil\n",
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import requests\n",
    "\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "HISTORY_FILE = None\n",
    "\n",
    "summary_dict = {\"losses\":0,\"wins\":0,\"draws\":0}\n",
    "\n",
    "cycle = 0\n",
    "\n",
    "actions = [[], []]\n",
    "rewards = [[], []]\n",
    "history = {\"cycle\": cycle, \"actions\": actions, \"rewards\": rewards, \"value\": 0}\n",
    "\n",
    "score_board = {}\n",
    "#step_action_dict = defaultdict()\n",
    "#step_action_dict['random_moves'] = defaultdict()\n",
    "#step_action_dict['policy_moves'] = defaultdict()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting numeric_names.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile numeric_names.py\n",
    "\n",
    "numeric_names = {'w_R0': 1, 'w_K0': 2, 'w_B0': 3, 'w__K': 4, 'w__Q': 5, 'w_B1': 6, 'w_K1': 7, 'w_R1': 8, 'w_P0': 9, 'w_P1':\n",
    " 10, 'w_P2': 11, 'w_P3': 12, 'w_P4': 13, 'w_P5': 14, 'w_P6': 15, 'w_P7': 16, 'b_P0': -16, 'b_P1': -15, 'b_P2': -14, 'b_P3':\n",
    " -13, 'b_P4': -12, 'b_P5': -11, 'b_P6': -10, 'b_P7': -9, 'b_R0': -8, 'b_K0': -7, 'b_B0': -6, 'b__K': -5, 'b__Q': -4, 'b_B1'\n",
    ": -3, 'b_K1': -2, 'b_R1': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1,1,0': 0, '2,1,-2': 1, '2,1,2': 2, '2,2,-1': 3, '2,2,1': 4, '3,1,-1': 5, '3,1,1': 6, '4,1,-1': 7, '4,1,0': 8, '4,1,1': 9, '5,1,-1': 10, '5,1,0': 11, '5,1,1': 12, '6,1,-1': 13, '6,1,1': 14, '7,1,-2': 15, '7,1,2': 16, '7,2,-1': 17, '7,2,1': 18, '8,1,0': 19, '9,1,0': 20, '9,1,-1': 21, '9,1,1': 22, '10,1,0': 23, '10,1,-1': 24, '10,1,1': 25, '11,1,0': 26, '11,1,-1': 27, '11,1,1': 28, '12,1,0': 29, '12,1,-1': 30, '12,1,1': 31, '13,1,0': 32, '13,1,-1': 33, '13,1,1': 34, '14,1,0': 35, '14,1,-1': 36, '14,1,1': 37, '15,1,0': 38, '15,1,-1': 39, '15,1,1': 40, '16,1,0': 41, '16,1,-1': 42, '16,1,1': 43, '-16,-1,0': 44, '-16,-1,-1': 45, '-16,-1,1': 46, '-15,-1,0': 47, '-15,-1,-1': 48, '-15,-1,1': 49, '-14,-1,0': 50, '-14,-1,-1': 51, '-14,-1,1': 52, '-13,-1,0': 53, '-13,-1,-1': 54, '-13,-1,1': 55, '-12,-1,0': 56, '-12,-1,-1': 57, '-12,-1,1': 58, '-11,-1,0': 59, '-11,-1,-1': 60, '-11,-1,1': 61, '-10,-1,0': 62, '-10,-1,-1': 63, '-10,-1,1': 64, '-9,-1,0': 65, '-9,-1,-1': 66, '-9,-1,1': 67, '-8,-1,0': 68, '-7,-1,-2': 69, '-7,-1,2': 70, '-7,-2,-1': 71, '-7,-2,1': 72, '-6,-1,-1': 73, '-6,-1,1': 74, '-5,-1,-1': 75, '-5,-1,0': 76, '-5,-1,1': 77, '-4,-1,-1': 78, '-4,-1,0': 79, '-4,-1,1': 80, '-3,-1,-1': 81, '-3,-1,1': 82, '-2,-1,-2': 83, '-2,-1,2': 84, '-2,-2,-1': 85, '-2,-2,1': 86, '-1,-1,0': 87}\n",
      "{0: '1,1,0', 1: '2,1,-2', 2: '2,1,2', 3: '2,2,-1', 4: '2,2,1', 5: '3,1,-1', 6: '3,1,1', 7: '4,1,-1', 8: '4,1,0', 9: '4,1,1', 10: '5,1,-1', 11: '5,1,0', 12: '5,1,1', 13: '6,1,-1', 14: '6,1,1', 15: '7,1,-2', 16: '7,1,2', 17: '7,2,-1', 18: '7,2,1', 19: '8,1,0', 20: '9,1,0', 21: '9,1,-1', 22: '9,1,1', 23: '10,1,0', 24: '10,1,-1', 25: '10,1,1', 26: '11,1,0', 27: '11,1,-1', 28: '11,1,1', 29: '12,1,0', 30: '12,1,-1', 31: '12,1,1', 32: '13,1,0', 33: '13,1,-1', 34: '13,1,1', 35: '14,1,0', 36: '14,1,-1', 37: '14,1,1', 38: '15,1,0', 39: '15,1,-1', 40: '15,1,1', 41: '16,1,0', 42: '16,1,-1', 43: '16,1,1', 44: '-16,-1,0', 45: '-16,-1,-1', 46: '-16,-1,1', 47: '-15,-1,0', 48: '-15,-1,-1', 49: '-15,-1,1', 50: '-14,-1,0', 51: '-14,-1,-1', 52: '-14,-1,1', 53: '-13,-1,0', 54: '-13,-1,-1', 55: '-13,-1,1', 56: '-12,-1,0', 57: '-12,-1,-1', 58: '-12,-1,1', 59: '-11,-1,0', 60: '-11,-1,-1', 61: '-11,-1,1', 62: '-10,-1,0', 63: '-10,-1,-1', 64: '-10,-1,1', 65: '-9,-1,0', 66: '-9,-1,-1', 67: '-9,-1,1', 68: '-8,-1,0', 69: '-7,-1,-2', 70: '-7,-1,2', 71: '-7,-2,-1', 72: '-7,-2,1', 73: '-6,-1,-1', 74: '-6,-1,1', 75: '-5,-1,-1', 76: '-5,-1,0', 77: '-5,-1,1', 78: '-4,-1,-1', 79: '-4,-1,0', 80: '-4,-1,1', 81: '-3,-1,-1', 82: '-3,-1,1', 83: '-2,-1,-2', 84: '-2,-1,2', 85: '-2,-2,-1', 86: '-2,-2,1', 87: '-1,-1,0'}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from games import games\n",
    "from numeric_names import numeric_names\n",
    "\n",
    "Pieces_detail = defaultdict()\n",
    "actions_array = []\n",
    "sparse_action_dict = defaultdict()\n",
    "bin_action_dict = defaultdict()\n",
    "actions = []\n",
    "\n",
    "for piece in numeric_names.keys():\n",
    "    action_sparse = []\n",
    "    if piece[0] == 'w':\n",
    "        idx = 0\n",
    "    else:\n",
    "        idx = 1\n",
    "        \n",
    "    if piece[1:3] == '_R':\n",
    "        player_id = 'Rook'\n",
    "    elif piece[1:3] == '_K':\n",
    "        player_id = 'Knight'\n",
    "    elif piece[1:3] == '_B':\n",
    "        player_id = 'Bishop'\n",
    "    elif piece[1:3] == '_P':\n",
    "        player_id = 'Pawn'\n",
    "    elif piece[1:4] == '__K':\n",
    "        player_id = 'King'\n",
    "    elif piece[:3] == '__Q':\n",
    "        player_id = 'Queen'\n",
    "        \n",
    "    actions = games['chessy']['players'][player_id]['moves']\n",
    "    \n",
    "    actions = actions[idx]\n",
    "    \n",
    "    #action_diff = 4 - len(actions)\n",
    "    \n",
    "    #for i in range(action_diff):\n",
    "    #    actions.append((0,0))\n",
    "    \n",
    "    piece_num_id = numeric_names[piece]\n",
    "    \n",
    "    piece_bin_id = '{0:06b}'.format(piece_num_id + 17)\n",
    "    \n",
    "    actions_ = []\n",
    "    \n",
    "    for action in actions:\n",
    "        action_value_  = (8*action[0] + action[1] + 17)\n",
    "        action_value   = '{0:06b}'.format(action_value_)\n",
    "        actions_.append(action_value)\n",
    "        \n",
    "        piece_move_bin = (str(piece_bin_id)+str(action_value))\n",
    "        \n",
    "        actions_array.append(piece_move_bin)\n",
    "        \n",
    "        sparse_action  = str(piece_num_id)+\",\"+str(action[0])+ \",\" +str(action[1])\n",
    "        \n",
    "        action_sparse.append(sparse_action)\n",
    "        \n",
    "        sparse_action_dict[sparse_action] = piece_move_bin\n",
    "    \n",
    "    #print(piece_num_id, \"\\t\",piece_bin_id, \"\\t\", action_sparse,  \"\\t\", actions, \"\\t\", actions_)\n",
    "    \n",
    "    Pieces_detail[piece_num_id] = {\n",
    "        'piece_num_id':piece_num_id,\n",
    "        'piece_bin_id':piece_bin_id,\n",
    "        'action_sparse':action_sparse,\n",
    "        'actions_verbose': actions,\n",
    "        'actions_bin': actions_\n",
    "       }\n",
    "    \n",
    "action_ids = np.arange(0,len(sparse_action_dict.keys()),1)\n",
    "#print(action_ids)\n",
    "\n",
    "\n",
    "action_id_dict = {y:x for x,y in zip(action_ids,sparse_action_dict.keys())}\n",
    "print(action_id_dict) \n",
    "\n",
    "id_action_dict = {x:y for x,y in zip(action_ids,sparse_action_dict.keys())}\n",
    "print(id_action_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins_draws_losses = [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting args.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile args.py\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numeric_names import numeric_names\n",
    "\n",
    "args = { \n",
    "    \"update_every\":100,\n",
    "    \"window_size\":100,\n",
    "    \"BUFFER_SIZE\":int(1e6),\n",
    "    \"BATCH_SIZE\":1024,  \n",
    "    \"GAMMA\":0.99,\n",
    "    \"TAU\":2e-3,\n",
    "    \"LR_ACTOR\":1e-3,\n",
    "    \"LR_CRITIC\":1.1e-3,\n",
    "    \"WEIGHT_DECAY\":0.0001,\n",
    "    \"UPDATE_EVERY\":5,\n",
    "    \"EXPLORE_NOISE\":0.05,\n",
    "    \"FC1_UNITS\":10240,\n",
    "    \"FC2_UNITS\":4096,\n",
    "    \"FC3_UNITS\":256,\n",
    "    \"seed\":0,\n",
    "    \"state_size\":384,\n",
    "    \"action_size\":88,\n",
    "    \"action_size_binary\":12,\n",
    "    \"num_agents\":2,\n",
    "    \"device\":torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cuda:0\"),\n",
    "    'mcritic_path':'/home/ubuntu/chessy/checkpoint_mCritic_twoSided.pth',\n",
    "    'agent_p0_path':'/home/ubuntu/chessy/checkpoint_p0_twoSided.pth',\n",
    "    'agent_p1_path':'/home/ubuntu/chessy/checkpoint_p1_twoSided.pth',\n",
    "    'action_id_dict': {'1,1,0': 0, '2,1,-2': 1, '2,1,2': 2, '2,2,-1': 3, '2,2,1': 4, '3,1,-1': 5, '3,1,1': 6, '4,1,-1': 7, '4,1,0': 8, '4,1,1': 9, '5,1,-1': 10, '5,1,0': 11, '5,1,1': 12, '6,1,-1': 13, '6,1,1': 14, '7,1,-2': 15, '7,1,2': 16, '7,2,-1': 17, '7,2,1': 18, '8,1,0': 19, '9,1,0': 20, '9,1,-1': 21, '9,1,1': 22, '10,1,0': 23, '10,1,-1': 24, '10,1,1': 25, '11,1,0': 26, '11,1,-1': 27, '11,1,1': 28, '12,1,0': 29, '12,1,-1': 30, '12,1,1': 31, '13,1,0': 32, '13,1,-1': 33, '13,1,1': 34, '14,1,0': 35, '14,1,-1': 36, '14,1,1': 37, '15,1,0': 38, '15,1,-1': 39, '15,1,1': 40, '16,1,0': 41, '16,1,-1': 42, '16,1,1': 43, '-16,-1,0': 44, '-16,-1,-1': 45, '-16,-1,1': 46, '-15,-1,0': 47, '-15,-1,-1': 48, '-15,-1,1': 49, '-14,-1,0': 50, '-14,-1,-1': 51, '-14,-1,1': 52, '-13,-1,0': 53, '-13,-1,-1': 54, '-13,-1,1': 55, '-12,-1,0': 56, '-12,-1,-1': 57, '-12,-1,1': 58, '-11,-1,0': 59, '-11,-1,-1': 60, '-11,-1,1': 61, '-10,-1,0': 62, '-10,-1,-1': 63, '-10,-1,1': 64, '-9,-1,0': 65, '-9,-1,-1': 66, '-9,-1,1': 67, '-8,-1,0': 68, '-7,-1,-2': 69, '-7,-1,2': 70, '-7,-2,-1': 71, '-7,-2,1': 72, '-6,-1,-1': 73, '-6,-1,1': 74, '-5,-1,-1': 75, '-5,-1,0': 76, '-5,-1,1': 77, '-4,-1,-1': 78, '-4,-1,0': 79, '-4,-1,1': 80, '-3,-1,-1': 81, '-3,-1,1': 82, '-2,-1,-2': 83, '-2,-1,2': 84, '-2,-2,-1': 85, '-2,-2,1': 86, '-1,-1,0': 87},\n",
    "    'id_action_dict': {0: '1,1,0', 1: '2,1,-2', 2: '2,1,2', 3: '2,2,-1', 4: '2,2,1', 5: '3,1,-1', 6: '3,1,1', 7: '4,1,-1', 8: '4,1,0', 9: '4,1,1', 10: '5,1,-1', 11: '5,1,0', 12: '5,1,1', 13: '6,1,-1', 14: '6,1,1', 15: '7,1,-2', 16: '7,1,2', 17: '7,2,-1', 18: '7,2,1', 19: '8,1,0', 20: '9,1,0', 21: '9,1,-1', 22: '9,1,1', 23: '10,1,0', 24: '10,1,-1', 25: '10,1,1', 26: '11,1,0', 27: '11,1,-1', 28: '11,1,1', 29: '12,1,0', 30: '12,1,-1', 31: '12,1,1', 32: '13,1,0', 33: '13,1,-1', 34: '13,1,1', 35: '14,1,0', 36: '14,1,-1', 37: '14,1,1', 38: '15,1,0', 39: '15,1,-1', 40: '15,1,1', 41: '16,1,0', 42: '16,1,-1', 43: '16,1,1', 44: '-16,-1,0', 45: '-16,-1,-1', 46: '-16,-1,1', 47: '-15,-1,0', 48: '-15,-1,-1', 49: '-15,-1,1', 50: '-14,-1,0', 51: '-14,-1,-1', 52: '-14,-1,1', 53: '-13,-1,0', 54: '-13,-1,-1', 55: '-13,-1,1', 56: '-12,-1,0', 57: '-12,-1,-1', 58: '-12,-1,1', 59: '-11,-1,0', 60: '-11,-1,-1', 61: '-11,-1,1', 62: '-10,-1,0', 63: '-10,-1,-1', 64: '-10,-1,1', 65: '-9,-1,0', 66: '-9,-1,-1', 67: '-9,-1,1', 68: '-8,-1,0', 69: '-7,-1,-2', 70: '-7,-1,2', 71: '-7,-2,-1', 72: '-7,-2,1', 73: '-6,-1,-1', 74: '-6,-1,1', 75: '-5,-1,-1', 76: '-5,-1,0', 77: '-5,-1,1', 78: '-4,-1,-1', 79: '-4,-1,0', 80: '-4,-1,1', 81: '-3,-1,-1', 82: '-3,-1,1', 83: '-2,-1,-2', 84: '-2,-1,2', 85: '-2,-2,-1', 86: '-2,-2,1', 87: '-1,-1,0'},\n",
    "    'numeric_names' : numeric_names,\n",
    "    'initial_state'  : \"010010010011010100010101010110010111011000011001011010011011011100011101011110011111100000100001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000010000011000100000101000110000111001000001001001010001011001100001101001110001111010000\",\n",
    "    'SUMMARY_FILE':\"/data_data/reinforcement_learning/results/summary_file.tsv\",\n",
    "    'HISTORY_FILE':None,\n",
    "    'TAKE_KING_REWARD':10,\n",
    "    'MORE_POINTS_REWARD':1,\n",
    "    'EQUAL_POINTS_REWARD':0,\n",
    "    'STEP_REWARD':-1,\n",
    "    'WINS_DRAWS_LOSSES': [0,0,0],\n",
    "    'in_channels_1':1,\n",
    "    'in_channels_2':64,\n",
    "    'in_channels_l':128,\n",
    "    'out_channels_1':64,\n",
    "    'out_channels_2':128,\n",
    "    'out_channels_l':16,\n",
    "    'kernel_1_size':(6,1),\n",
    "    'kernel_2_size':(1,1),\n",
    "    'kernel_l_size':(1,1),\n",
    "    'stride_1_size':(6,1),\n",
    "    'stride_2_size':(6,1),\n",
    "    'stride_l_size':(1,1),\n",
    "    'reshape_size':(8,48),\n",
    "    'reshape_buffer':(1,384)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sparse_action_dict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sparse_action_dict.py\n",
    "\n",
    "sparse_action_dict = {'1,1,0':'010010011001','2,1,-2':'010011010111','2,1,2':'010011011011','2,2,-1':'010011100000','2,2,1':'010011100010','3,1,-1':'010100011000','3,1,1':'010100011010','4,1,-1':'010101011000','4,1,0':'010101011001','4,1,1':'010101011010','5,1,-1':'010110011000','5,1,0':'010110011001','5,1,1':'010110011010','6,1,-1':'010111011000','6,1,1':'010111011010','7,1,-2':'011000010111','7,1,2':'011000011011','7,2,-1':'011000100000','7,2,1':'011000100010','8,1,0':'011001011001','9,1,0':'011010011001','9,1,-1':'011010011000','9,1,1':'011010011010','10,1,0':'011011011001','10,1,-1':'011011011000','10,1,1':'011011011010','11,1,0':'011100011001','11,1,-1':'011100011000','11,1,1':'011100011010','12,1,0':'011101011001','12,1,-1':'011101011000','12,1,1':'011101011010','13,1,0':'011110011001','13,1,-1':'011110011000','13,1,1':'011110011010','14,1,0':'011111011001','14,1,-1':'011111011000','14,1,1':'011111011010','15,1,0':'100000011001','15,1,-1':'100000011000','15,1,1':'100000011010','16,1,0':'100001011001','16,1,-1':'100001011000','16,1,1':'100001011010','-16,-1,0':'000001001001','-16,-1,-1':'000001001000','-16,-1,1':'000001001010','-15,-1,0':'000010001001','-15,-1,-1':'000010001000','-15,-1,1':'000010001010','-14,-1,0':'000011001001','-14,-1,-1':'000011001000','-14,-1,1':'000011001010','-13,-1,0':'000100001001','-13,-1,-1':'000100001000','-13,-1,1':'000100001010','-12,-1,0':'000101001001','-12,-1,-1':'000101001000','-12,-1,1':'000101001010','-11,-1,0':'000110001001','-11,-1,-1':'000110001000','-11,-1,1':'000110001010','-10,-1,0':'000111001001','-10,-1,-1':'000111001000','-10,-1,1':'000111001010','-9,-1,0':'001000001001','-9,-1,-1':'001000001000','-9,-1,1':'001000001010','-8,-1,0':'001001001001','-7,-1,-2':'001010000111','-7,-1,2':'001010001011','-7,-2,-1':'001010000000','-7,-2,1':'001010000010','-6,-1,-1':'001011001000','-6,-1,1':'001011001010','-5,-1,-1':'001100001000','-5,-1,0':'001100001001','-5,-1,1':'001100001010','-4,-1,-1':'001101001000','-4,-1,0':'001101001001','-4,-1,1':'001101001010','-3,-1,-1':'001110001000','-3,-1,1':'001110001010','-2,-1,-2':'001111000111','-2,-1,2':'001111001011','-2,-2,-1':'001111000000','-2,-2,1':'001111000010','-1,-1,0':'010000001001'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pieces = {\n",
    "    'Pawn':(\"w_P\",\"b_P\"),\n",
    "    'Knight':(\"w_K\",\"b_K\"),\n",
    "    'King':(\"w__K\",\"b__K\"),\n",
    "    'Queen':(\"w__Q\",\"b__Q\"),\n",
    "    'Bishop':(\"w_B\",\"b_B\"),\n",
    "    'Rook':(\"w_R\",\"b_R\"),\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def getBin(num):\n",
    "        if int(num) != 0:\n",
    "            return \"{0:{fill}6b}\".format(int(num)+17, fill='0')\n",
    "        else:\n",
    "            return \"{0:{fill}6b}\".format(0, fill='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse_action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_moves   = 0\n",
    "policy_moves = 0\n",
    "random_moves = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFunc(piece):\n",
    "    def piece_func():\n",
    "        return piece\n",
    "    return piece_func()\n",
    "\n",
    "def Bishop():\n",
    "    return getFunc(\"Bishop\")\n",
    "\n",
    "def Pawn():\n",
    "    return getFunc(\"Pawn\")\n",
    "\n",
    "def King():\n",
    "    return getFunc(\"King\")\n",
    "\n",
    "def Queen():\n",
    "    return getFunc(\"Queen\")\n",
    "\n",
    "def Knight():\n",
    "    return getFunc(\"Knight\")\n",
    "\n",
    "def Rook():\n",
    "    return getFunc(\"Rook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pawn'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pawn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Incorrect_Input_error(Exception):\n",
    "  \"\"\"Generic input error handler: raised in the case that any of the user inputed data is incorrect\"\"\"\n",
    "  pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp0  = None\n",
    "temp1  = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from args import args\n",
    "from sparse_action_dict import sparse_action_dict\n",
    "from numeric_names import numeric_names\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "#from tensorboardX import SummaryWriter\n",
    "import sys\n",
    "\n",
    "sys.path.append('./anaconda3/lib/python3.7/site-packages/torchvision')\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(comment=\"MADDPG Chessy\")\n",
    "\n",
    "id_names = {value:key for key,value in numeric_names.items()}\n",
    "names    = {value: key for key in Pieces for value in Pieces[key]}\n",
    "\n",
    "\n",
    "stats_rewards_list = []\n",
    "\n",
    "def run_trial(user_input, env, mCritic, userInput, prev_wins_draws_losses):\n",
    "    global temp0\n",
    "    global temp1\n",
    "    global cycle\n",
    "    global scores\n",
    "    global total_reward_agent\n",
    "    global total_reward_env\n",
    "    \n",
    "    for j in range(user_input.num_sides):\n",
    "        team_name = \"team_\" + str(j)\n",
    "        team_name = Team(user_input.game, user_input.teams[j][\"team_name\"], j,user_input.teams[j][\"skill\"] / 10, user_input.teams[j][\"strategy\"])\n",
    "\n",
    "        env.insert_team(team_name)\n",
    "            \n",
    "    env.not_deadlocked  = True\n",
    "    env.states.append(args['initial_state'])\n",
    "    time_step    = 0\n",
    "        \n",
    "    total_reward_agent = 0\n",
    "    \n",
    "    total_reward_env   = 0\n",
    "        \n",
    "    while env.not_deadlocked:\n",
    "         \n",
    "        curr_board_ids = {value:key for key,value in env.board.items()}\n",
    "        state   = env.state \n",
    "        state   = np.array(list(map(int, state))).astype(np.float32)\n",
    "        \n",
    "        #state_prior_tensor = torch.IntTensor(state_prior_bin).to(device)\n",
    "        if time_step % 2 == 0:\n",
    "            action_ = agent_p0.act(state)\n",
    "        else:\n",
    "            action_ = agent_p1.act(state)\n",
    "\n",
    "        #Determine feasible moves for each team:\n",
    "        feasible_moves     = []\n",
    "        feasible_moves_0   = env.get_feasible_moves(env.team[0])\n",
    "        feasible_moves_1   = env.get_feasible_moves(env.team[1])\n",
    "        [feasible_moves.append(x) for x in feasible_moves_0]\n",
    "        [feasible_moves.append(x) for x in feasible_moves_1]\n",
    "        \n",
    "        f_moves   = set([int(x) for x in [action_id_dict[str((args['numeric_names'][env.board[curr_pos]],move)).replace(\"(\",\"\").replace(\")\",\"\").replace(\" \",\"\")] for name, move, curr_pos, new_pos in feasible_moves]])\n",
    "        \n",
    "        f_moves_sparse = [id_action_dict[f_move] for f_move in f_moves]\n",
    "        \n",
    "        #f_moves  = set(np.sort(f_moves, axis=-1, kind='quicksort', order=None))\n",
    "        \n",
    "        action_   = [v if k in f_moves else 0 for k,v in enumerate(action_)]\n",
    "\n",
    "        mid_point        = int((args['action_size'])/2)\n",
    "\n",
    "        action_idx_0     = np.argmax(action_[:mid_point])\n",
    "        action_idx_1     = np.argmax(action_[mid_point:]) + mid_point\n",
    "        \n",
    "        #action_idx_1     = np.argmax(action_[-mid_point:]) \n",
    "\n",
    "        action_sparse_0  = id_action_dict[action_idx_0]\n",
    "        action_sparse_1  = id_action_dict[action_idx_1]\n",
    "        #action_verbose_0 = (curr_board_ids[action_sparse_0[0]],action_sparse_0[1:])\n",
    "        \n",
    "        action_one_hot   = np.zeros(args['action_size'])\n",
    "        \n",
    "        action_one_hot[action_idx_0]   = 1\n",
    "        action_one_hot[action_idx_1]   = 1\n",
    "        \n",
    "        env.best_moves_sparse          = [action_sparse_0,action_sparse_1]\n",
    "        \n",
    "        \n",
    "        #############################################\n",
    "        #  SEPARATE OUT INTO A STANDALONE FUNCTION  #\n",
    "        #############################################\n",
    "        \n",
    "        \n",
    "        def get_verbose_action(sp_action,i):\n",
    "\n",
    "            act      = sp_action.split(\",\")\n",
    "            \n",
    "            name     = id_names[int(act[0])]\n",
    "\n",
    "            curr_pos = curr_board_ids[name]\n",
    "                \n",
    "            next_pos = zip(curr_pos,action)\n",
    "\n",
    "            move     = tuple([int(x) for x in act[1:]])\n",
    "\n",
    "            next_pos = tuple(sum(tuples) for tuples in zip(curr_pos,move))\n",
    "            \n",
    "            player   = env.team[i].players[name]\n",
    "                \n",
    "            moves    = [player,move, curr_pos,next_pos]\n",
    "            \n",
    "            return moves\n",
    "\n",
    "        \n",
    "        #############################################\n",
    "        # \n",
    "        #############################################\n",
    "        \n",
    "        try:\n",
    "            env.best_moves_verbose         = [get_verbose_action(action_sparse_0,0),get_verbose_action(action_sparse_1,1)]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        next_state, reward, done, info = env.step(cycle,user_input,args)\n",
    "        \n",
    "        next_state                     = np.array(list(map(int, env.state))).astype(np.float32)\n",
    "        \n",
    "        total_reward_agent             += reward[0]\n",
    "        total_reward_env               += reward[1]\n",
    "        \n",
    "        if userInput.teams[0][\"skill\"] > 1:\n",
    "            agent_p0.step(state, action_one_hot, int(reward[0]), next_state, done, mCritic)\n",
    "        \n",
    "        if userInput.teams[1][\"skill\"] > 1:\n",
    "            agent_p1.step(state, action_one_hot, int(reward[1]), next_state, done, mCritic)\n",
    "        \n",
    "        if cycle > 10:\n",
    "            num_iter = cycle\n",
    "            writer.add_scalar('Actor_loss', agent_p0.actorLoss, num_iter)\n",
    "            writer.add_scalar('Critic_loss', agent_p0.mCriticLoss, num_iter)\n",
    "            writer.add_scalar('Total_reward_agent', total_reward_agent, num_iter)\n",
    "            writer.add_scalar('Total_reward_env', total_reward_env , num_iter)\n",
    "            writer.add_scalar('Agent % Wins/(Wins + Losses + Draws)', args['WINS_DRAWS_LOSSES'][0]/sum(args['WINS_DRAWS_LOSSES']) , num_iter)\n",
    "            writer.add_scalar('Agent % Losses/(Wins + Losses + Draws)', args['WINS_DRAWS_LOSSES'][2]/sum(args['WINS_DRAWS_LOSSES']) , num_iter)\n",
    "            writer.add_scalar('Agent % Draws/(Wins + Losses + Draws)', args['WINS_DRAWS_LOSSES'][1]/sum(args['WINS_DRAWS_LOSSES']) , num_iter)\n",
    "            writer.add_scalar('Wins net Losses)', (args['WINS_DRAWS_LOSSES'][0] - prev_wins_draws_losses[0]) - (args['WINS_DRAWS_LOSSES'][2] - prev_wins_draws_losses[2]), num_iter)\n",
    "            writer.add_scalar('Episode Length', stats_rewards_list[-1][3], num_iter)\n",
    "            writer.add_scalar('Avg Episode Length', np.mean(stats_rewards_list[-10:], axis=0)[3], num_iter)\n",
    "\n",
    "        time_step                      += 1\n",
    "\n",
    "    scores.append((cycle, info[0],info[1]))\n",
    "    \n",
    "    stats_rewards_list.append((cycle, total_reward_agent, total_reward_env, time_step))\n",
    "    \n",
    "    if cycle % args['update_every'] == 0:\n",
    "            print(\"Agent scores: {}\".format(np.mean(agent_scores[-args['window_size']:], axis=0)))\n",
    "            \n",
    "    if time_step > args['window_size'] and cycle % args['update_every'] == 0:\n",
    "        \n",
    "        torch.save(agent_p0.actor_network.state_dict(), 'checkpoint_p0_twoSided.pth')\n",
    "        torch.save(agent_p1.actor_network.state_dict(), 'checkpoint_p1_twoSided.pth')\n",
    "        torch.save(mCritic.network.state_dict(), 'checkpoint_mCritic_twoSided.pth')\n",
    "        \n",
    "        agent_win_margin = np.mean(stats_rewards_list[-args['window_size']:],  axis=0)[1] - np.mean(stats_rewards_list[-args['window_size']:],  axis=0)[2]\n",
    "        print('Episode: {}'.format(cycle),\n",
    "        'Timestep: {}'.format(cycle),\n",
    "        'Total reward Agent: {}'.format(np.mean(stats_rewards_list[-args['window_size']:],  axis=0)[1]),\n",
    "        'Total reward Env: {}'.format(np.mean(stats_rewards_list[-args['window_size']:],  axis=0)[2]),\n",
    "        'Agent Win Margin: {}'.format(agent_win_margin),\n",
    "        'Agent Wins: {}'.format(args['WINS_DRAWS_LOSSES'][0]),\n",
    "        'Agent Draws: {}'.format(args['WINS_DRAWS_LOSSES'][1]),\n",
    "        'Agent Losses: {}'.format(args['WINS_DRAWS_LOSSES'][2]),\n",
    "        'Episode length: {}'.format(np.mean(stats_rewards_list[-args['window_size']:],axis=0)[3]),\n",
    "        'mCriticLoss: {}'.format(agent_p0.mCriticLoss),\n",
    "        'actorLoss: {}'.format(agent_p0.actorLoss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_trials(user_input):\n",
    "    \"\"\" Runs the num_trials \"\"\"\n",
    "    global cycle\n",
    "    global HISTORY_FILE\n",
    "    global SUMMARY_FILE\n",
    "    global env\n",
    "    global agent_scores\n",
    "    \n",
    "    print(\"==============================RUN_TRIALS==================================\")\n",
    "    \n",
    "    t = time.localtime()\n",
    "    timestamp = time.strftime('%b_%d_%Y_%H%M', t)\n",
    "    num_trials = user_input.num_trials\n",
    "    num_sides = user_input.num_sides\n",
    "    \n",
    "    #SUMMARY_FILE = (\"/data_data/reinforcement_learning/results/summary_file.tsv\")\n",
    "\n",
    "    #HISTORY_FILE = (\"/data_data/reinforcement_learning/results/history_file_\" + str(num_trials) + \"_trials_\" + str(num_sides) + \"_sides_\" + str(timestamp))\n",
    "    \n",
    "    \"\"\" Clear output file \"\"\"\n",
    "    \n",
    "    args['HISTORY_FILE'] = \"/data_data/reinforcement_learning/results/history_file_\" + str(num_trials) + \"_trials_\" + str(num_sides) + \"_sides_\" + str(timestamp)\n",
    "    \n",
    "    with open(args['HISTORY_FILE'], \"w\") as history_file:\n",
    "        history_file.write(\"\\n\")\n",
    "        \n",
    "        \n",
    "    for i in range(num_trials):\n",
    "        \n",
    "        sides = [x for x in range(num_sides)]\n",
    "        \n",
    "        env = Game(user_input.game, 8, sides, user_input.display_board_positions)\n",
    "\n",
    "        run_trial(user_input,env, mCritic, user_input, args['WINS_DRAWS_LOSSES'])\n",
    "\n",
    "        cycle += 1\n",
    "        \n",
    "        agent_scores.append(env.last_reward)\n",
    "  \n",
    "        \n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from model import ActorNetwork, CriticNetwork, MCritic\n",
    "from replaybuffer import ReplayBuffer\n",
    "from args import args\n",
    "\n",
    "mCritic       =  MCritic(args['state_size'],args['action_size'])\n",
    "agent_p0      =  Agent(args['state_size'],args['action_size'], 0)\n",
    "agent_p1      =  Agent(args['state_size'],args['action_size'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CriticNetwork(\n",
      "  (fc1): Linear(in_features=472, out_features=10240, bias=True)\n",
      "  (fc2): Linear(in_features=10240, out_features=4096, bias=True)\n",
      "  (fc3): Linear(in_features=4096, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "ActorNetwork(\n",
      "  (fc1): Linear(in_features=384, out_features=10240, bias=True)\n",
      "  (fc2): Linear(in_features=10240, out_features=4096, bias=True)\n",
      "  (fc3): Linear(in_features=4096, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=88, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(mCritic.network)\n",
    "print(agent_p0.actor_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select a game: ['chessy' or 'checkers']: chessy\n",
      "select number of teams:  [0, 1 or 2] 2\n",
      "Choose a name for this team [e.g. 'blue_team' or 'green_team']: Agent\n",
      "Choose a color for team_2 [e.g. 'blue': green\n",
      "Please enter team's skill_level [1 = Novice, 10 = expert]: 10\n",
      "Please enter team's strategy [0 = 'cooperative', 1 = 'competitive']: 1\n",
      "Choose a name for this team [e.g. 'blue_team' or 'green_team']: Environment\n",
      "Choose a color for team_2 [e.g. 'blue': red\n",
      "Please enter team's skill_level [1 = Novice, 10 = expert]: 10\n",
      "Please enter team's strategy [0 = 'cooperative', 1 = 'competitive']: 1\n",
      "How many trials would you like to run? [1 - 1,000,000] 100000\n",
      "Do you want to see the board positions in realtime? [ 'Yes' or 'No' ]no\n",
      "==============================RUN_TRIALS==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/chessy/model.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.fc4(x))\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent scores: nan\n",
      "Agent scores: [-1.07  1.07]\n",
      "Agent scores: [-1.23  1.23]\n",
      "Agent scores: [-2.7  2.7]\n",
      "Agent scores: [-0.72  0.72]\n",
      "Episode: 400 Timestep: 400 Total reward Agent: -78.78 Total reward Env: -77.34 Agent Win Margin: -1.4399999999999977 Agent Wins: 368 Agent Draws: 0 Agent Losses: 434 Episode length: 79.06 mCriticLoss: 0.8797740936279297 actorLoss: 0.9971776008605957\n",
      "Agent scores: [-3.45  3.45]\n",
      "Episode: 500 Timestep: 500 Total reward Agent: -69.59 Total reward Env: -62.87 Agent Win Margin: -6.720000000000006 Agent Wins: 438 Agent Draws: 0 Agent Losses: 564 Episode length: 67.23 mCriticLoss: 1.468734860420227 actorLoss: 1.0012733936309814\n",
      "Agent scores: [-1.39  1.39]\n",
      "Agent scores: [-3.03  3.03]\n",
      "Agent scores: [-3.97  3.97]\n",
      "Agent scores: [-1.78  1.78]\n",
      "Agent scores: [-3.56  3.56]\n",
      "Agent scores: [-2.35  2.35]\n",
      "Agent scores: [-2.78  2.78]\n",
      "Agent scores: [-3.45  3.45]\n",
      "Episode: 1300 Timestep: 1300 Total reward Agent: -64.18 Total reward Env: -57.1 Agent Win Margin: -7.080000000000005 Agent Wins: 1078 Agent Draws: 2 Agent Losses: 1522 Episode length: 61.64 mCriticLoss: 1.2727675437927246 actorLoss: 1.0035754442214966\n",
      "Agent scores: [-2.92  2.92]\n",
      "Agent scores: [-2.21  2.21]\n",
      "Agent scores: [-2.72  2.72]\n",
      "Episode: 1600 Timestep: 1600 Total reward Agent: -66.2 Total reward Env: -60.94 Agent Win Margin: -5.260000000000005 Agent Wins: 1320 Agent Draws: 2 Agent Losses: 1880 Episode length: 64.57 mCriticLoss: 0.8813249468803406 actorLoss: 1.0001295804977417\n",
      "Agent scores: [-2.52  2.52]\n",
      "Agent scores: [-2.08  2.08]\n",
      "Agent scores: [-1.49  1.49]\n",
      "Agent scores: [-1.42  1.42]\n",
      "Agent scores: [-2.78  2.78]\n",
      "Agent scores: [-3.27  3.27]\n",
      "Agent scores: [-1.33  1.33]\n",
      "Episode: 2300 Timestep: 2300 Total reward Agent: -68.25 Total reward Env: -65.77 Agent Win Margin: -2.480000000000004 Agent Wins: 1888 Agent Draws: 6 Agent Losses: 2708 Episode length: 68.01 mCriticLoss: 0.8793979287147522 actorLoss: 0.99916672706604\n",
      "Agent scores: [-2.52  2.52]\n",
      "Agent scores: [-2.42  2.42]\n",
      "Agent scores: [ 0.08 -0.08]\n",
      "Agent scores: [-2.25  2.25]\n",
      "Episode: 2700 Timestep: 2700 Total reward Agent: -66.91 Total reward Env: -62.63 Agent Win Margin: -4.279999999999994 Agent Wins: 2236 Agent Draws: 6 Agent Losses: 3160 Episode length: 65.77 mCriticLoss: 1.1726739406585693 actorLoss: 1.0008862018585205\n",
      "Agent scores: [-1.88  1.88]\n",
      "Agent scores: [-0.51  0.51]\n",
      "Agent scores: [-0.5  0.5]\n",
      "Agent scores: [-0.63  0.63]\n",
      "Agent scores: [-1.41  1.41]\n",
      "Agent scores: [-1.03  1.03]\n",
      "Episode: 3300 Timestep: 3300 Total reward Agent: -63.34 Total reward Env: -61.5 Agent Win Margin: -1.8400000000000034 Agent Wins: 2816 Agent Draws: 6 Agent Losses: 3780 Episode length: 63.42 mCriticLoss: 0.39394208788871765 actorLoss: 1.0088599920272827\n",
      "Agent scores: [ 0.03 -0.03]\n",
      "Agent scores: [-4.11  4.11]\n",
      "Agent scores: [-1.36  1.36]\n",
      "Agent scores: [-4.25  4.25]\n",
      "Agent scores: [-2.8  2.8]\n",
      "Agent scores: [-1.83  1.83]\n",
      "Agent scores: [-2.03  2.03]\n",
      "Episode: 4000 Timestep: 4000 Total reward Agent: -70.77 Total reward Env: -66.93 Agent Win Margin: -3.839999999999989 Agent Wins: 3384 Agent Draws: 6 Agent Losses: 4612 Episode length: 69.85 mCriticLoss: 0.9795163869857788 actorLoss: 1.0075101852416992\n",
      "Agent scores: [-1.05  1.05]\n",
      "Episode: 4100 Timestep: 4100 Total reward Agent: -66.77 Total reward Env: -64.63 Agent Win Margin: -2.1400000000000006 Agent Wins: 3476 Agent Draws: 6 Agent Losses: 4720 Episode length: 66.7 mCriticLoss: 0.8806992769241333 actorLoss: 1.0014444589614868\n",
      "Agent scores: [-2.21  2.21]\n",
      "Agent scores: [-1.31  1.31]\n",
      "Agent scores: [-2.11  2.11]\n",
      "Agent scores: [-1.58  1.58]\n",
      "Agent scores: [-1.69  1.69]\n",
      "Episode: 4600 Timestep: 4600 Total reward Agent: -64.61 Total reward Env: -61.45 Agent Win Margin: -3.1599999999999966 Agent Wins: 3922 Agent Draws: 12 Agent Losses: 5268 Episode length: 64.03 mCriticLoss: 0.7857917547225952 actorLoss: 1.000360369682312\n",
      "Agent scores: [-0.32  0.32]\n",
      "Agent scores: [-3.63  3.63]\n",
      "Agent scores: [-3.83  3.83]\n",
      "Agent scores: [-1.61  1.61]\n",
      "Episode: 5000 Timestep: 5000 Total reward Agent: -72.48 Total reward Env: -68.86 Agent Win Margin: -3.6200000000000045 Agent Wins: 4254 Agent Draws: 12 Agent Losses: 5736 Episode length: 71.67 mCriticLoss: 1.1712794303894043 actorLoss: 1.0073357820510864\n",
      "Agent scores: [-2.72  2.72]\n",
      "Agent scores: [-2.92  2.92]\n",
      "Episode: 5200 Timestep: 5200 Total reward Agent: -65.95 Total reward Env: -60.29 Agent Win Margin: -5.660000000000004 Agent Wins: 4412 Agent Draws: 12 Agent Losses: 5978 Episode length: 64.12 mCriticLoss: 2.0523781776428223 actorLoss: 0.9912546873092651\n",
      "Agent scores: [-2.01  2.01]\n",
      "Episode: 5300 Timestep: 5300 Total reward Agent: -65.33 Total reward Env: -61.35 Agent Win Margin: -3.979999999999997 Agent Wins: 4502 Agent Draws: 12 Agent Losses: 6088 Episode length: 64.34 mCriticLoss: 0.8784735202789307 actorLoss: 1.0072842836380005\n",
      "Agent scores: [-2.19  2.19]\n",
      "Episode: 5400 Timestep: 5400 Total reward Agent: -65.19 Total reward Env: -60.81 Agent Win Margin: -4.3799999999999955 Agent Wins: 4590 Agent Draws: 12 Agent Losses: 6200 Episode length: 64.0 mCriticLoss: 1.6620548963546753 actorLoss: 1.0005342960357666\n",
      "Agent scores: [-1.88  1.88]\n",
      "Episode: 5500 Timestep: 5500 Total reward Agent: -68.74 Total reward Env: -64.94 Agent Win Margin: -3.799999999999997 Agent Wins: 4680 Agent Draws: 12 Agent Losses: 6310 Episode length: 67.84 mCriticLoss: 1.0756951570510864 actorLoss: 0.9985536932945251\n",
      "Agent scores: [ 0.04 -0.04]\n",
      "Agent scores: [ 0.53 -0.53]\n",
      "Agent scores: [-1.85  1.85]\n",
      "Agent scores: [-2.66  2.66]\n",
      "Episode: 5900 Timestep: 5900 Total reward Agent: -72.1 Total reward Env: -67.0 Agent Win Margin: -5.099999999999994 Agent Wins: 5056 Agent Draws: 14 Agent Losses: 6732 Episode length: 70.55 mCriticLoss: 1.178253173828125 actorLoss: 1.004349946975708\n",
      "Agent scores: [-1.65  1.65]\n",
      "Episode: 6000 Timestep: 6000 Total reward Agent: -71.59 Total reward Env: -68.25 Agent Win Margin: -3.3400000000000034 Agent Wins: 5142 Agent Draws: 14 Agent Losses: 6846 Episode length: 70.92 mCriticLoss: 1.6635836362838745 actorLoss: 0.9969342947006226\n",
      "Agent scores: [-0.92  0.92]\n",
      "Agent scores: [-0.72  0.72]\n",
      "Agent scores: [-2.68  2.68]\n",
      "Agent scores: [-1.6  1.6]\n",
      "Agent scores: [-3.39  3.39]\n",
      "Agent scores: [-0.28  0.28]\n",
      "Agent scores: [-2.56  2.56]\n",
      "Agent scores: [-2.11  2.11]\n",
      "Episode: 6800 Timestep: 6800 Total reward Agent: -66.17 Total reward Env: -61.73 Agent Win Margin: -4.440000000000005 Agent Wins: 5852 Agent Draws: 24 Agent Losses: 7726 Episode length: 64.95 mCriticLoss: 0.7842246294021606 actorLoss: 1.0034457445144653\n",
      "Agent scores: [-3.04  3.04]\n",
      "Agent scores: [-3.32  3.32]\n",
      "Agent scores: [-2.45  2.45]\n",
      "Agent scores: [-3.07  3.07]\n",
      "Agent scores: [-1.45  1.45]\n",
      "Agent scores: [-3.19  3.19]\n",
      "Episode: 7400 Timestep: 7400 Total reward Agent: -68.15 Total reward Env: -61.55 Agent Win Margin: -6.6000000000000085 Agent Wins: 6320 Agent Draws: 26 Agent Losses: 8456 Episode length: 65.85 mCriticLoss: 1.2733044624328613 actorLoss: 0.9977566003799438\n",
      "Agent scores: [-1.8  1.8]\n",
      "Agent scores: [-2.81  2.81]\n",
      "Episode: 7600 Timestep: 7600 Total reward Agent: -70.49 Total reward Env: -65.05 Agent Win Margin: -5.439999999999998 Agent Wins: 6482 Agent Draws: 26 Agent Losses: 8694 Episode length: 68.77 mCriticLoss: 1.0734946727752686 actorLoss: 1.0039429664611816\n",
      "Agent scores: [-2.47  2.47]\n",
      "Agent scores: [-2.81  2.81]\n",
      "Agent scores: [-2.75  2.75]\n",
      "Agent scores: [-1.25  1.25]\n",
      "Agent scores: [-2.21  2.21]\n",
      "Agent scores: [-4.03  4.03]\n",
      "Episode: 8200 Timestep: 8200 Total reward Agent: -67.58 Total reward Env: -59.3 Agent Win Margin: -8.280000000000001 Agent Wins: 6960 Agent Draws: 28 Agent Losses: 9414 Episode length: 64.44 mCriticLoss: 1.1746965646743774 actorLoss: 1.0030490159988403\n",
      "Agent scores: [-2.87  2.87]\n",
      "Agent scores: [-1.49  1.49]\n",
      "Agent scores: [-2.52  2.52]\n",
      "Agent scores: [-3.16  3.16]\n",
      "Agent scores: [-1.82  1.82]\n",
      "Agent scores: [-4.34  4.34]\n",
      "Episode: 8800 Timestep: 8800 Total reward Agent: -63.1 Total reward Env: -54.2 Agent Win Margin: -8.899999999999999 Agent Wins: 7432 Agent Draws: 32 Agent Losses: 10138 Episode length: 59.65 mCriticLoss: 1.0757155418395996 actorLoss: 1.0046747922897339\n",
      "Agent scores: [-2.31  2.31]\n",
      "Episode: 8900 Timestep: 8900 Total reward Agent: -64.16 Total reward Env: -59.58 Agent Win Margin: -4.579999999999998 Agent Wins: 7518 Agent Draws: 34 Agent Losses: 10250 Episode length: 62.87 mCriticLoss: 0.5898027420043945 actorLoss: 1.0003066062927246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent scores: [-2.79  2.79]\n",
      "Episode: 9000 Timestep: 9000 Total reward Agent: -71.94 Total reward Env: -66.32 Agent Win Margin: -5.6200000000000045 Agent Wins: 7598 Agent Draws: 34 Agent Losses: 10370 Episode length: 70.13 mCriticLoss: 0.3906659483909607 actorLoss: 1.0035724639892578\n",
      "Agent scores: [-2.45  2.45]\n",
      "Episode: 9100 Timestep: 9100 Total reward Agent: -70.67 Total reward Env: -65.77 Agent Win Margin: -4.900000000000006 Agent Wins: 7678 Agent Draws: 34 Agent Losses: 10490 Episode length: 69.22 mCriticLoss: 0.981993556022644 actorLoss: 0.9987435340881348\n",
      "Agent scores: [-1.01  1.01]\n",
      "Agent scores: [-3.22  3.22]\n",
      "Agent scores: [-1.61  1.61]\n",
      "Agent scores: [-2.18  2.18]\n",
      "Agent scores: [-2.71  2.71]\n",
      "Episode: 9600 Timestep: 9600 Total reward Agent: -64.33 Total reward Env: -58.73 Agent Win Margin: -5.600000000000001 Agent Wins: 8112 Agent Draws: 40 Agent Losses: 11050 Episode length: 62.53 mCriticLoss: 1.4668018817901611 actorLoss: 0.9921092987060547\n",
      "Agent scores: [-2.92  2.92]\n",
      "Episode: 9700 Timestep: 9700 Total reward Agent: -63.2 Total reward Env: -57.36 Agent Win Margin: -5.840000000000003 Agent Wins: 8190 Agent Draws: 40 Agent Losses: 11172 Episode length: 61.28 mCriticLoss: 0.5895993709564209 actorLoss: 0.9956601858139038\n",
      "Agent scores: [-2.66  2.66]\n",
      "Agent scores: [-3.41  3.41]\n",
      "Agent scores: [-2.21  2.21]\n",
      "Agent scores: [-3.81  3.81]\n",
      "Agent scores: [-2.82  2.82]\n",
      "Agent scores: [-3.23  3.23]\n",
      "Episode: 10300 Timestep: 10300 Total reward Agent: -65.8 Total reward Env: -59.52 Agent Win Margin: -6.279999999999994 Agent Wins: 8638 Agent Draws: 42 Agent Losses: 11922 Episode length: 63.66 mCriticLoss: 1.3677690029144287 actorLoss: 0.99947589635849\n",
      "Agent scores: [-3.46  3.46]\n",
      "Agent scores: [-2.15  2.15]\n",
      "Agent scores: [-1.65  1.65]\n",
      "Agent scores: [-3.01  3.01]\n",
      "Agent scores: [-2.83  2.83]\n",
      "Agent scores: [-3.5  3.5]\n",
      "Episode: 10900 Timestep: 10900 Total reward Agent: -64.74 Total reward Env: -57.56 Agent Win Margin: -7.179999999999993 Agent Wins: 9108 Agent Draws: 46 Agent Losses: 12648 Episode length: 62.15 mCriticLoss: 1.1741948127746582 actorLoss: 0.9966557621955872\n",
      "Agent scores: [-1.9  1.9]\n",
      "Episode: 11000 Timestep: 11000 Total reward Agent: -69.63 Total reward Env: -65.83 Agent Win Margin: -3.799999999999997 Agent Wins: 9198 Agent Draws: 46 Agent Losses: 12758 Episode length: 68.73 mCriticLoss: 1.2726086378097534 actorLoss: 0.9995592832565308\n",
      "Agent scores: [-0.92  0.92]\n",
      "Agent scores: [-4.16  4.16]\n",
      "Agent scores: [-3.01  3.01]\n",
      "Agent scores: [-3.32  3.32]\n",
      "Episode: 11400 Timestep: 11400 Total reward Agent: -66.43 Total reward Env: -60.01 Agent Win Margin: -6.420000000000009 Agent Wins: 9510 Agent Draws: 46 Agent Losses: 13246 Episode length: 64.22 mCriticLoss: 0.9778274297714233 actorLoss: 1.0050643682479858\n",
      "Agent scores: [-2.6  2.6]\n",
      "Episode: 11500 Timestep: 11500 Total reward Agent: -60.99 Total reward Env: -55.79 Agent Win Margin: -5.200000000000003 Agent Wins: 9602 Agent Draws: 46 Agent Losses: 13354 Episode length: 59.39 mCriticLoss: 1.561915636062622 actorLoss: 1.0016238689422607\n",
      "Agent scores: [-1.76  1.76]\n",
      "Agent scores: [-2.84  2.84]\n",
      "Agent scores: [-0.7  0.7]\n",
      "Agent scores: [-2.02  2.02]\n",
      "Agent scores: [-1.17  1.17]\n",
      "Agent scores: [-2.67  2.67]\n",
      "Agent scores: [-2.17  2.17]\n",
      "Episode: 12200 Timestep: 12200 Total reward Agent: -65.33 Total reward Env: -61.21 Agent Win Margin: -4.119999999999997 Agent Wins: 10208 Agent Draws: 52 Agent Losses: 14142 Episode length: 64.27 mCriticLoss: 1.7604718208312988 actorLoss: 0.9985411763191223\n",
      "Agent scores: [-1.22  1.22]\n",
      "Agent scores: [-2.94  2.94]\n",
      "Agent scores: [-3.58  3.58]\n",
      "Agent scores: [-3.87  3.87]\n",
      "Agent scores: [-3.32  3.32]\n",
      "Agent scores: [-2.83  2.83]\n",
      "Agent scores: [-2.81  2.81]\n",
      "Agent scores: [-0.53  0.53]\n",
      "Agent scores: [-2.83  2.83]\n",
      "Episode: 13100 Timestep: 13100 Total reward Agent: -71.78 Total reward Env: -66.34 Agent Win Margin: -5.439999999999998 Agent Wins: 10918 Agent Draws: 56 Agent Losses: 15228 Episode length: 70.06 mCriticLoss: 0.7844349145889282 actorLoss: 0.9997771978378296\n",
      "Agent scores: [-1.95  1.95]\n",
      "Agent scores: [-3.19  3.19]\n",
      "Agent scores: [-3.05  3.05]\n",
      "Episode: 13400 Timestep: 13400 Total reward Agent: -70.07 Total reward Env: -64.15 Agent Win Margin: -5.9199999999999875 Agent Wins: 11152 Agent Draws: 58 Agent Losses: 15592 Episode length: 68.11 mCriticLoss: 0.7841082215309143 actorLoss: 0.99638432264328\n",
      "Agent scores: [-2.54  2.54]\n",
      "Agent scores: [-3.05  3.05]\n",
      "Agent scores: [-3.01  3.01]\n",
      "Agent scores: [-2.56  2.56]\n",
      "Episode: 13800 Timestep: 13800 Total reward Agent: -74.7 Total reward Env: -69.58 Agent Win Margin: -5.1200000000000045 Agent Wins: 11464 Agent Draws: 58 Agent Losses: 16080 Episode length: 73.14 mCriticLoss: 1.3706239461898804 actorLoss: 1.0067064762115479\n",
      "Agent scores: [-3.31  3.31]\n",
      "Agent scores: [-2.08  2.08]\n",
      "Agent scores: [-3.25  3.25]\n",
      "Agent scores: [-2.34  2.34]\n",
      "Agent scores: [-1.56  1.56]\n",
      "Agent scores: [-2.56  2.56]\n",
      "Episode: 14400 Timestep: 14400 Total reward Agent: -71.42 Total reward Env: -66.3 Agent Win Margin: -5.1200000000000045 Agent Wins: 11938 Agent Draws: 58 Agent Losses: 16806 Episode length: 69.86 mCriticLoss: 1.273179054260254 actorLoss: 0.9966193437576294\n",
      "Agent scores: [-3.01  3.01]\n",
      "Agent scores: [-3.14  3.14]\n",
      "Episode: 14600 Timestep: 14600 Total reward Agent: -61.38 Total reward Env: -55.28 Agent Win Margin: -6.100000000000001 Agent Wins: 12090 Agent Draws: 58 Agent Losses: 17054 Episode length: 59.33 mCriticLoss: 1.075392723083496 actorLoss: 0.9972500205039978\n",
      "Agent scores: [-1.95  1.95]\n",
      "Agent scores: [-2.16  2.16]\n",
      "Agent scores: [-2.5  2.5]\n",
      "Agent scores: [-3.35  3.35]\n",
      "Agent scores: [-1.99  1.99]\n",
      "Episode: 15100 Timestep: 15100 Total reward Agent: -78.34 Total reward Env: -74.58 Agent Win Margin: -3.760000000000005 Agent Wins: 12512 Agent Draws: 60 Agent Losses: 17630 Episode length: 77.46 mCriticLoss: 1.0782616138458252 actorLoss: 0.9980831146240234\n",
      "Agent scores: [-2.94  2.94]\n",
      "Agent scores: [-2.09  2.09]\n",
      "Episode: 15300 Timestep: 15300 Total reward Agent: -63.54 Total reward Env: -59.58 Agent Win Margin: -3.960000000000001 Agent Wins: 12668 Agent Draws: 60 Agent Losses: 17874 Episode length: 62.56 mCriticLoss: 0.7817850112915039 actorLoss: 1.0025177001953125\n",
      "Agent scores: [-2.85  2.85]\n",
      "Episode: 15400 Timestep: 15400 Total reward Agent: -68.36 Total reward Env: -62.66 Agent Win Margin: -5.700000000000003 Agent Wins: 12744 Agent Draws: 60 Agent Losses: 17998 Episode length: 66.51 mCriticLoss: 1.3737404346466064 actorLoss: 1.002243995666504\n",
      "Agent scores: [-2.81  2.81]\n",
      "Agent scores: [-1.03  1.03]\n",
      "Agent scores: [-2.76  2.76]\n",
      "Agent scores: [-3.36  3.36]\n",
      "Episode: 15800 Timestep: 15800 Total reward Agent: -63.17 Total reward Env: -56.67 Agent Win Margin: -6.5 Agent Wins: 13056 Agent Draws: 62 Agent Losses: 18484 Episode length: 60.92 mCriticLoss: 1.173231840133667 actorLoss: 1.0044668912887573\n",
      "Agent scores: [-4.34  4.34]\n",
      "Agent scores: [-1.74  1.74]\n",
      "Episode: 16000 Timestep: 16000 Total reward Agent: -61.78 Total reward Env: -58.52 Agent Win Margin: -3.259999999999998 Agent Wins: 13206 Agent Draws: 62 Agent Losses: 18734 Episode length: 61.15 mCriticLoss: 1.075609803199768 actorLoss: 0.9945194125175476\n",
      "Agent scores: [-3.74  3.74]\n",
      "Agent scores: [-2.7  2.7]\n",
      "Episode: 16200 Timestep: 16200 Total reward Agent: -75.23 Total reward Env: -70.05 Agent Win Margin: -5.180000000000007 Agent Wins: 13356 Agent Draws: 62 Agent Losses: 18984 Episode length: 73.64 mCriticLoss: 1.1735506057739258 actorLoss: 1.0010071992874146\n",
      "Agent scores: [-2.42  2.42]\n",
      "Episode: 16300 Timestep: 16300 Total reward Agent: -66.61 Total reward Env: -61.77 Agent Win Margin: -4.839999999999996 Agent Wins: 13438 Agent Draws: 64 Agent Losses: 19100 Episode length: 65.19 mCriticLoss: 1.5621033906936646 actorLoss: 1.0017569065093994\n",
      "Agent scores: [-2.15  2.15]\n",
      "Agent scores: [-2.1  2.1]\n",
      "Agent scores: [-1.44  1.44]\n",
      "Agent scores: [ 0.14 -0.14]\n",
      "Agent scores: [-3.13  3.13]\n",
      "Episode: 16800 Timestep: 16800 Total reward Agent: -75.18 Total reward Env: -69.14 Agent Win Margin: -6.040000000000006 Agent Wins: 13876 Agent Draws: 72 Agent Losses: 19654 Episode length: 73.16 mCriticLoss: 1.467534065246582 actorLoss: 0.9966433048248291\n",
      "Agent scores: [-2.3  2.3]\n",
      "Agent scores: [-2.41  2.41]\n",
      "Agent scores: [-2.19  2.19]\n",
      "Agent scores: [-2.41  2.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent scores: [-1.44  1.44]\n",
      "Agent scores: [-0.16  0.16]\n",
      "Agent scores: [-3.17  3.17]\n",
      "Agent scores: [-1.72  1.72]\n",
      "Agent scores: [-2.52  2.52]\n",
      "Agent scores: [-1.37  1.37]\n",
      "Agent scores: [-2.43  2.43]\n",
      "Agent scores: [-2.5  2.5]\n",
      "Agent scores: [-2.93  2.93]\n",
      "Episode: 18100 Timestep: 18100 Total reward Agent: -73.98 Total reward Env: -68.34 Agent Win Margin: -5.640000000000001 Agent Wins: 14992 Agent Draws: 78 Agent Losses: 21132 Episode length: 72.16 mCriticLoss: 1.8608219623565674 actorLoss: 0.9992356896400452\n",
      "Agent scores: [-0.21  0.21]\n",
      "Agent scores: [-1.24  1.24]\n",
      "Episode: 18300 Timestep: 18300 Total reward Agent: -77.74 Total reward Env: -75.48 Agent Win Margin: -2.259999999999991 Agent Wins: 15200 Agent Draws: 78 Agent Losses: 21324 Episode length: 77.61 mCriticLoss: 1.8583972454071045 actorLoss: 1.0005505084991455\n",
      "Agent scores: [-1.67  1.67]\n",
      "Episode: 18400 Timestep: 18400 Total reward Agent: -66.83 Total reward Env: -63.49 Agent Win Margin: -3.3399999999999963 Agent Wins: 15286 Agent Draws: 78 Agent Losses: 21438 Episode length: 66.16 mCriticLoss: 1.0749683380126953 actorLoss: 1.0016883611679077\n",
      "Agent scores: [-1.32  1.32]\n",
      "Agent scores: [-1.56  1.56]\n",
      "Agent scores: [-2.89  2.89]\n",
      "Agent scores: [-1.14  1.14]\n",
      "Agent scores: [-0.84  0.84]\n",
      "Agent scores: [-1.1  1.1]\n",
      "Agent scores: [-1.1  1.1]\n",
      "Agent scores: [-1.52  1.52]\n",
      "Episode: 19200 Timestep: 19200 Total reward Agent: -71.29 Total reward Env: -68.47 Agent Win Margin: -2.8200000000000074 Agent Wins: 16034 Agent Draws: 84 Agent Losses: 22284 Episode length: 70.88 mCriticLoss: 0.6862360239028931 actorLoss: 0.99370276927948\n",
      "Agent scores: [-2.01  2.01]\n",
      "Episode: 19300 Timestep: 19300 Total reward Agent: -73.65 Total reward Env: -69.63 Agent Win Margin: -4.02000000000001 Agent Wins: 16122 Agent Draws: 84 Agent Losses: 22396 Episode length: 72.64 mCriticLoss: 0.7828161120414734 actorLoss: 0.9993948936462402\n",
      "Agent scores: [-2.15  2.15]\n",
      "Episode: 19400 Timestep: 19400 Total reward Agent: -69.57 Total reward Env: -65.23 Agent Win Margin: -4.339999999999989 Agent Wins: 16212 Agent Draws: 84 Agent Losses: 22506 Episode length: 68.4 mCriticLoss: 0.8792842030525208 actorLoss: 1.0105891227722168\n",
      "Agent scores: [-1.83  1.83]\n",
      "Agent scores: [-2.05  2.05]\n",
      "Agent scores: [-2.92  2.92]\n",
      "Episode: 19700 Timestep: 19700 Total reward Agent: -68.14 Total reward Env: -62.7 Agent Win Margin: -5.439999999999998 Agent Wins: 16472 Agent Draws: 86 Agent Losses: 22844 Episode length: 66.42 mCriticLoss: 1.3721961975097656 actorLoss: 0.9990162253379822\n",
      "Agent scores: [-2.04  2.04]\n",
      "Agent scores: [-2.45  2.45]\n",
      "Agent scores: [-4.7  4.7]\n",
      "Agent scores: [-3.05  3.05]\n",
      "Agent scores: [-1.77  1.77]\n",
      "Episode: 20200 Timestep: 20200 Total reward Agent: -69.09 Total reward Env: -65.77 Agent Win Margin: -3.3200000000000074 Agent Wins: 16876 Agent Draws: 86 Agent Losses: 23440 Episode length: 68.43 mCriticLoss: 1.273364543914795 actorLoss: 0.9965298175811768\n",
      "Agent scores: [-3.14  3.14]\n",
      "Agent scores: [-1.96  1.96]\n",
      "Agent scores: [-2.27  2.27]\n",
      "Agent scores: [-2.52  2.52]\n",
      "Agent scores: [-3.02  3.02]\n",
      "Agent scores: [-3.28  3.28]\n",
      "Agent scores: [-2.64  2.64]\n",
      "Agent scores: [-3.88  3.88]\n",
      "Agent scores: [-2.18  2.18]\n",
      "Episode: 21100 Timestep: 21100 Total reward Agent: -61.57 Total reward Env: -57.03 Agent Win Margin: -4.539999999999999 Agent Wins: 17572 Agent Draws: 88 Agent Losses: 24542 Episode length: 60.3 mCriticLoss: 0.7847108840942383 actorLoss: 1.0027652978897095\n",
      "Agent scores: [-2.92  2.92]\n",
      "Agent scores: [-2.77  2.77]\n",
      "Agent scores: [-1.52  1.52]\n",
      "Episode: 21400 Timestep: 21400 Total reward Agent: -69.66 Total reward Env: -66.62 Agent Win Margin: -3.039999999999992 Agent Wins: 17814 Agent Draws: 90 Agent Losses: 24898 Episode length: 69.14 mCriticLoss: 0.7848780751228333 actorLoss: 0.9970542192459106\n",
      "Agent scores: [-1.5  1.5]\n",
      "Agent scores: [-1.57  1.57]\n",
      "Agent scores: [-3.47  3.47]\n",
      "Episode: 21700 Timestep: 21700 Total reward Agent: -66.14 Total reward Env: -59.38 Agent Win Margin: -6.759999999999998 Agent Wins: 18072 Agent Draws: 90 Agent Losses: 25240 Episode length: 63.76 mCriticLoss: 0.6839399337768555 actorLoss: 1.0019410848617554\n",
      "Agent scores: [-2.5  2.5]\n",
      "Episode: 21800 Timestep: 21800 Total reward Agent: -62.45 Total reward Env: -57.49 Agent Win Margin: -4.960000000000001 Agent Wins: 18158 Agent Draws: 90 Agent Losses: 25354 Episode length: 60.97 mCriticLoss: 1.560373306274414 actorLoss: 1.0021860599517822\n",
      "Agent scores: [-2.61  2.61]\n",
      "Agent scores: [-1.66  1.66]\n",
      "Agent scores: [-2.6  2.6]\n",
      "Agent scores: [-2.88  2.88]\n",
      "Agent scores: [-2.82  2.82]\n",
      "Agent scores: [-3.  3.]\n",
      "Agent scores: [-3.53  3.53]\n",
      "Agent scores: [-2.23  2.23]\n",
      "Agent scores: [-1.52  1.52]\n",
      "Agent scores: [-2.7  2.7]\n",
      "Agent scores: [-3.34  3.34]\n",
      "Agent scores: [-2.84  2.84]\n",
      "Agent scores: [-3.63  3.63]\n",
      "Agent scores: [-2.76  2.76]\n",
      "Agent scores: [-1.11  1.11]\n",
      "Agent scores: [-0.25  0.25]\n",
      "Episode: 23400 Timestep: 23400 Total reward Agent: -60.99 Total reward Env: -60.27 Agent Win Margin: -0.7199999999999989 Agent Wins: 19456 Agent Draws: 102 Agent Losses: 27244 Episode length: 61.63 mCriticLoss: 1.2716079950332642 actorLoss: 0.9912071228027344\n",
      "Agent scores: [-3.49  3.49]\n",
      "Agent scores: [-2.36  2.36]\n",
      "Episode: 23600 Timestep: 23600 Total reward Agent: -72.52 Total reward Env: -68.02 Agent Win Margin: -4.5 Agent Wins: 19604 Agent Draws: 102 Agent Losses: 27496 Episode length: 71.27 mCriticLoss: 1.0792853832244873 actorLoss: 0.9999643564224243\n",
      "Agent scores: [-1.28  1.28]\n",
      "Agent scores: [-0.83  0.83]\n",
      "Agent scores: [-1.6  1.6]\n",
      "Agent scores: [-2.5  2.5]\n",
      "Agent scores: [-1.4  1.4]\n",
      "Episode: 24100 Timestep: 24100 Total reward Agent: -61.85 Total reward Env: -58.87 Agent Win Margin: -2.980000000000004 Agent Wins: 20050 Agent Draws: 110 Agent Losses: 28042 Episode length: 61.36 mCriticLoss: 1.0789544582366943 actorLoss: 1.003395438194275\n",
      "Agent scores: [-1.21  1.21]\n",
      "Agent scores: [-4.01  4.01]\n",
      "Agent scores: [-2.69  2.69]\n",
      "Agent scores: [-2.7  2.7]\n",
      "Episode: 24500 Timestep: 24500 Total reward Agent: -71.26 Total reward Env: -66.08 Agent Win Margin: -5.180000000000007 Agent Wins: 20378 Agent Draws: 112 Agent Losses: 28512 Episode length: 69.67 mCriticLoss: 1.079785943031311 actorLoss: 1.0029847621917725\n",
      "Agent scores: [-2.25  2.25]\n",
      "Agent scores: [-2.1  2.1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from agent import Agent\n",
    "from model import ActorNetwork, CriticNetwork, MCritic\n",
    "from replaybuffer import ReplayBuffer\n",
    "from plotutils import plot_results\n",
    "\n",
    "from games  import games\n",
    "from games import games\n",
    "from game   import Game\n",
    "from player import Player_Template\n",
    "from team   import Team\n",
    "from userinput import userInput\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    global cycle\n",
    "    global HISTORY_FILE\n",
    "    user_input = userInput()\n",
    "    run_trials(user_input)\n",
    "    \n",
    "    \n",
    "\n",
    "#main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_p0.actor_network.actor_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_board = []\n",
    "for i in [0,1]:\n",
    "    score_board.append([(x[0],x[i+1]) for x in scores])\n",
    "#score_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  team_scores = []\n",
    "  min_score = 99999999\n",
    "  max_score = 0\n",
    "\n",
    "  for j in range(user_input.num_sides):\n",
    "    team_scores.append([team_points for cycle, team_points in score_board[j]])\n",
    "    plot.hist(\n",
    "      team_scores[j],\n",
    "      bins=100,\n",
    "      label=user_input.teams[j][\"team_name\"],\n",
    "      color=user_input.teams[j][\"color\"])\n",
    "\n",
    "    min_j_score = min(team_scores[j])\n",
    "    max_j_score = max(team_scores[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins   = args['WINS_DRAWS_LOSSES'][0]\n",
    "draws  = args['WINS_DRAWS_LOSSES'][1]\n",
    "losses = args['WINS_DRAWS_LOSSES'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " args['WINS_DRAWS_LOSSES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.hist(wins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent_p0.actor_network.state_dict(), 'checkpoint_p0_oneSided.pth')\n",
    "torch.save(agent_p1.actor_network.state_dict(), 'checkpoint_p1_oneSided.pth')\n",
    "torch.save(mCritic.network.state_dict(), 'checkpoint_mCritic_oneSided.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
