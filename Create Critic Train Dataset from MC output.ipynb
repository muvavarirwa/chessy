{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pos_numeric_name = {\n",
    "    (0, 0): 1,\n",
    "    (0, 1): 2,\n",
    "    (0, 2): 3,\n",
    "    (0, 3): 4,\n",
    "    (0, 4): 5,\n",
    "    (0, 5): 6,\n",
    "    (0, 6): 7,\n",
    "    (0, 7): 8,\n",
    "    (1, 0): 9,\n",
    "    (1, 1): 10,\n",
    "    (1, 2): 11,\n",
    "    (1, 3): 12,\n",
    "    (1, 4): 13,\n",
    "    (1, 5): 14,\n",
    "    (1, 6): 15,\n",
    "    (1, 7): 16,\n",
    "    (6, 0): -16,\n",
    "    (6, 1): -15,\n",
    "    (6, 2): -14,\n",
    "    (6, 3): -13,\n",
    "    (6, 4): -12,\n",
    "    (6, 5): -11,\n",
    "    (6, 6): -10,\n",
    "    (6, 7): -9,\n",
    "    (7, 0): -8,\n",
    "    (7, 1): -7,\n",
    "    (7, 2): -6,\n",
    "    (7, 3): -5,\n",
    "    (7, 4): -4,\n",
    "    (7, 5): -3,\n",
    "    (7, 6): -2,\n",
    "    (7, 7): -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ubuntu ubuntu 1.4G Jun 28 08:39 /data_data/reinforcement_learning/results/combined_file.bak.gz\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  69G Jun 28 08:47 /data_data/reinforcement_learning/results/combined_policy_file\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  25G Jun 28 08:54 /data_data/reinforcement_learning/results/combined_policy_file.bak\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  59G Jun 28 09:01 /data_data/reinforcement_learning/results/combined_file\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 1.5G Jun 28 09:03 /data_data/reinforcement_learning/results/combined_policy_file.bak.gz\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 1.7G Jun 28 09:06 /data_data/reinforcement_learning/results/combined_file.gz\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 469M Aug 13 00:33 /data_data/reinforcement_learning/results/combined_policy_file_1k\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 463M Aug 13 00:34 /data_data/reinforcement_learning/results/combined_policy_file_1M\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alrth /data_data/reinforcement_learning/results/comb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10240000 /data_data/reinforcement_learning/results/combined_policy_file >> /data_data/reinforcement_learning/results/combined_policy_file_10M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"state_prior\":\"000000000000000000000000000000000000000000000000000000000011011100000110000000011111000000000000000001000000001100000000001101000000100001100000000000000000000000000101000000000000011110000000000010000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000010010000000010011010100000000010101011000011001\",\"expected_value\":4.166666666666667,\"policy\":\"(Pawn,(1,0),(1,2),(2,2))\"}\r\n",
      "{\"state_prior\":\"000000000000000000000000000000000000000000000000000000000011011100000110000000011111000000000000000001000000001100000000001101000000100001100000000000000000000000000101000000000000011110000000000010000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000010010000000010011010100000000010101011000011001\",\"expected_value\":0.2331338002025748,\"policy\":\"(Pawn,(1,0),(1,2),(2,2))\"}\r\n",
      "{\"state_prior\":\"000000000000000000000000000000000000000000000000000000011011000000000000001111000110000111000000010010011100000010000000000000000000000000000000011010000000011101001100011111001101000000000000000000001011000000000000000101000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001110000000000000000000001001010101010011000000000000000000000000011001\",\"expected_value\":8.333333333333334,\"policy\":\"(Pawn,(1,1),(1,1),(2,2))\"}\r\n",
      "{\"state_prior\":\"000000000000000000000000000000000000000000000000000000011011000000001010011110000100000000011001000000000000011100000000011111000000000000100000000000001100000000000000000000000000000000000000000000000000000000001101000101000000000000000000000000000000000000000000000000000000010111001110010010000000000000000000000000000000000000010000011000000000010101000000000000000000001111010110\",\"expected_value\":-2.19,\"policy\":\"(Bishop,(1,-1),(5,6),(6,5))\"}\r\n",
      "{\"state_prior\":\"000000000000000000000000000000000000000000000000000000011011011100000000000000011111001101000000000100011010010100000011000000000000000000010000000000000000000000000000000000000111000000100001000000000000001100000000000000000000000000000000000010000000000000000000000000000000001000000000000000000000000000000000000000010111000000000000010010000000000000000000010011010110010101000000\",\"expected_value\":8.333333333333334,\"policy\":\"(Bishop,(1,-1),(2,2),(3,1))\"}\r\n",
      "{\"state_prior\":\"000000000000000000000000000000000000000000000000000000011011011100000000000000011111100000000000000000000000000000000011010100010111000111000000001100000000000000000100000000000000000000000000000000000000000000000101000000000000000000010000001001000000011010000000000000000000000000000000000000000000000000000000000000000000000000000000000000010101010011000000000000011000000000000000\",\"expected_value\":-1.7282475249,\"policy\":\"(Bishop,(1,-1),(2,5),(3,4))\"}\r\n",
      "{\"state_prior\":\"000000000000000000000000000000000000000000000000001111011011000000000000000000011111100000011001011010011100000000000000011101011110001000100001000000010100000011000000000000000000000000000111000010000000000000000000000000000000000000000000000000000000000000000000001100000000000000001110000000000000000000000000000000000000000000000000010111010011011000000000010101000000000000010000\",\"expected_value\":-1.7282475249,\"policy\":\"(Bishop,(1,1),(3,1),(4,2))\"}\r\n",
      "{\"state_prior\":\"000000000000000000000000000000000000000000000000010010000000000000011101010111011111100000011001011010000101000110000000000000001000000000100001000000000000000000000000001100000100000000000000000010000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000000000000010110000000000000000000000000010011010101011100010100011000000000000000000000\",\"expected_value\":8.333333333333334,\"policy\":\"(Bishop,(1,1),(1,4),(2,5))\"}\r\n",
      "{\"state_prior\":\"000000000000000000000000000000000000000000000000010010000000000000011101011110010110000000011001011010001011000000000000000010000111011111100001000000000000000000000000000000001100000000000000000000000000000000000000000000000000000101000000001001000000000100000000000000000000000000000000000000000000000000000000000000000000000000010000000000010100010101000000010011001110010111000000\",\"expected_value\":-2.043,\"policy\":\"(Queen,(1,-1),(1,5),(2,4))\"}\r\n",
      "{\"state_prior\":\"000000000000000000000000000000000000000000000000010010000000011100000000011110001111000101000000011011000000000000000000011101001000000111011001000000000000000000000000000000000000000000001100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001011000000000000000000000000000000010000010011010100010110010101011000000000000000000000\",\"expected_value\":4.166666666666667,\"policy\":\"(Rook,(1,0),(2,7),(3,7))\"}\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 /data_data/reinforcement_learning/results/combined_policy_file_10M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file = '/data_data/reinforcement_learning/results/combined_policy_file_10M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pctl       = 0.65\n",
    "validation_pctl  = 0.80\n",
    "test_pctl        = 0.20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state  = None\n",
    "value  = None\n",
    "action = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactor as Spark job..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import ast\n",
    "\n",
    "from itertools import cycle, compress\n",
    "\n",
    "from sparse_action_dict import sparse_action_dict\n",
    "\n",
    "partition = {}\n",
    "labels    = {}\n",
    "\n",
    "partition['train']      = []\n",
    "partition['validation'] = []\n",
    "\n",
    "partitions = ['train','validation']\n",
    "\n",
    "with open(src_file,'r') as policy_file:\n",
    "    policies_        = policy_file.readlines()\n",
    "    num_policies     = len(policies_)\n",
    "    train_cut_pt     = int(train_pctl*num_policies)\n",
    "    val_cut_pt       = int(validation_pctl*num_policies)\n",
    "    policy_num       = 0\n",
    "    \n",
    "    for part in partitions:\n",
    "        if part      == 'train':\n",
    "            policies = policies_[:train_cut_pt]\n",
    "        if part      == 'validation':\n",
    "            policies = policies_[train_cut_pt:val_cut_pt]\n",
    "    \n",
    "        for policy_ in policies:\n",
    "            policy   = ast.literal_eval(policy_)\n",
    "            state, value, action = policy.values()\n",
    "            state    = np.array(list(map(int, state))).astype(np.float32)\n",
    "            action   = [int(x) for x in action.replace(\"(\",\"\").replace(\")\",\"\").split(\",\")[1:]]\n",
    "            \n",
    "            start_pos, curr_pos, next_pos   = list(map(list, zip(compress(action, cycle([1, 0])), compress(action, cycle([0, 1])))))\n",
    "            \n",
    "            move0 = tuple(next_pos)[0] - tuple(curr_pos)[0]\n",
    "            move1 = tuple(next_pos)[1] - tuple(curr_pos)[1]\n",
    "            \n",
    "            try:\n",
    "                sparse_move = str(start_pos_numeric_name[tuple(start_pos)]) + \",\" + str(move0) + \",\" + str(move1)\n",
    "            \n",
    "                action_ = np.array(list(map(int, sparse_action_dict[sparse_move]))).astype(np.float32)\n",
    "\n",
    "\n",
    "                state_action = str(np.concatenate((state,action_), axis=0))\n",
    "\n",
    "\n",
    "                if not (state_action) in partition[part]:\n",
    "                    partition[part].append(state_action)\n",
    "                    labels.update({state_action:value})\n",
    "                else:\n",
    "                    curr_policy_value = labels[state_action]\n",
    "                    if value > curr_policy_value:\n",
    "                        labels.update({state_action:value})\n",
    "                    else:\n",
    "                        pass\n",
    "            except:\n",
    "                #Pass for now -- but a big issue here....\n",
    "                pass\n",
    "                \n",
    "            policy_num += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_ = {}\n",
    "partition_['train']      = []\n",
    "partition_['validation'] = []\n",
    "parts = ['train','validation']\n",
    "\n",
    "for part in parts:\n",
    "\n",
    "    for state_ in partition[part]:\n",
    "        state = np.array([int(x) for x in np.array(state_.strip().replace(\"\\n\",\"\").replace(\".\",\"\").replace(\"''\",\"\").replace(\"]\",\"\").replace(\"[\",\"\").split(\" \"))])\n",
    "        partition_[part].append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ = {}\n",
    "\n",
    "for state_action_, value in labels.items():\n",
    "    state_action = str([int(x) for x in np.array(state_action_.strip().replace(\"\\n\",\"\").replace(\".\",\"\").replace(\"''\",\"\").replace(\"]\",\"\").replace(\"[\",\"\").split(\" \"))])\n",
    "    labels_.update({state_action:value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_policies     = len(policies_)\n",
    "train_cut_pt     = int(train_pctl*num_policies)\n",
    "val_cut_pt       = int(validation_pctl*num_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "From https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile my_classes.py\n",
    "\n",
    "import torch\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, list_IDs, labels):\n",
    "        'Initialization'\n",
    "        self.labels   = labels\n",
    "        self.list_IDs = list_IDs\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = torch.tensor( ID + '.pt')\n",
    "        y = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to modify our PyTorch script accordingly so that it accepts the generator that we just created. In order to do so, we use PyTorch's DataLoader class, which in addition to our Dataset class, also takes in the following important arguments:\n",
    "\n",
    "- batch_size:  Denotes the number of samples contained in each generated batch.\n",
    "- shuffle:     If set to True, we will get a new order of exploration at each pass (or just keep a linear exploration scheme otherwise). Shuffling the order in which examples are fed to the classifier is helpful so that batches between epochs do not look alike. Doing so will eventually make our model more robust.\n",
    "- num_workers: Denotes the number of processes that generate batches in parallel. A high enough number of workers assures that CPU computations are efficiently managed, i.e. that the bottleneck is indeed the neural network's forward and backward operations on the GPU (and not data generation).\n",
    "\n",
    "A proposition of code template that you can write in your script is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from model import MCritic\n",
    "from replaybuffer import ReplayBuffer\n",
    "from args import args\n",
    "\n",
    "mCritic       =  MCritic(396)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mCritic.network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from my_classes import Dataset\n",
    "from random import sample\n",
    "from operator import itemgetter\n",
    "\n",
    "sys.path.append('./anaconda3/lib/python3.7/site-packages/torchvision')\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(comment=\"MADDPG Chessy II\")\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cuda:1\")\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 1024,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 2}\n",
    "\n",
    "max_epochs = 100000\n",
    "\n",
    "num_policies       = len(labels.keys())\n",
    "train_cut_pt       = int(train_pctl*num_policies)\n",
    "val_cut_pt         = int(validation_pctl*num_policies)\n",
    "training_generator = None\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    global training_generator\n",
    "    missing_train_samples = 0\n",
    "    missing_val_samples = 0\n",
    "    partition      = partition_\n",
    "    labels         = labels_\n",
    "    sample_train   = list(np.arange(0,len(partition['train']),1))\n",
    "    sample_val     = list(np.arange(0,len(partition['validation']),1))\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # Training\n",
    "        train_idxs   = sample(sample_train, params['batch_size'])\n",
    "            \n",
    "        train_data   = torch.tensor([x for x in map(partition['train'].__getitem__, (tuple(train_idxs)))]).float().to(device)\n",
    "            \n",
    "        train_labels = torch.tensor([labels_[str(list(partition_['train'][x]))] for x in train_idxs]).float().reshape((params['batch_size'],)).to(device)\n",
    "                    \n",
    "        local_batch  = train_data.to(device)\n",
    "        \n",
    "        local_labels = train_labels.view(params['batch_size'], -1).to(device)\n",
    "        \n",
    "        Q_targets    = local_labels\n",
    "\n",
    "        Q_expected   = mCritic.network(local_batch)\n",
    "        \n",
    "        mCritic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        mCriticLoss  = mCritic_loss\n",
    "\n",
    "        \n",
    "        # Minimize the loss\n",
    "        mCritic.optimizer.zero_grad()\n",
    "        mCritic_loss.backward()\n",
    "        mCritic.optimizer.step()\n",
    "            \n",
    "        # Validation\n",
    "        with torch.set_grad_enabled(False):\n",
    "            \n",
    "            val_idxs     = sample(sample_val, params['batch_size'])\n",
    "        \n",
    "            val_data     = torch.tensor([x for x in map(partition['validation'].__getitem__, (tuple(val_idxs)))]).float().to(device)\n",
    "\n",
    "            val_labels   = torch.tensor([labels_[str(list(partition_['validation'][x]))] for x in val_idxs]).float().reshape((params['batch_size'],)).to(device)\n",
    "\n",
    "            val_batch    = val_data.to(device) \n",
    "            \n",
    "            val_labels   = val_labels.view(params['batch_size'], -1).to(device)\n",
    "            \n",
    "            V_targets    = val_labels\n",
    "            \n",
    "            V_expected   = mCritic.network(val_batch)\n",
    "            \n",
    "            mCritic_val_loss = F.mse_loss(V_expected, V_targets)\n",
    "\n",
    "            \n",
    "        if epoch % 100 == 0 and epoch > 0:\n",
    "            num_iter   = epoch\n",
    "            writer.add_scalars('mCritic_loss_II', {\n",
    "                'mCritic_train_loss': mCritic_loss, \n",
    "                'mCritic_val_loss': mCritic_val_loss,\n",
    "            },  num_iter)\n",
    "            print(\"Epoch:\\t{}\\tmCritic_train_loss:\\t{}\\tmCritic_val_loss:\\t{}\\n\".format(epoch,mCritic_loss, mCritic_val_loss))\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feasible_moves = [('Pawn',(1,0),(1,2),(2,2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_names = {'w_R0': 1, 'w_K0': 2, 'w_B0': 3, 'w__K': 4, 'w__Q': 5, 'w_B1': 6, 'w_K1': 7, 'w_R1': 8, 'w_P0': 9, 'w_P1':\n",
    " 10, 'w_P2': 11, 'w_P3': 12, 'w_P4': 13, 'w_P5': 14, 'w_P6': 15, 'w_P7': 16, 'b_P0': -16, 'b_P1': -15, 'b_P2': -14, 'b_P3':\n",
    " -13, 'b_P4': -12, 'b_P5': -11, 'b_P6': -10, 'b_P7': -9, 'b_R0': -8, 'b_K0': -7, 'b_B0': -6, 'b__K': -5, 'b__Q': -4, 'b_B1'\n",
    ": -3, 'b_K1': -2, 'b_R1': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile args.py\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device  = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cuda:1\")\n",
    "device\n",
    "\n",
    "args = { \n",
    "    \"update_every\":100,\n",
    "    \"window_size\":100,\n",
    "    \"BUFFER_SIZE\":int(1e6),\n",
    "    \"BATCH_SIZE\":1024,  \n",
    "    \"GAMMA\":0.99,\n",
    "    \"TAU\":2e-3,\n",
    "    \"LR_ACTOR\":1e-3,\n",
    "    \"LR_CRITIC\":1.1e-3,\n",
    "    \"WEIGHT_DECAY\":0.0001,\n",
    "    \"UPDATE_EVERY\":5,\n",
    "    \"EXPLORE_NOISE\":0.05,\n",
    "    \"FC1_UNITS\":768,\n",
    "    \"FC2_UNITS\":2048,\n",
    "    \"FC3_UNITS\":256,\n",
    "    \"seed\":0,\n",
    "    \"state_size\":384,\n",
    "    \"action_size\":88,\n",
    "    \"action_size_binary\":12,\n",
    "    \"num_agents\":2,\n",
    "    \"device\":torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cuda:1\"),\n",
    "    'mcritic_path':'/home/ubuntu/chessy/checkpoint_mCritic.pth',\n",
    "    'agent_p0_path':'/home/ubuntu/chessy/checkpoint_p0.pth',\n",
    "    'agent_p1_path':'/home/ubuntu/chessy/checkpoint_p1.pth',\n",
    "    'action_id_dict': {'1,1,0': 0, '2,1,-2': 1, '2,1,2': 2, '2,2,-1': 3, '2,2,1': 4, '3,1,-1': 5, '3,1,1': 6, '4,1,-1': 7, '4,1,0': 8, '4,1,1': 9, '5,1,-1': 10, '5,1,0': 11, '5,1,1': 12, '6,1,-1': 13, '6,1,1': 14, '7,1,-2': 15, '7,1,2': 16, '7,2,-1': 17, '7,2,1': 18, '8,1,0': 19, '9,1,0': 20, '9,1,-1': 21, '9,1,1': 22, '10,1,0': 23, '10,1,-1': 24, '10,1,1': 25, '11,1,0': 26, '11,1,-1': 27, '11,1,1': 28, '12,1,0': 29, '12,1,-1': 30, '12,1,1': 31, '13,1,0': 32, '13,1,-1': 33, '13,1,1': 34, '14,1,0': 35, '14,1,-1': 36, '14,1,1': 37, '15,1,0': 38, '15,1,-1': 39, '15,1,1': 40, '16,1,0': 41, '16,1,-1': 42, '16,1,1': 43, '-16,-1,0': 44, '-16,-1,-1': 45, '-16,-1,1': 46, '-15,-1,0': 47, '-15,-1,-1': 48, '-15,-1,1': 49, '-14,-1,0': 50, '-14,-1,-1': 51, '-14,-1,1': 52, '-13,-1,0': 53, '-13,-1,-1': 54, '-13,-1,1': 55, '-12,-1,0': 56, '-12,-1,-1': 57, '-12,-1,1': 58, '-11,-1,0': 59, '-11,-1,-1': 60, '-11,-1,1': 61, '-10,-1,0': 62, '-10,-1,-1': 63, '-10,-1,1': 64, '-9,-1,0': 65, '-9,-1,-1': 66, '-9,-1,1': 67, '-8,-1,0': 68, '-7,-1,-2': 69, '-7,-1,2': 70, '-7,-2,-1': 71, '-7,-2,1': 72, '-6,-1,-1': 73, '-6,-1,1': 74, '-5,-1,-1': 75, '-5,-1,0': 76, '-5,-1,1': 77, '-4,-1,-1': 78, '-4,-1,0': 79, '-4,-1,1': 80, '-3,-1,-1': 81, '-3,-1,1': 82, '-2,-1,-2': 83, '-2,-1,2': 84, '-2,-2,-1': 85, '-2,-2,1': 86, '-1,-1,0': 87},\n",
    "    'id_action_dict': {0: '1,1,0', 1: '2,1,-2', 2: '2,1,2', 3: '2,2,-1', 4: '2,2,1', 5: '3,1,-1', 6: '3,1,1', 7: '4,1,-1', 8: '4,1,0', 9: '4,1,1', 10: '5,1,-1', 11: '5,1,0', 12: '5,1,1', 13: '6,1,-1', 14: '6,1,1', 15: '7,1,-2', 16: '7,1,2', 17: '7,2,-1', 18: '7,2,1', 19: '8,1,0', 20: '9,1,0', 21: '9,1,-1', 22: '9,1,1', 23: '10,1,0', 24: '10,1,-1', 25: '10,1,1', 26: '11,1,0', 27: '11,1,-1', 28: '11,1,1', 29: '12,1,0', 30: '12,1,-1', 31: '12,1,1', 32: '13,1,0', 33: '13,1,-1', 34: '13,1,1', 35: '14,1,0', 36: '14,1,-1', 37: '14,1,1', 38: '15,1,0', 39: '15,1,-1', 40: '15,1,1', 41: '16,1,0', 42: '16,1,-1', 43: '16,1,1', 44: '-16,-1,0', 45: '-16,-1,-1', 46: '-16,-1,1', 47: '-15,-1,0', 48: '-15,-1,-1', 49: '-15,-1,1', 50: '-14,-1,0', 51: '-14,-1,-1', 52: '-14,-1,1', 53: '-13,-1,0', 54: '-13,-1,-1', 55: '-13,-1,1', 56: '-12,-1,0', 57: '-12,-1,-1', 58: '-12,-1,1', 59: '-11,-1,0', 60: '-11,-1,-1', 61: '-11,-1,1', 62: '-10,-1,0', 63: '-10,-1,-1', 64: '-10,-1,1', 65: '-9,-1,0', 66: '-9,-1,-1', 67: '-9,-1,1', 68: '-8,-1,0', 69: '-7,-1,-2', 70: '-7,-1,2', 71: '-7,-2,-1', 72: '-7,-2,1', 73: '-6,-1,-1', 74: '-6,-1,1', 75: '-5,-1,-1', 76: '-5,-1,0', 77: '-5,-1,1', 78: '-4,-1,-1', 79: '-4,-1,0', 80: '-4,-1,1', 81: '-3,-1,-1', 82: '-3,-1,1', 83: '-2,-1,-2', 84: '-2,-1,2', 85: '-2,-2,-1', 86: '-2,-2,1', 87: '-1,-1,0'},\n",
    "    'numeric_names' : {'w_R0': 1, 'w_K0': 2, 'w_B0': 3, 'w__K': 4, 'w__Q': 5, 'w_B1': 6, 'w_K1': 7, 'w_R1': 8, 'w_P0': 9, 'w_P1': 10, 'w_P2': 11, 'w_P3': 12, 'w_P4': 13, 'w_P5': 14, 'w_P6': 15, 'w_P7': 16, 'b_P0': -16, 'b_P1': -15, 'b_P2': -14, 'b_P3': -13, 'b_P4': -12, 'b_P5': -11, 'b_P6': -10, 'b_P7': -9, 'b_R0': -8, 'b_K0': -7, 'b_B0': -6, 'b__K': -5, 'b__Q': -4, 'b_B1': -3, 'b_K1': -2, 'b_R1': -1},\n",
    "    'initial_state'  : \"010010010011010100010101010110010111011000011001011010011011011100011101011110011111100000100001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000010000011000100000101000110000111001000001001001010001011001100001101001110001111010000\",\n",
    "    'SUMMARY_FILE':\"/data_data/reinforcement_learning/results/summary_file.tsv\",\n",
    "    'HISTORY_FILE':None,\n",
    "    'TAKE_KING_REWARD':10,\n",
    "    'MORE_POINTS_REWARD':1,\n",
    "    'EQUAL_POINTS_REWARD':0,\n",
    "    'STEP_REWARD':-1,\n",
    "    'WINS_DRAWS_LOSSES': [0,0,0],\n",
    "    'in_channels_1':1024,\n",
    "    'in_channels_2':64,\n",
    "    'in_channels_l':128,\n",
    "    'out_channels_1':64,\n",
    "    'out_channels_2':128,\n",
    "    'out_channels_l':16,\n",
    "    'kernel_1_size':(6,1),\n",
    "    'kernel_2_size':(1,1),\n",
    "    'kernel_l_size':(1,1),\n",
    "    'stride_1_size':(6,1),\n",
    "    'stride_2_size':(6,1),\n",
    "    'stride_l_size':(1,1),\n",
    "    'reshape_size':(8,48),\n",
    "    'reshape_buffer':(1,384)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_id_dict = {'1,1,0': 0, '2,1,-2': 1, '2,1,2': 2, '2,2,-1': 3, '2,2,1': 4, '3,1,-1': 5, '3,1,1': 6, '4,1,-1': 7, '4,1,0': 8, '4,1,1': 9, '5,1,-1': 10, '5,1,0': 11, '5,1,1': 12, '6,1,-1': 13, '6,1,1': 14, '7,1,-2': 15, '7,1,2': 16, '7,2,-1': 17, '7,2,1': 18, '8,1,0': 19, '9,1,0': 20, '9,1,-1': 21, '9,1,1': 22, '10,1,0': 23, '10,1,-1': 24, '10,1,1': 25, '11,1,0': 26, '11,1,-1': 27, '11,1,1': 28, '12,1,0': 29, '12,1,-1': 30, '12,1,1': 31, '13,1,0': 32, '13,1,-1': 33, '13,1,1': 34, '14,1,0': 35, '14,1,-1': 36, '14,1,1': 37, '15,1,0': 38, '15,1,-1': 39, '15,1,1': 40, '16,1,0': 41, '16,1,-1': 42, '16,1,1': 43, '-16,-1,0': 44, '-16,-1,-1': 45, '-16,-1,1': 46, '-15,-1,0': 47, '-15,-1,-1': 48, '-15,-1,1': 49, '-14,-1,0': 50, '-14,-1,-1': 51, '-14,-1,1': 52, '-13,-1,0': 53, '-13,-1,-1': 54, '-13,-1,1': 55, '-12,-1,0': 56, '-12,-1,-1': 57, '-12,-1,1': 58, '-11,-1,0': 59, '-11,-1,-1': 60, '-11,-1,1': 61, '-10,-1,0': 62, '-10,-1,-1': 63, '-10,-1,1': 64, '-9,-1,0': 65, '-9,-1,-1': 66, '-9,-1,1': 67, '-8,-1,0': 68, '-7,-1,-2': 69, '-7,-1,2': 70, '-7,-2,-1': 71, '-7,-2,1': 72, '-6,-1,-1': 73, '-6,-1,1': 74, '-5,-1,-1': 75, '-5,-1,0': 76, '-5,-1,1': 77, '-4,-1,-1': 78, '-4,-1,0': 79, '-4,-1,1': 80, '-3,-1,-1': 81, '-3,-1,1': 82, '-2,-1,-2': 83, '-2,-1,2': 84, '-2,-2,-1': 85, '-2,-2,1': 86, '-1,-1,0': 87}\n",
    "\n",
    "id_action_dict = {0: '1,1,0', 1: '2,1,-2', 2: '2,1,2', 3: '2,2,-1', 4: '2,2,1', 5: '3,1,-1', 6: '3,1,1', 7: '4,1,-1', 8: '4,1,0', 9: '4,1,1', 10: '5,1,-1', 11: '5,1,0', 12: '5,1,1', 13: '6,1,-1', 14: '6,1,1', 15: '7,1,-2', 16: '7,1,2', 17: '7,2,-1', 18: '7,2,1', 19: '8,1,0', 20: '9,1,0', 21: '9,1,-1', 22: '9,1,1', 23: '10,1,0', 24: '10,1,-1', 25: '10,1,1', 26: '11,1,0', 27: '11,1,-1', 28: '11,1,1', 29: '12,1,0', 30: '12,1,-1', 31: '12,1,1', 32: '13,1,0', 33: '13,1,-1', 34: '13,1,1', 35: '14,1,0', 36: '14,1,-1', 37: '14,1,1', 38: '15,1,0', 39: '15,1,-1', 40: '15,1,1', 41: '16,1,0', 42: '16,1,-1', 43: '16,1,1', 44: '-16,-1,0', 45: '-16,-1,-1', 46: '-16,-1,1', 47: '-15,-1,0', 48: '-15,-1,-1', 49: '-15,-1,1', 50: '-14,-1,0', 51: '-14,-1,-1', 52: '-14,-1,1', 53: '-13,-1,0', 54: '-13,-1,-1', 55: '-13,-1,1', 56: '-12,-1,0', 57: '-12,-1,-1', 58: '-12,-1,1', 59: '-11,-1,0', 60: '-11,-1,-1', 61: '-11,-1,1', 62: '-10,-1,0', 63: '-10,-1,-1', 64: '-10,-1,1', 65: '-9,-1,0', 66: '-9,-1,-1', 67: '-9,-1,1', 68: '-8,-1,0', 69: '-7,-1,-2', 70: '-7,-1,2', 71: '-7,-2,-1', 72: '-7,-2,1', 73: '-6,-1,-1', 74: '-6,-1,1', 75: '-5,-1,-1', 76: '-5,-1,0', 77: '-5,-1,1', 78: '-4,-1,-1', 79: '-4,-1,0', 80: '-4,-1,1', 81: '-3,-1,-1', 82: '-3,-1,1', 83: '-2,-1,-2', 84: '-2,-1,2', 85: '-2,-2,-1', 86: '-2,-2,1', 87: '-1,-1,0'}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device  = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games  import games\n",
    "from game   import Game\n",
    "from player import Player_Template\n",
    "from team   import Team\n",
    "from userinput import userInput\n",
    "\n",
    "def run_trials(user_input):\n",
    "    \"\"\" Runs the num_trials \"\"\"\n",
    "    global cycle\n",
    "    global HISTORY_FILE\n",
    "    global SUMMARY_FILE\n",
    "    global env\n",
    "    global agent_scores\n",
    "    \n",
    "    print(\"==============================RUN_TRIALS==================================\")\n",
    "    \n",
    "\n",
    "    num_trials = user_input.num_trials\n",
    "    num_sides  = user_input.num_sides\n",
    "        \n",
    "    env = Game(user_input.game, 8, num_sides, user_input.display_board_positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = userInput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_trials(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(user_input.num_sides):\n",
    "    team_name = \"team_\" + str(j)\n",
    "    team_name = Team(user_input.game, user_input.teams[j][\"team_name\"], j,user_input.teams[j][\"skill\"] / 10, user_input.teams[j][\"strategy\"])\n",
    "\n",
    "    env.insert_team(team_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pos_numeric_name = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pos_numeric_name_ = {start_pos_numeric_name.update({k:numeric_names[v]}) if v else None for k,v in env.board.items()}\n",
    "start_pos_numeric_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "472 - 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
