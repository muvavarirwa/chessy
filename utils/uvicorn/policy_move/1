from fastapi import FastAPI
import sys
import os
import csv
import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from random import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

fc1 = 16384
fc2 = 33768
fc3 = 33768
fc4 = 4096
fc5 = 64


class QNetwork(nn.Module):
    """Actor (Policy) Model."""

    def __init__(self, state_size, reward_size, fc1_units=fc1, fc2_units=fc2, fc3_units=fc3, fc4_units=fc4, fc5_units=fc5):
        """Initialize parameters and build model.
        Params
        ======
            state_size (int): Dimension of each state
            action_size (int): Dimension of each action
            seed (int): Random seed
            fc1_units (int): Number of nodes in first hidden layer
            fc2_units (int): Number of nodes in second hidden layer
        """
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size , fc1_units)
        #torch.nn.BatchNorm1d(fc1_units)
        self.fc2 = nn.Linear(fc1_units, fc2_units)
        #torch.nn.BatchNorm1d(fc2_units)
        self.fc3 = nn.Linear(fc2_units, fc3_units)
        #torch.nn.BatchNorm1d(fc3_units)
        self.fc4 = nn.Linear(fc3_units, fc4_units)
        #torch.nn.BatchNorm1d(fc4_units)
        self.fc5 = nn.Linear(fc4_units, reward_size)

    def forward(self, state_):
        """Build a network that maps state -> action values."""
        state = state_/16 # scale inputs from -16:+16 to -1:+1
        x = F.tanh(self.fc1(state))
        x = F.tanh(self.fc2(x))
        x = F.tanh(self.fc3(x))
        x = F.tanh(self.fc4(x))
        x = 2*F.sigmoid(self.fc5(x)) - 1
        return F.tanh(x)


predictNet = QNetwork(state_size+action_size, 1).to(device)

predictNet.load_state_dict(torch.load("Chessy_PredictNet_State_Dict_16Kx32Kx16Kx4Kx64_net_4xtanH_MSE_Loss_06_29_2023_001"))


app = FastAPI()

cycle = 0

@app.get('/policy/cycle')
def getCycle():
    global cycle
    cycle += 1
    return cycle

